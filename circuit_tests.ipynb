{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6e30ed7-19bd-495b-9c3b-37b7753f3934",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Manual mobilenet implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "69987bda-3e35-44f6-9117-a90513a9513b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from models.mobilenet import MyMobileNet\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pytorch_lightning as pl\n",
    "import torchvision\n",
    "import torchvision.transforms.v2 as transforms\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "1a79f141-4c15-4283-9a09-4825a99028f9",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Circom defines the range of positives are [0, p/2] and the range of negatives are [(p/2)+1, (p-1)].\n",
    "# CIRCOM_PRIME = 21888242871839275222246405745257275088548364400416034343698204186575808495617\n",
    "CIRCOM_PRIME = 28948022309329048855892746252171976963363056481941647379679742748393362948097\n",
    "MAX_POSITIVE = CIRCOM_PRIME // 2\n",
    "MAX_NEGATIVE = MAX_POSITIVE + 1 # The most positive number\n",
    "CIRCOM_NEGATIVE_1 = CIRCOM_PRIME - 1\n",
    "EXPONENT = 15\n",
    "\n",
    "def from_circom(x):\n",
    "    if type(x) != int:\n",
    "        x = int(x)\n",
    "    if x > MAX_POSITIVE: \n",
    "        return x - CIRCOM_PRIME\n",
    "    return x\n",
    "    \n",
    "def to_circom(x):\n",
    "    return x % CIRCOM_PRIME\n",
    "    \n",
    "def to_circom_input(array: np.array): \n",
    "    if type(array) != np.array:\n",
    "        array = np.array(array)\n",
    "    int_array = array.round().astype(int)\n",
    "    int_array = to_circom(int_array)\n",
    "    return int_array.astype(str).tolist()\n",
    "\n",
    "\n",
    "def DepthwiseConv(nRows, nCols, nChannels, nFilters, kernelSize, strides, n, input, weights, bias):\n",
    "    assert(nFilters % nChannels == 0)\n",
    "    outRows = (nRows - kernelSize)//strides + 1\n",
    "    outCols = (nCols - kernelSize)//strides + 1\n",
    "    \n",
    "    # out = np.zeros((outRows, outCols, nFilters))\n",
    "    out = [[[0 for _ in range(nFilters)] for _ in range(outCols)] for _ in range(outRows)]\n",
    "    remainder = [[[0 for _ in range(nFilters)] for _ in range(outCols)] for _ in range(outRows)]\n",
    "    # remainder = np.zeros((outRows, outCols, nFilters))\n",
    "    \n",
    "    for row in range(outRows):\n",
    "        for col in range(outCols):\n",
    "            for channel in range(nChannels):\n",
    "                for x in range(kernelSize):\n",
    "                    for y in range(kernelSize):\n",
    "                        out[row][col][channel] += int(input[row*strides+x, col*strides+y, channel]) * int(weights[x, y, channel])\n",
    "                \n",
    "                out[row][col][channel] += int(bias[channel])\n",
    "                remainder[row][col][channel] = str(int(out[row][col][channel] % n))\n",
    "                out[row][col][channel] = int(out[row][col][channel] // n)\n",
    "                            \n",
    "    return out, remainder\n",
    "\n",
    "def PointwiseConv2d(nRows, nCols, nChannels, nFilters, strides, n, input, weights, bias):\n",
    "    kernelSize = 1\n",
    "    outRows = (nRows - kernelSize)//strides + 1\n",
    "    outCols = (nCols - kernelSize)//strides + 1\n",
    "    out = [[[0 for _ in range(nFilters)] for _ in range(outCols)] for _ in range(outRows)]\n",
    "    str_out = [[[0 for _ in range(nFilters)] for _ in range(outCols)] for _ in range(outRows)]\n",
    "    remainder = [[[None for _ in range(nFilters)] for _ in range(outCols)] for _ in range(outRows)]\n",
    "    for row in range(outRows):\n",
    "        for col in range(outCols):\n",
    "            for filter in range(nFilters):\n",
    "                for k in range(nChannels):\n",
    "                    out[row][col][filter] += int(input[row*strides, col*strides, k]) * int(weights[k, filter])\n",
    "                    \n",
    "                out[row][col][filter] += int(bias[filter])\n",
    "                remainder[row][col][filter] = str(int(out[row][col][filter] % n))\n",
    "                out[row][col][filter] = int(out[row][col][filter] // n)\n",
    "                str_out[row][col][filter] = str(out[row][col][filter] % p)\n",
    "                            \n",
    "    return out, str_out, remainder\n",
    "\n",
    "def SeparableConvImpl(nRows, nCols, nChannels, nDepthFilters, nPointFilters, kernelSize, strides, n, input, depthWeights, pointWeights, depthBias, pointBias):\n",
    "    outRows = (nRows - kernelSize)//strides + 1\n",
    "    outCols = (nCols - kernelSize)//strides + 1\n",
    "\n",
    "    depth_out, depth_remainder = DepthwiseConv(nRows, nCols, nChannels, nDepthFilters, kernelSize, strides, n, input, depthWeights, depthBias)\n",
    "    point_out, point_str_out, point_remainder = PointwiseConv2d(outRows, outCols, nChannels, nPointFilters, strides, n, depth_out, pointWeights, pointBias)\n",
    "    return depth_out, depth_remainder, point_out, point_str_out, point_remainder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "faaa5941-253c-4676-94bd-c4dfbb1e524f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetWrapper(torch.utils.data.Dataset):\n",
    "    def __init__(self, subset, transform=None):\n",
    "        self.subset = subset\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        x, y = self.subset[index]\n",
    "        \n",
    "        if self.transform:\n",
    "            x = self.transform(x)\n",
    "        return x, y\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "622df7cd-d2f1-40bd-b801-27df6c3b0cdf",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(\n",
    "    root='./CIFAR10', train=False, download=True, transform=transform)\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(\n",
    "    root='./CIFAR10', train=True, download=True)\n",
    "\n",
    "# split the train set into train/validation\n",
    "train_set_size = int(len(trainset) * 0.8)\n",
    "valid_set_size = len(trainset) - train_set_size\n",
    "\n",
    "seed = torch.Generator().manual_seed(42)\n",
    "trainset, validset = torch.utils.data.random_split(trainset, [train_set_size, valid_set_size], generator=seed)\n",
    "\n",
    "trainset = DatasetWrapper(trainset, transform)\n",
    "validset = DatasetWrapper(validset, transform)\n",
    "\n",
    "# Create train dataloader\n",
    "trainloader = torch.utils.data.DataLoader(\n",
    "    trainset, batch_size=512, shuffle=True, num_workers=24)\n",
    "\n",
    "# Create validation dataloader\n",
    "validloader = torch.utils.data.DataLoader(\n",
    "    validset, batch_size=512, shuffle=False, num_workers=24)\n",
    "\n",
    "# Create test dataloader\n",
    "testloader = torch.utils.data.DataLoader(\n",
    "    testset, batch_size=512, shuffle=False, num_workers=24)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat', 'deer',\n",
    "           'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0ef5c968-2309-4bcb-a680-9450536ef079",
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted horse - idx: 7\n"
     ]
    }
   ],
   "source": [
    "MODEL_WEIGHTS_PATH = './checkpoints/model_small_100epochs.pth'\n",
    "\n",
    "model = MyMobileNet(trainloader, num_classes=10, alpha=0.25, max_epochs=100)\n",
    "checkpoint = torch.load(MODEL_WEIGHTS_PATH)\n",
    "# model.load_state_dict(checkpoint['state_dict'])\n",
    "model.load_state_dict(checkpoint['net'])\n",
    "model.eval()\n",
    "\n",
    "image, label = validset[0]\n",
    "image = image.unsqueeze(0)\n",
    "logits = model(image)\n",
    "pred_idx = logits.argmax()\n",
    "\n",
    "print(f\"Predicted {classes[pred_idx]} - idx: {pred_idx}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "da689ff6-ad5a-421b-b388-bf0b26467475",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Conv2DInt(nRows, nCols, nChannels, nFilters, kernelSize, strides, n, input, weights, bias):\n",
    "    p = CIRCOM_PRIME\n",
    "    Input = [[[str(int(input[i][j][k]) % p) for k in range(nChannels)] for j in range(nCols)] for i in range(nRows)]\n",
    "    Weights = [[[[str(int(weights[i][j][k][l]) % p) for l in range(nFilters)] for k in range(nChannels)] for j in range(kernelSize)] for i in range(kernelSize)]\n",
    "    Bias = [str(int(bias[i]) % p) for i in range(nFilters)]\n",
    "    out = [[[0 for _ in range(nFilters)] for _ in range((nCols - kernelSize)//strides + 1)] for _ in range((nRows - kernelSize)//strides + 1)]\n",
    "    remainder = [[[None for _ in range(nFilters)] for _ in range((nCols - kernelSize)//strides + 1)] for _ in range((nRows - kernelSize)//strides + 1)]\n",
    "    for i in range((nRows - kernelSize)//strides + 1):\n",
    "        for j in range((nCols - kernelSize)//strides + 1):\n",
    "            for m in range(nFilters):\n",
    "                for k in range(nChannels):\n",
    "                    for x in range(kernelSize):\n",
    "                        for y in range(kernelSize):\n",
    "                            out[i][j][m] += int(input[i*strides+x][j*strides+y][k]) * int(weights[x][y][k][m])\n",
    "                out[i][j][m] += int(bias[m])\n",
    "                remainder[i][j][m] = str(int(out[i][j][m]) % n)\n",
    "                out[i][j][m] = str(int(out[i][j][m]) // n % p)\n",
    "    return Input, Weights, Bias, out, remainder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "247d5284-3c4f-4fd3-9781-43b47c592365",
   "metadata": {},
   "outputs": [],
   "source": [
    "def BatchNormalizationInt(nRows, nCols, nChannels, n, X_in, a_in, b_in):\n",
    "    p = CIRCOM_PRIME\n",
    "    X = [[[str(int(X_in[i][j][k]) % p) for k in range(nChannels)] for j in range(nCols)] for i in range(nRows)]\n",
    "    A = [str(int(a_in[k]) % p) for k in range(nChannels)]\n",
    "    B = [str(int(b_in[k]) % p) for k in range(nChannels)]\n",
    "    out = [[[0 for _ in range(nChannels)] for _ in range(nCols)] for _ in range(nRows)]\n",
    "    remainder = [[[None for _ in range(nChannels)] for _ in range(nCols)] for _ in range(nRows)]\n",
    "    for i in range(nRows):\n",
    "        for j in range(nCols):\n",
    "            for k in range(nChannels):\n",
    "                out[i][j][k] = int(int(X_in[i][j][k]) * int(a_in[k]) + int(b_in[k]))\n",
    "                remainder[i][j][k] = str(int(out[i][j][k]) % n)\n",
    "                out[i][j][k] = int(out[i][j][k] // n)\n",
    "    return X, A, B, out, remainder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "56e21b0a-fbb2-46db-80c1-5d8f57518c95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out.shape=torch.Size([1, 8, 32, 32])\n",
      "expected.shape=torch.Size([32, 32, 8])\n",
      "torch.Size([1, 8, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "gamma = model.bn.weight\n",
    "beta = model.bn.bias\n",
    "mean = model.bn.running_mean\n",
    "var = model.bn.running_var\n",
    "eps = model.bn.eps\n",
    "\n",
    "a = (gamma/(var+eps)**.5).detach()\n",
    "b = (beta-gamma*mean/(var+eps)**.5).detach().tolist()\n",
    "\n",
    "image, label = testset[0]\n",
    "image = image.unsqueeze(0)\n",
    "out = model.conv(image)\n",
    "expected = model.bn(out)\n",
    "# out = out.squeeze()\n",
    "print(f\"{out.shape=}\")\n",
    "\n",
    "expected = torch.permute(expected.squeeze(), (1, 2, 0))\n",
    "\n",
    "print(f\"{expected.shape=}\")\n",
    "\n",
    "quantized_in = torch.permute(out.squeeze(), (1, 2, 0)) * 10**EXPONENT\n",
    "quantized_a = (a * 10**EXPONENT).tolist()\n",
    "quantized_b = [bi * 10**(2*EXPONENT) for bi in b]\n",
    "print(out.shape)\n",
    "\n",
    "X, A, B, actual, remainder = BatchNormalizationInt(32, 32, 8, 10**EXPONENT, quantized_in, quantized_a, quantized_b)\n",
    "test_output = [[[from_circom(int(out)) / 10**EXPONENT for out in asdf] for asdf in asdfasdf] for asdfasdf in actual]\n",
    "\n",
    "assert(torch.allclose(torch.Tensor(test_output), expected, atol=1e-6))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea13b7cc-41ef-4127-b1fd-99ffcc4723b8",
   "metadata": {},
   "source": [
    "# Testing head layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fbe4fd19-94b7-4b11-9dac-e308c523b7a8",
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "something\n",
      "before conv2d\n",
      "after conv2d\n",
      "after bn\n",
      "after relu\n",
      "end\n",
      "ok\n"
     ]
    }
   ],
   "source": [
    "# CONVOLUTION LAYER\n",
    "weights = model.conv.weight.detach().numpy().transpose(2, 3, 1, 0)\n",
    "image, label = testset[0]\n",
    "\n",
    "expected = model.conv(image).detach().numpy()\n",
    "bias = torch.zeros(weights.shape[3]).numpy()\n",
    "\n",
    "# padded = pad(image, 1).transpose(1,2,0)\n",
    "padded = F.pad(image, (1,1,1,1), \"constant\", 0).numpy()\n",
    "padded = padded.transpose(1,2,0)\n",
    "\n",
    "quantized_image = (padded * 10**EXPONENT).round()\n",
    "quantized_weights = (weights * 10**EXPONENT).round() # .transpose(0, 3, 1, 0) # [nFilters, nChannels, H, W] -> \n",
    "\n",
    "circuit_in, circuit_conv_weights, circuit_conv_bias, circuit_conv_out, circuit_conv_remainder = Conv2DInt(34, 34, 3, 8, 3, 1, 10**EXPONENT, quantized_image, quantized_weights, bias)\n",
    "# output, remainder = Conv2DInt(34, 34, 3, 8, 3, 1, 10**EXPONENT, quantized_image, quantized_weights, bias)\n",
    "# test_output = output / 10**(EXPONENT)\n",
    "test_output = [[[from_circom(int(out)) / 10**EXPONENT for out in asdf] for asdf in asdfasdf] for asdfasdf in circuit_conv_out]\n",
    "\n",
    "expected = expected.transpose((1, 2, 0))\n",
    "\n",
    "assert(np.allclose(test_output, expected, atol=1e-6))\n",
    "\n",
    "# BATCH NORM CONSTANTS\n",
    "gamma = model.bn.weight\n",
    "beta = model.bn.bias\n",
    "mean = model.bn.running_mean\n",
    "var = model.bn.running_var\n",
    "eps = model.bn.eps\n",
    "\n",
    "a = (gamma/(var+eps)**.5).detach()\n",
    "b = (beta-gamma*mean/(var+eps)**.5).detach().tolist()\n",
    "\n",
    "quantized_a = (a * 10**EXPONENT).tolist()\n",
    "quantized_b = [bi * 10**(2*EXPONENT) for bi in b]\n",
    "\n",
    "# BATCH NORM USING PYTORCH OUTPUT\n",
    "image, label = testset[0]\n",
    "image = image.unsqueeze(0)\n",
    "out = model.conv(image)\n",
    "expected = model.bn(out)\n",
    "\n",
    "expected = torch.permute(expected.squeeze(), (1, 2, 0))\n",
    "\n",
    "quantized_in = torch.permute(out.squeeze(), (1, 2, 0)) * 10**EXPONENT\n",
    "\n",
    "X, A, B, actual, remainder = BatchNormalizationInt(32, 32, 8, 10**EXPONENT, quantized_in, quantized_a, quantized_b)\n",
    "test_output = [[[from_circom(int(out)) / 10**EXPONENT for out in vec] for vec in matrix] for matrix in actual]\n",
    "assert(torch.allclose(torch.Tensor(test_output), expected, atol=1e-6))\n",
    "\n",
    "# BATCH NORM USING CIRCUIT CONV OUTPUT\n",
    "quantized_in = [[[from_circom(int(out)) for out in vec] for vec in matrix] for matrix in circuit_conv_out]\n",
    "\n",
    "_, circuit_bn_a, circuit_bn_b, circuit_bn_out, circuit_bn_remainder = BatchNormalizationInt(32, 32, 8, 10**EXPONENT, quantized_in, quantized_a, quantized_b)\n",
    "\n",
    "test_output = [[[int(from_circom(int(out))) / 10**EXPONENT for out in vec] for vec in matrix] for matrix in actual]\n",
    "\n",
    "assert(torch.allclose(torch.Tensor(test_output), expected, atol=1e-6))\n",
    "\n",
    "# RELU USING CIRCUIT OUTPUT\n",
    "relu_in = [[[to_circom(bn_out) if bn_out < MAX_POSITIVE else 0 for bn_out in vec] for vec in matrix] for matrix in circuit_bn_out]\n",
    "relu_out = [[[str(bn_out) if bn_out < MAX_POSITIVE else 0 for bn_out in vec] for vec in matrix] for matrix in relu_in]\n",
    "\n",
    "input_json_path = \"head_input1.json\"\n",
    "with open(input_json_path, \"w\") as input_file:\n",
    "    json.dump({\"in\": circuit_in,\n",
    "               \"conv2d_weights\": circuit_conv_weights,\n",
    "               \"conv2d_bias\": circuit_conv_bias,\n",
    "               \"conv2d_out\": circuit_conv_out,\n",
    "               \"conv2d_remainder\": circuit_conv_remainder,\n",
    "               \n",
    "               \"bn_a\": circuit_bn_a,\n",
    "               \"bn_b\": circuit_bn_b,\n",
    "               \"bn_out\": circuit_bn_out,\n",
    "               \"bn_remainder\": circuit_bn_remainder,\n",
    "               \n",
    "               \"relu_out\": relu_out,\n",
    "               }, input_file)\n",
    "\n",
    "os.chdir(\"circuits\")\n",
    "!./head/head_cpp/head ../head_input1.json head.wtns\n",
    "!npx snarkjs groth16 prove head/circuit_final.zkey head.wtns proof.json public_test.json\n",
    "os.chdir(\"../\")\n",
    "\n",
    "print(\"ok\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f410f561-1bc2-4fc4-94cb-cf4c1d4ed258",
   "metadata": {},
   "source": [
    "# Testing padding over highly padded input (to try and fold the circuit using nova)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "ecdd2b74-e553-4f68-b755-7f50d91a58a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SeparableConv2d(nn.Module):\n",
    "    '''Separable convolution'''\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super(SeparableConv2d, self).__init__()\n",
    "        self.dw_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, in_channels, kernel_size=3, stride=stride, padding=0, groups=in_channels, bias=False),\n",
    "            # nn.BatchNorm2d(in_channels),\n",
    "            # nn.ReLU(inplace=False),\n",
    "        )\n",
    "        self.pw_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0, bias=False),\n",
    "            # nn.BatchNorm2d(out_channels),\n",
    "            # nn.ReLU(inplace=False),\n",
    "        )\n",
    "        \n",
    "input = torch.randn((1, 3, 5, 5))\n",
    "test_model = SeparableConv2d(3, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "2288f8d3-b0d8-4954-9d15-b7ca41f5b065",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 3, 5, 5)\n"
     ]
    }
   ],
   "source": [
    "weights = test_model.dw_conv[0].weight.squeeze().detach().numpy()\n",
    "bias = torch.zeros(weights.shape[0]).numpy()\n",
    "\n",
    "expected = test_model.dw_conv(input).detach().numpy()\n",
    "print(expected.shape)\n",
    "\n",
    "padded = F.pad(input, (1,1,1,1), \"constant\", 0)\n",
    "padded = padded.squeeze().numpy().transpose((1, 2, 0))\n",
    "weights = weights.transpose((1, 2, 0))\n",
    "\n",
    "quantized_image = padded * 10**EXPONENT\n",
    "quantized_weights = weights * 10**EXPONENT\n",
    "\n",
    "actual, rem = DepthwiseConv(7, 7, 3, 3, 3, 1, 10**EXPONENT, quantized_image.round(), quantized_weights.round(), bias)\n",
    "\n",
    "expected = expected.squeeze().transpose((1, 2, 0))\n",
    "expected = expected * 10**EXPONENT\n",
    "\n",
    "assert(np.allclose(expected, actual, atol=0.00001))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "abef6b48-7498-40f8-ace1-84603933c80c",
   "metadata": {},
   "outputs": [],
   "source": [
    "input = torch.randn((1, 3, 5, 5))\n",
    "model = SeparableConv2d(3, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "804fb4ae-b6e6-4d73-b30d-3450511b20da",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[[-84775161859527321744602510080,\n",
       "   -280863115324426945947120868368,\n",
       "   -316509859287301671233788074460],\n",
       "  [-412066814475013670629964755790,\n",
       "   -307907089920012261542884320079,\n",
       "   288733480109590434131766261999],\n",
       "  [88725553719746761339810928348,\n",
       "   57251477502350644469532169751,\n",
       "   167073417747014310631386775640],\n",
       "  [-451547140994921540780591667954,\n",
       "   1001315881174683093580772914720,\n",
       "   -234907986425664705310764373280],\n",
       "  [-368065005502237197896044401205,\n",
       "   -49827584055244381693338364130,\n",
       "   57013630737515115414246974069],\n",
       "  [0, 0, 0],\n",
       "  [0, 0, 0]],\n",
       " [[225228896280826144742018725083,\n",
       "   937129546496503660185568529414,\n",
       "   -424060379269759190852526866365],\n",
       "  [1000974140683911103665188311425,\n",
       "   436335591241852765388689964311,\n",
       "   -210043991425453207740685880708],\n",
       "  [-1325614803459234583698167899456,\n",
       "   -66160312754055519347597073647,\n",
       "   497778292584125615331388252343],\n",
       "  [-2810563469212315852893212650,\n",
       "   -203075372530997502527143471525,\n",
       "   -51156603705155919392846418305],\n",
       "  [73081377014346700343771608459,\n",
       "   276631808676749813561859104084,\n",
       "   -298937115046140476607297394139],\n",
       "  [0, 0, 0],\n",
       "  [0, 0, 0]],\n",
       " [[488233277459294339589150646134,\n",
       "   -257060382322927716232019607560,\n",
       "   538686731966689771423321042676],\n",
       "  [158675488973494755410279325588,\n",
       "   628500716535402725412119056273,\n",
       "   391399905968010802871171879815],\n",
       "  [374448094870163446111291159468,\n",
       "   -565056286414698044156747149827,\n",
       "   -84318127755918306987945288711],\n",
       "  [81761699162622852196104719041,\n",
       "   106776511563294891361002170015,\n",
       "   476778425100678428193609334585],\n",
       "  [-217300864697707252897299696997,\n",
       "   398261619750319368980303945136,\n",
       "   -306541101792726846638493074124],\n",
       "  [0, 0, 0],\n",
       "  [0, 0, 0]],\n",
       " [[39933026105203869404911987658,\n",
       "   145540548521220338810105371522,\n",
       "   -208572871635821255365358112613],\n",
       "  [-891117321457787348540035898623,\n",
       "   -998141746422527956946324147793,\n",
       "   423928628878786049249456981670],\n",
       "  [490712217074363848906027313205,\n",
       "   553233029394454785496237718689,\n",
       "   -234820302539604473281307823527],\n",
       "  [717274953055299386073968808432,\n",
       "   -1110998108713562893329147390887,\n",
       "   271462729943421982257161748123],\n",
       "  [69869392525160265891422302957,\n",
       "   780121010512127911499056568573,\n",
       "   599576454251464695868293257206],\n",
       "  [0, 0, 0],\n",
       "  [0, 0, 0]],\n",
       " [[247122215198635219695721345488,\n",
       "   375867137451024154152642004192,\n",
       "   -273632323583436772886692677275],\n",
       "  [-393689509405904299942516343435,\n",
       "   -176704562469034199229563953850,\n",
       "   -204350269213501658565400102816],\n",
       "  [-276778960751967618533178792859,\n",
       "   234939765899033506985734160480,\n",
       "   56190366007568347545331665060],\n",
       "  [23043710976812998767705503269,\n",
       "   68967619202967831349415463592,\n",
       "   -145437269147036036017997015733],\n",
       "  [294681153767306029485777813534,\n",
       "   -398386168342861219826655458746,\n",
       "   375393951002133551116435075528],\n",
       "  [0, 0, 0],\n",
       "  [0, 0, 0]],\n",
       " [[0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0]],\n",
       " [[0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0]]]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actual, rem = DepthwiseConv(7, 7, 3, 3, 3, 1, 10**EXPONENT, quantized_image.round(), quantized_weights.round(), bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "811eec43-c91a-4750-99a9-21b0789928cc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 3, 5, 5)\n",
      "Signal not found\n",
      "origDepthwiseConv2d: calcwit.cpp:60: uint Circom_CalcWit::getInputSignalHashPosition(u64): Assertion `false' failed.\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "def DepthwiseConv(nRows, nCols, nChannels, nFilters, kernelSize, strides, n, input, weights, bias):\n",
    "    assert(nFilters % nChannels == 0)\n",
    "    outRows = (nRows - kernelSize)//strides + 1\n",
    "    outCols = (nCols - kernelSize)//strides + 1\n",
    "    \n",
    "    # out = np.zeros((outRows, outCols, nFilters))\n",
    "    out = [[[0 for _ in range(nFilters)] for _ in range(outCols)] for _ in range(outRows)]\n",
    "    remainder = [[[0 for _ in range(nFilters)] for _ in range(outCols)] for _ in range(outRows)]\n",
    "    # remainder = np.zeros((outRows, outCols, nFilters))\n",
    "    \n",
    "    for row in range(outRows):\n",
    "        for col in range(outCols):\n",
    "            for channel in range(nChannels):\n",
    "                for x in range(kernelSize):\n",
    "                    for y in range(kernelSize):\n",
    "                        out[row][col][channel] += int(input[row*strides+x, col*strides+y, channel]) * int(weights[x, y, channel])\n",
    "                \n",
    "                out[row][col][channel] += int(bias[channel])\n",
    "                remainder[row][col][channel] = str(int(out[row][col][channel] % n))\n",
    "                out[row][col][channel] = int(out[row][col][channel] // n)\n",
    "                            \n",
    "    return out, remainder\n",
    "    \n",
    "weights = test_model.dw_conv[0].weight.squeeze().detach().numpy()\n",
    "bias = torch.zeros(weights.shape[0]).numpy()\n",
    "\n",
    "expected = test_model.dw_conv(input).detach().numpy()\n",
    "print(expected.shape)\n",
    "\n",
    "padded = F.pad(input, (1,1,1,1), \"constant\", 0)\n",
    "padded = padded.squeeze().numpy().transpose((1, 2, 0))\n",
    "weights = weights.transpose((1, 2, 0))\n",
    "\n",
    "quantized_image = padded * 10**EXPONENT\n",
    "quantized_weights = weights * 10**EXPONENT\n",
    "\n",
    "actual, rem = DepthwiseConv(7, 7, 3, 3, 3, 1, 10**EXPONENT, quantized_image.round(), quantized_weights.round(), bias)\n",
    "\n",
    "test_output = np.array([[[int(out) / 10**EXPONENT for out in asdf] for asdf in asdfasdf] for asdfasdf in actual])\n",
    "# test_output = test_output[1:6, 1:6, :]\n",
    "\n",
    "expected = expected.squeeze().transpose((1, 2, 0))\n",
    "# expected = expected * 10**EXPONENT\n",
    "assert(np.allclose(expected, test_output, atol=0.00001))\n",
    "\n",
    "circuit_in = quantized_image.round().astype(int).astype(str).tolist()\n",
    "# circuit_weights = quantized_weights.round().astype(int).astype(str).tolist()\n",
    "circuit_bias = bias.round().astype(int).astype(str).tolist()\n",
    "\n",
    "p = CIRCOM_PRIME\n",
    "Input = [[[str(int(quantized_image[i][j][k].round()) % p) for k in range(quantized_image.shape[2])] for j in range(quantized_image.shape[1])] for i in range(quantized_image.shape[0])]\n",
    "Weights = [[[str(int(quantized_weights[i][j][k].round()) % p) for k in range(quantized_weights.shape[2])] for j in range(quantized_weights.shape[1])] for i in range(quantized_weights.shape[0])]\n",
    "# Out = [[[str(int(actual[i][j][k]) % p) for k in range(3)] for j in range(7)] for i in range(7)]\n",
    "\n",
    "test_output = np.array([[[int(out) for out in asdf] for asdf in asdfasdf] for asdfasdf in actual])\n",
    "# test_output = test_output[1:6, 1:6, :]\n",
    "Out = [[[str(int(test_output[i][j][k]) % p) for k in range(3)] for j in range(5)] for i in range(5)]\n",
    "Rem = [[[str(rem[i][j][k]) for k in range(3)] for j in range(5)] for i in range(5)]\n",
    "\n",
    "input_json_path = \"backbone_input.json\"\n",
    "with open(input_json_path, \"w\") as input_file:\n",
    "    json.dump({\n",
    "               \"in\": Input,\n",
    "               \"weights\": Weights,\n",
    "               \"bias\": circuit_bias,\n",
    "               \"remainder\": Rem,\n",
    "               \"out\": Out,\n",
    "              },\n",
    "              input_file)\n",
    "\n",
    "\n",
    "# os.chdir(\"circuits\")\n",
    "# !./origDepthwiseConv2d/origDepthwiseConv2d_cpp/origDepthwiseConv2d ../backbone_input.json head.wtns\n",
    "# !npx snarkjs groth16 prove ./origDepthwiseConv2d/circuit_final.zkey head.wtns proof.json public_test.json\n",
    "# os.chdir(\"../\")\n",
    "\n",
    "print(\"OK\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f933a59-1605-4a64-ac94-f4e7107a79e7",
   "metadata": {},
   "source": [
    "# Padded Convolution test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c6fd691-e729-404e-a858-2d3e45e26846",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "b8a3222d-4eee-4060-b336-07da6efa1ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "input = torch.randn((1, 3, 7, 7))\n",
    "model = SeparableConv2d(3, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "b70bb0a6-f586-4719-a200-501ad9bb708d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape:  torch.Size([1, 3, 7, 7])\n",
      "Expected shape:  torch.Size([1, 3, 5, 5])\n",
      "Expected shape:  (5, 5, 3)\n",
      "test_output shape:  (5, 5, 3)\n",
      "--------------------------------\n",
      "Input shape:  torch.Size([1, 3, 7, 7])\n",
      "expected shape:  torch.Size([1, 3, 5, 5])\n",
      "expected shape:  (3, 3, 3)\n",
      "1st shape:  (7, 7, 3)\n",
      "2nd shape:  (7, 7, 3)\n",
      "actual shape:  (7, 7, 3)\n",
      "expected shape:  (3, 3, 3)\n",
      "test_output shape:  (7, 7, 3)\n",
      "OUT SHAPE:  (7, 7, 3)\n",
      "REM SHAPE:  (7, 7, 3)\n",
      "START\n",
      "PRIME - 1 28948022309329048855892746252171976963363056481941647379679742748393362948096\n",
      "END\n"
     ]
    }
   ],
   "source": [
    "def PaddedDepthwiseConv(nRows, nCols, nChannels, nFilters, kernelSize, strides, n, input, weights, bias):\n",
    "    assert(nFilters % nChannels == 0)\n",
    "    outRows = (nRows - kernelSize)//strides + 1\n",
    "    outCols = (nCols - kernelSize)//strides + 1\n",
    "    \n",
    "    # out = np.zeros((outRows, outCols, nFilters))\n",
    "    out = [[[0 for _ in range(nFilters)] for _ in range(nCols)] for _ in range(nRows)]\n",
    "    remainder = [[[str(0) for _ in range(nFilters)] for _ in range(nCols)] for _ in range(nRows)]\n",
    "    \n",
    "    for row in range(outRows):\n",
    "        for col in range(outCols):\n",
    "            for channel in range(nChannels):\n",
    "                for x in range(kernelSize):\n",
    "                    for y in range(kernelSize):\n",
    "                        out[row+1][col+1][channel] += int(input[row*strides+x][col*strides+y][channel]) * int(weights[x, y, channel])\n",
    "                \n",
    "                out[row+1][col+1][channel] += int(bias[channel])\n",
    "                remainder[row+1][col+1][channel] = str(int(out[row+1][col+1][channel] % n))\n",
    "                out[row+1][col+1][channel] = int(out[row+1][col+1][channel] // n)\n",
    "                            \n",
    "    return out, remainder\n",
    "    \n",
    "weights = model.dw_conv[0].weight.squeeze().detach().numpy()\n",
    "bias = torch.zeros(weights.shape[0]).numpy()\n",
    "\n",
    "print(\"Input shape: \", input.shape)\n",
    "expected = model.dw_conv(input).detach()\n",
    "print(\"Expected shape: \", expected.shape)\n",
    "\n",
    "padded = F.pad(input, (1,1,1,1), \"constant\", 0)\n",
    "padded = padded.squeeze().numpy().transpose((1, 2, 0))\n",
    "weights = weights.transpose((1, 2, 0))\n",
    "\n",
    "quantized_image = input.squeeze().numpy().transpose((1,2,0)) * 10**EXPONENT\n",
    "# quantized_image = padded * 10**EXPONENT\n",
    "quantized_weights = weights * 10**EXPONENT\n",
    "\n",
    "actual, rem = PaddedDepthwiseConv(7, 7, 3, 3, 3, 1, 10**EXPONENT, quantized_image.round(), quantized_weights.round(), bias)\n",
    "\n",
    "test_output = np.array([[[int(out) / 10**EXPONENT for out in asdf] for asdf in asdfasdf] for asdfasdf in actual])\n",
    "test_output = test_output[1:6, 1:6, :]\n",
    "\n",
    "# expected = expected.squeeze().transpose((1, 2, 0))\n",
    "# expected = F.pad(expected.squeeze(), (1,1,1,1)).numpy().transpose((1,2,0))\n",
    "expected = expected.squeeze().numpy().transpose((1,2,0))\n",
    "print(\"Expected shape: \", expected.shape)\n",
    "print(\"test_output shape: \", test_output.shape)\n",
    "\n",
    "# expected = expected * 10**EXPONENT\n",
    "assert(np.allclose(expected, test_output, atol=0.00001))\n",
    "print(\"--------------------------------\")\n",
    "\n",
    "print(\"Input shape: \", input.shape)\n",
    "expected = model.dw_conv(input).detach()\n",
    "print(\"expected shape: \", expected.shape)\n",
    "expected = model.dw_conv(expected).detach()\n",
    "expected = expected.squeeze().numpy().transpose((1,2,0))\n",
    "print(\"expected shape: \", expected.shape)\n",
    "dw_input, rem = PaddedDepthwiseConv(7, 7, 3, 3, 3, 1, 10**EXPONENT, quantized_image.round(), quantized_weights.round(), bias)\n",
    "print(\"1st shape: \", np.array(dw_input).shape)\n",
    "actual, rem = PaddedDepthwiseConv(7, 7, 3, 3, 3, 1, 10**EXPONENT, dw_input, quantized_weights.round(), bias)\n",
    "print(\"2nd shape: \", np.array(actual).shape)\n",
    "test_output = np.array([[[int(out) / 10**EXPONENT for out in asdf] for asdf in asdfasdf] for asdfasdf in actual])\n",
    "test_output = test_output[2:5, 2:5, :]\n",
    "\n",
    "\n",
    "# expected = F.pad(expected.squeeze(), (2,2,2,2)).numpy().transpose((1,2,0))\n",
    "print(\"actual shape: \", np.array(actual).shape)\n",
    "print(\"expected shape: \", np.array(expected).shape)\n",
    "assert(np.allclose(expected, test_output, atol=0.00001))\n",
    "\n",
    "p = CIRCOM_PRIME\n",
    "quantized_image = np.array(dw_input)\n",
    "Input = [[[str(int(dw_input[i][j][k]) % p) for k in range(quantized_image.shape[2])] for j in range(quantized_image.shape[1])] for i in range(quantized_image.shape[0])]\n",
    "Weights = [[[str(int(quantized_weights[i][j][k].round()) % p) for k in range(quantized_weights.shape[2])] for j in range(quantized_weights.shape[1])] for i in range(quantized_weights.shape[0])]\n",
    "# Out = [[[str(int(actual[i][j][k]) % p) for k in range(3)] for j in range(7)] for i in range(7)]\n",
    "\n",
    "test_output = np.array([[[int(out) for out in asdf] for asdf in asdfasdf] for asdfasdf in actual])\n",
    "print(\"test_output shape: \", np.array(test_output).shape)\n",
    "# # test_output = test_output[1:6, 1:6, :]\n",
    "Out = [[[str(int(test_output[i][j][k]) % p) for k in range(test_output.shape[2])] for j in range(test_output.shape[1])] for i in range(test_output.shape[0])]\n",
    "Rem = [[[str(rem[i][j][k]) for k in range(test_output.shape[2])] for j in range(test_output.shape[1])] for i in range(test_output.shape[0])]\n",
    "print(\"OUT SHAPE: \", np.array(Out).shape)\n",
    "print(\"REM SHAPE: \", np.array(Rem).shape)\n",
    "\n",
    "input_json_path = \"padded_input.json\"\n",
    "with open(input_json_path, \"w\") as input_file:\n",
    "    json.dump({\n",
    "               \"in\": Input,\n",
    "               \"dw_conv_weights\": Weights,\n",
    "               \"dw_conv_bias\": circuit_bias,\n",
    "               \"dw_conv_remainder\": Rem,\n",
    "               \"dw_conv_out\": Out,\n",
    "              },\n",
    "              input_file)\n",
    "\n",
    "\n",
    "os.chdir(\"circuits\")\n",
    "!./padded/padded_cpp/padded ../padded_input.json head.wtns\n",
    "# # !npx snarkjs groth16 prove ./origDepthwiseConv2d/circuit_final.zkey head.wtns proof.json public_test.json\n",
    "os.chdir(\"../\")\n",
    "\n",
    "# print(\"OK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "e784ee82-82ac-4368-9a57-d3b51a9ba584",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7, 7, 3)"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "8bcc73b0-e18d-403c-a05a-664406df0b99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 3, 3)"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_expected.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb20792-ff40-4c22-8acc-73b7a96f594f",
   "metadata": {},
   "source": [
    "# Padded convolution 2 iterations test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bcbe0be-2d60-4ebf-8d0f-814d63b76dfe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
