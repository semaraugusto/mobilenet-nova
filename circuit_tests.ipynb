{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6e30ed7-19bd-495b-9c3b-37b7753f3934",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Manual mobilenet implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef7abf63-8f1d-46fb-a0da-3fe772e3fbc1",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69987bda-3e35-44f6-9117-a90513a9513b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from models.mobilenet import ZkMobileNet\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pytorch_lightning as pl\n",
    "import torchvision\n",
    "import torchvision.transforms.v2 as transforms\n",
    "import json\n",
    "import os\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50a02d42-4769-4bb7-bf47-1f5834722c41",
   "metadata": {},
   "source": [
    "# Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "1a79f141-4c15-4283-9a09-4825a99028f9",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Circom defines the range of positives are [0, p/2] and the range of negatives are [(p/2)+1, (p-1)].\n",
    "# CIRCOM_PRIME = 21888242871839275222246405745257275088548364400416034343698204186575808495617 # bn254\n",
    "p = CIRCOM_PRIME = 28948022309329048855892746252171976963363056481941647379679742748393362948097 # vesta\n",
    "MAX_POSITIVE = CIRCOM_PRIME // 2\n",
    "MAX_NEGATIVE = MAX_POSITIVE + 1 # The most positive number\n",
    "CIRCOM_NEGATIVE_1 = CIRCOM_PRIME - 1\n",
    "EXPONENT = 15\n",
    "\n",
    "def from_circom(x):\n",
    "    if type(x) != int:\n",
    "        x = int(x)\n",
    "    if x > MAX_POSITIVE: \n",
    "        return x - CIRCOM_PRIME\n",
    "    return x\n",
    "    \n",
    "def to_circom(x):\n",
    "    return x % CIRCOM_PRIME\n",
    "    \n",
    "def to_circom_input(array: np.array): \n",
    "    if type(array) != np.array:\n",
    "        array = np.array(array)\n",
    "    int_array = array.round().astype(int)\n",
    "    int_array = to_circom(int_array)\n",
    "    return int_array.astype(str).tolist()\n",
    "\n",
    "# taken from https://github.com/socathie/circomlib-ml\n",
    "def Conv2DInt(nRows, nCols, nChannels, nFilters, kernelSize, strides, n, input, weights, bias):\n",
    "    p = CIRCOM_PRIME\n",
    "    Input = [[[str(int(input[i][j][k]) % p) for k in range(nChannels)] for j in range(nCols)] for i in range(nRows)]\n",
    "    Weights = [[[[str(int(weights[i][j][k][l]) % p) for l in range(nFilters)] for k in range(nChannels)] for j in range(kernelSize)] for i in range(kernelSize)]\n",
    "    Bias = [str(int(bias[i]) % p) for i in range(nFilters)]\n",
    "    out = [[[0 for _ in range(nFilters)] for _ in range((nCols - kernelSize)//strides + 1)] for _ in range((nRows - kernelSize)//strides + 1)]\n",
    "    remainder = [[[None for _ in range(nFilters)] for _ in range((nCols - kernelSize)//strides + 1)] for _ in range((nRows - kernelSize)//strides + 1)]\n",
    "    for i in range((nRows - kernelSize)//strides + 1):\n",
    "        for j in range((nCols - kernelSize)//strides + 1):\n",
    "            for m in range(nFilters):\n",
    "                for k in range(nChannels):\n",
    "                    for x in range(kernelSize):\n",
    "                        for y in range(kernelSize):\n",
    "                            out[i][j][m] += int(input[i*strides+x][j*strides+y][k]) * int(weights[x][y][k][m])\n",
    "                out[i][j][m] += int(bias[m])\n",
    "                remainder[i][j][m] = str(int(out[i][j][m]) % n)\n",
    "                out[i][j][m] = str(int(out[i][j][m]) // n % p)\n",
    "    return Input, Weights, Bias, out, remainder\n",
    "    \n",
    "# taken from https://github.com/socathie/circomlib-ml\n",
    "def BatchNormalizationInt(nRows, nCols, nChannels, n, X_in, a_in, b_in):\n",
    "    p = CIRCOM_PRIME\n",
    "    X = [[[str(int(X_in[i][j][k]) % p) for k in range(nChannels)] for j in range(nCols)] for i in range(nRows)]\n",
    "    A = [str(int(a_in[k]) % p) for k in range(nChannels)]\n",
    "    B = [str(int(b_in[k]) % p) for k in range(nChannels)]\n",
    "    out = [[[int(0) for _ in range(nChannels)] for _ in range(nCols)] for _ in range(nRows)]\n",
    "    out_str = [[[str(0) for _ in range(nChannels)] for _ in range(nCols)] for _ in range(nRows)]\n",
    "    remainder = [[[str(0) for _ in range(nChannels)] for _ in range(nCols)] for _ in range(nRows)]\n",
    "    for i in range(nRows):\n",
    "        for j in range(nCols):\n",
    "            for k in range(nChannels):\n",
    "                out[i][j][k] = int(int(X_in[i][j][k]) * int(a_in[k]) + int(b_in[k]))\n",
    "                remainder[i][j][k] = str(int(out[i][j][k]) % n)\n",
    "                out[i][j][k] = int(out[i][j][k] // n)\n",
    "                out_str[i][j][k] = str(out[i][j][k] % p)\n",
    "    return X, A, B, out_str, out, remainder\n",
    "\n",
    "def AveragePooling2DInt (nRows, nCols, nChannels, poolSize, strides, input):\n",
    "    Input = [[[str(input[i][j][k] % p) for k in range(nChannels)] for j in range(nCols)] for i in range(nRows)]\n",
    "    out = [[[0 for _ in range(nChannels)] for _ in range((nCols-poolSize)//strides + 1)] for _ in range((nRows-poolSize)//strides + 1)]\n",
    "    out_str = [[[str(0) for _ in range(nChannels)] for _ in range((nCols-poolSize)//strides + 1)] for _ in range((nRows-poolSize)//strides + 1)]\n",
    "    remainder = [[[None for _ in range(nChannels)] for _ in range((nCols-poolSize)//strides + 1)] for _ in range((nRows-poolSize)//strides + 1)]\n",
    "    for i in range((nRows-poolSize)//strides + 1):\n",
    "        for j in range((nCols-poolSize)//strides + 1):\n",
    "            for k in range(nChannels):\n",
    "                for x in range(poolSize):\n",
    "                    for y in range(poolSize):\n",
    "                        out[i][j][k] += input[i*strides+x][j*strides+y][k]\n",
    "                remainder[i][j][k] = str(out[i][j][k] % poolSize**2 % p)\n",
    "                out[i][j][k] = int(int(out[i][j][k]) // poolSize**2)\n",
    "                out_str[i][j][k] = str(out[i][j][k] % p)\n",
    "    return Input, out_str, out, remainder\n",
    "    \n",
    "def DenseInt(nInputs, nOutputs, n, input, weights, bias):\n",
    "    Input = [str(int(input[i]) % p) for i in range(nInputs)]\n",
    "    Weights = [[str(int(weights[i][j]) % p) for j in range(nOutputs)] for i in range(nInputs)]\n",
    "    Bias = [str(int(bias[i]) % p) for i in range(nOutputs)]\n",
    "    out = [0 for _ in range(nOutputs)]\n",
    "    out_str = [str(0) for _ in range(nOutputs)]\n",
    "    remainder = [None for _ in range(nOutputs)]\n",
    "    for j in range(nOutputs):\n",
    "        for i in range(nInputs):\n",
    "            out[j] += int(input[i]) * int(weights[i][j])\n",
    "        out[j] += int(bias[j])\n",
    "        remainder[j] = str(int(out[j]) % n)\n",
    "        out[j] = int(int(out[j]) // n % p)\n",
    "        out_str[j] = str(int(out[j]) % p)\n",
    "    return Input, Weights, Bias, out_str, out, remainder\n",
    "        \n",
    "\n",
    "def PaddedDepthwiseConv(nRows, nCols, nChannels, nFilters, kernelSize, strides, n, input, weights, bias):\n",
    "    \"\"\"output is padded\"\"\"\n",
    "    assert(nFilters % nChannels == 0)\n",
    "    outRows = (nRows - kernelSize)//strides + 1\n",
    "    outCols = (nCols - kernelSize)//strides + 1\n",
    "    \n",
    "    # out = np.zeros((outRows, outCols, nFilters))\n",
    "    Input = [[[str(int(input[i][j][k]) % p) for k in range(nChannels)] for j in range(nCols)] for i in range(nRows)]\n",
    "    Weights = [[[str(int(weights[i][j][k]) % p) for k in range(weights.shape[2])] for j in range(weights.shape[1])] for i in range(weights.shape[0])]\n",
    "    out = [[[0 for _ in range(nFilters)] for _ in range(nCols)] for _ in range(nRows)]\n",
    "    Bias = bias.round().astype(int).astype(str).tolist()\n",
    "    out_str = [[[str(0) for _ in range(nFilters)] for _ in range(nCols)] for _ in range(nRows)]\n",
    "    remainder = [[[str(0) for _ in range(nFilters)] for _ in range(nCols)] for _ in range(nRows)]\n",
    "    \n",
    "    for row in range(outRows):\n",
    "        for col in range(outCols):\n",
    "            for channel in range(nChannels):\n",
    "                for x in range(kernelSize):\n",
    "                    for y in range(kernelSize):\n",
    "                        out[row+1][col+1][channel] += int(input[row*strides+x][col*strides+y][channel]) * int(weights[x, y, channel])\n",
    "                \n",
    "                out[row+1][col+1][channel] += int(bias[channel])\n",
    "                remainder[row+1][col+1][channel] = str(int(out[row+1][col+1][channel] % n))\n",
    "                out[row+1][col+1][channel] = int(out[row+1][col+1][channel] // n)\n",
    "                out_str[row+1][col+1][channel] = str(out[row+1][col+1][channel] % p)\n",
    "                            \n",
    "    return Input, Weights, Bias, out_str, out, remainder\n",
    "\n",
    "def PointwiseConv2d(nRows, nCols, nChannels, nFilters, strides, n, input, weights, bias):\n",
    "    kernelSize = 1\n",
    "    outRows = (nRows - kernelSize)//strides + 1\n",
    "    outCols = (nCols - kernelSize)//strides + 1\n",
    "    \n",
    "    Input = [[[str(int(input[i][j][k]) % p) for k in range(nChannels)] for j in range(nCols)] for i in range(nRows)]\n",
    "    Weights = [[str(int(weights[i][j]) % p)for j in range(weights.shape[1])] for i in range(weights.shape[0])]\n",
    "    out = [[[0 for _ in range(nFilters)] for _ in range(nCols)] for _ in range(nRows)]\n",
    "    Bias = bias.round().astype(int).astype(str).tolist()\n",
    "    out_str = [[[str(0) for _ in range(nFilters)] for _ in range(nCols)] for _ in range(nRows)]\n",
    "    remainder = [[[str(0) for _ in range(nFilters)] for _ in range(nCols)] for _ in range(nRows)]\n",
    "    \n",
    "    for row in range(outRows):\n",
    "        for col in range(outCols):\n",
    "            for filter in range(nFilters):\n",
    "                for k in range(nChannels):\n",
    "                    out[row][col][filter] += int(input[row*strides][col*strides][k]) * int(weights[k, filter])\n",
    "                    \n",
    "                out[row][col][filter] += int(bias[filter])\n",
    "                remainder[row][col][filter] = str(int(out[row][col][filter] % n))\n",
    "                out[row][col][filter] = int(out[row][col][filter] // n)\n",
    "                out_str[row][col][filter] = str(out[row][col][filter] % p)\n",
    "                            \n",
    "    return Input, Weights, Bias, out_str, out, remainder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "faaa5941-253c-4676-94bd-c4dfbb1e524f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetWrapper(torch.utils.data.Dataset):\n",
    "    def __init__(self, subset, transform=None):\n",
    "        self.subset = subset\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        x, y = self.subset[index]\n",
    "        \n",
    "        if self.transform:\n",
    "            x = self.transform(x)\n",
    "        return x, y\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "622df7cd-d2f1-40bd-b801-27df6c3b0cdf",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "    # transforms.Resize((28, 28)),\n",
    "])\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(\n",
    "    root='./CIFAR10', train=False, download=True, transform=transform)\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(\n",
    "    root='./CIFAR10', train=True, download=True)\n",
    "\n",
    "# split the train set into train/validation\n",
    "train_set_size = int(len(trainset) * 0.8)\n",
    "valid_set_size = len(trainset) - train_set_size\n",
    "\n",
    "seed = torch.Generator().manual_seed(42)\n",
    "trainset, validset = torch.utils.data.random_split(trainset, [train_set_size, valid_set_size], generator=seed)\n",
    "\n",
    "trainset = DatasetWrapper(trainset, transform)\n",
    "validset = DatasetWrapper(validset, transform)\n",
    "\n",
    "# Create train dataloader\n",
    "trainloader = torch.utils.data.DataLoader(\n",
    "    trainset, batch_size=512, shuffle=True, num_workers=24)\n",
    "\n",
    "# Create validation dataloader\n",
    "validloader = torch.utils.data.DataLoader(\n",
    "    validset, batch_size=512, shuffle=False, num_workers=24)\n",
    "\n",
    "# Create test dataloader\n",
    "testloader = torch.utils.data.DataLoader(\n",
    "    testset, batch_size=512, shuffle=False, num_workers=24)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat', 'deer',\n",
    "           'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5556731b-af12-4e77-b63b-7d26c3d46f6a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0ef5c968-2309-4bcb-a680-9450536ef079",
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.03125\n",
      "IMAGE SHAPE:  torch.Size([3, 32, 32])\n",
      "IMAGE SHAPE:  torch.Size([1, 3, 32, 32])\n",
      "Predicted plane - idx: 0\n"
     ]
    }
   ],
   "source": [
    "MODEL_WEIGHTS_PATH = \"./checkpoints/no_padding_100epochs.pth\", \n",
    "class Test(ZkMobileNet):\n",
    "    def forward(self, x):\n",
    "        # print(\"STARTING SHAPE: \", x.shape)\n",
    "        x = self.relu(self.bn(self.conv(x)))\n",
    "        # print(\"CONV1 SHAPE: \", x.shape)\n",
    "        x = self.features(x)\n",
    "        # x = self.relu(x)\n",
    "        # print(\"BACKBONE SHAPE: \", x.shape)\n",
    "        # print(\"PRE AVG-POOL SHAPE: \", x.shape)\n",
    "        x = F.avg_pool2d(x, 6)\n",
    "        # print(\"POST AVG-POOL SHAPE: \", x.shape)\n",
    "        x = x.view(x.size()[0], -1)\n",
    "        # print(\"PRE-CLASSIFIER SHAPE: \", x.shape)\n",
    "        x = self.linear(x)\n",
    "        # print(\"POST-CLASSIFIER SHAPE: \", x.shape)\n",
    "        return x\n",
    "        \n",
    "# alpha = (0.25 * 0.5 * 0.75)\n",
    "alpha = (0.25 * 0.125)\n",
    "# alpha = 0.25\n",
    "print(alpha)\n",
    "model = Test(trainloader, num_classes=10, alpha=alpha, max_epochs=100)\n",
    "\n",
    "saved = torch.load(\"./checkpoints/no_padding_100epochs.pth\")\n",
    "# model.load_state_dict(saved['state_dict'])\n",
    "# model.load_state_dict(saved['net'])\n",
    "model.eval()\n",
    "\n",
    "image, label = validset[0]\n",
    "print(\"IMAGE SHAPE: \", image.shape)\n",
    "image = image.unsqueeze(0)\n",
    "print(\"IMAGE SHAPE: \", image.shape)\n",
    "logits = model(image)\n",
    "pred_idx = logits.argmax()\n",
    "\n",
    "print(f\"Predicted {classes[pred_idx]} - idx: {pred_idx}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e0ac086c-7186-4e55-ad9b-706b60aaab06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.bn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea13b7cc-41ef-4127-b1fd-99ffcc4723b8",
   "metadata": {},
   "source": [
    "# Testing head layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a30b6454-aa6b-4962-a3a0-867824a5a100",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'quantized_in' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m np\u001b[38;5;241m.\u001b[39marray(\u001b[43mquantized_in\u001b[49m)\u001b[38;5;241m.\u001b[39mshape\n",
      "\u001b[0;31mNameError\u001b[0m: name 'quantized_in' is not defined"
     ]
    }
   ],
   "source": [
    "np.array(quantized_in).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fbe4fd19-94b7-4b11-9dac-e308c523b7a8",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "# CONVOLUTION LAYER\n",
    "weights = model.conv.weight.detach().numpy().transpose(2, 3, 1, 0)\n",
    "image, label = testset[0]\n",
    "\n",
    "expected = model.conv(image).detach().numpy()\n",
    "bias = torch.zeros(weights.shape[3]).numpy()\n",
    "\n",
    "# padded = pad(image, 1).transpose(1,2,0)\n",
    "padded = F.pad(image, (1,1,1,1), \"constant\", 0).numpy()\n",
    "padded = padded.transpose(1,2,0)\n",
    "\n",
    "quantized_image = (padded * 10**EXPONENT).round()\n",
    "quantized_weights = (weights * 10**EXPONENT).round() # .transpose(0, 3, 1, 0) # [nFilters, nChannels, H, W] -> \n",
    "\n",
    "circuit_in, circuit_conv_weights, circuit_conv_bias, circuit_conv_out, circuit_conv_remainder = Conv2DInt(34, 34, 3, 1, 3, 1, 10**EXPONENT, quantized_image, quantized_weights, bias)\n",
    "# output, remainder = Conv2DInt(34, 34, 3, 8, 3, 1, 10**EXPONENT, quantized_image, quantized_weights, bias)\n",
    "# test_output = output / 10**(EXPONENT)\n",
    "test_output = [[[from_circom(int(out)) / 10**EXPONENT for out in asdf] for asdf in asdfasdf] for asdfasdf in circuit_conv_out]\n",
    "\n",
    "expected = expected.transpose((1, 2, 0))\n",
    "\n",
    "assert(np.allclose(test_output, expected, atol=1e-6))\n",
    "\n",
    "# BATCH NORM CONSTANTS\n",
    "gamma = model.bn.weight\n",
    "beta = model.bn.bias\n",
    "mean = model.bn.running_mean\n",
    "var = model.bn.running_var\n",
    "eps = model.bn.eps\n",
    "\n",
    "a = (gamma/(var+eps)**.5).detach()\n",
    "b = (beta-gamma*mean/(var+eps)**.5).detach().tolist()\n",
    "\n",
    "quantized_a = (a * 10**EXPONENT).tolist()\n",
    "quantized_b = [bi * 10**(2*EXPONENT) for bi in b]\n",
    "\n",
    "# BATCH NORM USING PYTORCH OUTPUT\n",
    "image, label = testset[0]\n",
    "image = image.unsqueeze(0)\n",
    "out = model.conv(image)\n",
    "expected = model.bn(out)\n",
    "\n",
    "expected = torch.permute(expected.squeeze(0), (1, 2, 0))\n",
    "\n",
    "quantized_in = torch.permute(out.squeeze(0), (1, 2, 0)) * 10**EXPONENT\n",
    "\n",
    "X, A, B, out_str, actual, remainder = BatchNormalizationInt(32, 32, 1, 10**EXPONENT, quantized_in, quantized_a, quantized_b)\n",
    "test_output = [[[from_circom(int(out)) / 10**EXPONENT for out in vec] for vec in matrix] for matrix in actual]\n",
    "assert(torch.allclose(torch.Tensor(test_output), expected, atol=1e-6))\n",
    "\n",
    "# BATCH NORM USING CIRCUIT CONV OUTPUT\n",
    "quantized_in = [[[from_circom(int(out)) for out in vec] for vec in matrix] for matrix in circuit_conv_out]\n",
    "\n",
    "_, circuit_bn_a, circuit_bn_b, out_str, circuit_bn_out, circuit_bn_remainder = BatchNormalizationInt(32, 32, 1, 10**EXPONENT, quantized_in, quantized_a, quantized_b)\n",
    "\n",
    "test_output = [[[int(from_circom(int(out))) / 10**EXPONENT for out in vec] for vec in matrix] for matrix in actual]\n",
    "\n",
    "assert(torch.allclose(torch.Tensor(test_output), expected, atol=1e-6))\n",
    "\n",
    "# RELU USING CIRCUIT OUTPUT\n",
    "relu_in = [[[to_circom(bn_out) if bn_out < MAX_POSITIVE else 0 for bn_out in vec] for vec in matrix] for matrix in circuit_bn_out]\n",
    "relu_out = [[[str(bn_out) if bn_out < MAX_POSITIVE else 0 for bn_out in vec] for vec in matrix] for matrix in relu_in]\n",
    "\n",
    "input_json_path = \"head_input.json\"\n",
    "with open(input_json_path, \"w\") as input_file:\n",
    "    json.dump({\"in\": circuit_in,\n",
    "               \"conv2d_weights\": circuit_conv_weights,\n",
    "               \"conv2d_bias\": circuit_conv_bias,\n",
    "               \"conv2d_out\": circuit_conv_out,\n",
    "               \"conv2d_remainder\": circuit_conv_remainder,\n",
    "               \n",
    "               \"bn_a\": circuit_bn_a,\n",
    "               \"bn_b\": circuit_bn_b,\n",
    "               \"bn_out\": circuit_bn_out,\n",
    "               \"bn_remainder\": circuit_bn_remainder,\n",
    "               \n",
    "               \"relu_out\": relu_out,\n",
    "               }, input_file)\n",
    "\n",
    "os.chdir(\"circuits\")\n",
    "!./head/head_cpp/head ../head_input.json head.wtns\n",
    "# !npx snarkjs groth16 prove head/circuit_final.zkey head.wtns proof.json public_test.json\n",
    "os.chdir(\"../\")\n",
    "\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9812bfb-62fb-4a46-95ab-6253fb8ecd6d",
   "metadata": {},
   "source": [
    "# Generating input for backbone layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "5b7d7f64-c2c6-420d-aae5-084443e83cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONVOLUTION LAYER\n",
    "weights = model.conv.weight.detach().numpy().transpose(2, 3, 1, 0)\n",
    "image, label = testset[0]\n",
    "\n",
    "expected = model.conv(image).detach().numpy()\n",
    "bias = torch.zeros(weights.shape[3]).numpy()\n",
    "\n",
    "# padded = pad(image, 1).transpose(1,2,0)\n",
    "padded = F.pad(image, (1,1,1,1), \"constant\", 0).numpy()\n",
    "padded = padded.transpose(1,2,0)\n",
    "\n",
    "quantized_image = (padded * 10**EXPONENT).round()\n",
    "quantized_weights = (weights * 10**EXPONENT).round() # .transpose(0, 3, 1, 0) # [nFilters, nChannels, H, W] -> \n",
    "\n",
    "circuit_in, circuit_conv_weights, circuit_conv_bias, circuit_conv_out, circuit_conv_remainder = Conv2DInt(34, 34, 3, 1, 3, 1, 10**EXPONENT, quantized_image, quantized_weights, bias)\n",
    "# output, remainder = Conv2DInt(34, 34, 3, 8, 3, 1, 10**EXPONENT, quantized_image, quantized_weights, bias)\n",
    "# test_output = output / 10**(EXPONENT)\n",
    "test_output = [[[from_circom(int(out)) / 10**EXPONENT for out in asdf] for asdf in asdfasdf] for asdfasdf in circuit_conv_out]\n",
    "\n",
    "expected = expected.transpose((1, 2, 0))\n",
    "\n",
    "assert(np.allclose(test_output, expected, atol=1e-6))\n",
    "\n",
    "# BATCH NORM CONSTANTS\n",
    "gamma = model.bn.weight\n",
    "beta = model.bn.bias\n",
    "mean = model.bn.running_mean\n",
    "var = model.bn.running_var\n",
    "eps = model.bn.eps\n",
    "\n",
    "a = (gamma/(var+eps)**.5).detach()\n",
    "b = (beta-gamma*mean/(var+eps)**.5).detach().tolist()\n",
    "\n",
    "quantized_a = (a * 10**EXPONENT).tolist()\n",
    "quantized_b = [bi * 10**(2*EXPONENT) for bi in b]\n",
    "\n",
    "# BATCH NORM USING PYTORCH OUTPUT\n",
    "image, label = testset[0]\n",
    "image = image.unsqueeze(0)\n",
    "out = model.conv(image)\n",
    "expected = model.bn(out)\n",
    "\n",
    "expected = torch.permute(expected.squeeze(0), (1, 2, 0))\n",
    "\n",
    "quantized_in = torch.permute(out.squeeze(0), (1, 2, 0)) * 10**EXPONENT\n",
    "\n",
    "X, A, B, _, actual, remainder = BatchNormalizationInt(32, 32, 1, 10**EXPONENT, quantized_in, quantized_a, quantized_b)\n",
    "test_output = [[[from_circom(int(out)) / 10**EXPONENT for out in vec] for vec in matrix] for matrix in actual]\n",
    "assert(torch.allclose(torch.Tensor(test_output), expected, atol=1e-6))\n",
    "\n",
    "# BATCH NORM USING CIRCUIT CONV OUTPUT\n",
    "quantized_in = [[[from_circom(int(out)) for out in vec] for vec in matrix] for matrix in circuit_conv_out]\n",
    "\n",
    "_, circuit_bn_a, circuit_bn_b, _, circuit_bn_out, circuit_bn_remainder = BatchNormalizationInt(32, 32, 1, 10**EXPONENT, quantized_in, quantized_a, quantized_b)\n",
    "\n",
    "test_output = [[[int(from_circom(int(out))) / 10**EXPONENT for out in vec] for vec in matrix] for matrix in circuit_bn_out]\n",
    "\n",
    "assert(torch.allclose(torch.Tensor(test_output), expected, atol=1e-6))\n",
    "\n",
    "# RELU USING CIRCUIT OUTPUT\n",
    "relu_in = [[[to_circom(bn_out) if bn_out < MAX_POSITIVE else 0 for bn_out in vec] for vec in matrix] for matrix in circuit_bn_out]\n",
    "relu_out = [[[str(bn_out) if bn_out < MAX_POSITIVE else 0 for bn_out in vec] for vec in matrix] for matrix in relu_in]\n",
    "\n",
    "image, label = testset[0]\n",
    "image = image.unsqueeze(0)\n",
    "conv_output = model.conv(image)\n",
    "bn_output = model.bn(conv_output)\n",
    "relu_expected = model.relu(bn_output).squeeze(0).detach().numpy().transpose((1,2,0))\n",
    "\n",
    "test_output = [[[out / 10**EXPONENT if out > 0 else 0 for out in vec] for vec in matrix] for matrix in circuit_bn_out]\n",
    "\n",
    "assert(np.allclose(test_output, relu_expected, atol=1e-6))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20a0dfae-0bda-408c-b58c-4833237b881f",
   "metadata": {},
   "source": [
    "# Auxiliary functions for "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "34c9e624-1d0e-4772-b2fd-be0064480030",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Optional, Union\n",
    "\n",
    "def circom2pytorch(circuit_output):\n",
    "    formatted = torch.Tensor([[[int(out) / 10**EXPONENT if out > 0 else 0 for out in vec] for vec in matrix] for matrix in circuit_output])\n",
    "    formatted = torch.permute(formatted, (2, 0, 1)).unsqueeze(0)\n",
    "    return formatted\n",
    "    \n",
    "def pytorch2quantized(pytorch_output: torch.Tensor):\n",
    "    return pytorch_output.squeeze().detach().numpy().transpose((1, 2, 0)) * 10**EXPONENT\n",
    "\n",
    "def dequantize(input: List[List[List[int]]], padding: int=1, channel_padding: Optional[int]=None):\n",
    "    test_output = np.array([[[int(value) / 10**EXPONENT for value in vec] for vec in matrix] for matrix in input])\n",
    "    \n",
    "    if channel_padding is None or channel_padding == 0:\n",
    "        return test_output[padding:-padding, padding:-padding, :]\n",
    "    \n",
    "    return test_output[padding:-padding, padding:-padding, :channel_padding]\n",
    "    \n",
    "def check_quantized_input(quantized_input):\n",
    "    \"\"\"quantized_input should be quantized and should be (Height, Depth, Channels)\"\"\"\n",
    "    assert(len(np.array(quantized_input).shape) == 3)\n",
    "\n",
    "def check_pytorch_input(quantized_input):\n",
    "    \"\"\"pytorch_input should be quantized and should be (N=1, Channels, Height, Depth)\"\"\"\n",
    "    assert(len(quantized_input.shape) == 4)\n",
    "    assert(quantized_input.shape[0] == 1)\n",
    "    assert(type(quantized_input) == torch.Tensor)\n",
    "    \n",
    "def pad(cube: List[List[List[int]]], square_pad: int, channel_pad: int):\n",
    "    max_i = len(cube) + square_pad*2\n",
    "    max_j = len(cube[0]) + square_pad*2\n",
    "    max_k = len(cube[0][0]) + channel_pad\n",
    "    result = [[[0 for k in range(max_k)] for j in range(max_j)] for i in range(max_i)]\n",
    "    \n",
    "    for i in range(len(cube) + square_pad*2):\n",
    "        for j in range(len(cube[0]) + square_pad*2):\n",
    "            for k in range(len(cube[0][0]) + channel_pad):\n",
    "                if i >= square_pad and i < len(cube) + square_pad and j >= square_pad and j < len(cube[0]) + square_pad and k < len(cube[0][0]):\n",
    "                    # print(f\"{i-square_pad=}, {j-square_pad=}, {k=} {channel_pad=} {len(cube[0][0])=} {channel_pad - len(cube[0][0])=}\")\n",
    "                    result[i][j][k] = cube[i-square_pad][j-square_pad][k]\n",
    "\n",
    "    # print(f\"{max_i=}, {max_j=}, {max_k=}\")\n",
    "                \n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "4f198400-2205-4415-8b9e-3e13094775d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "\n",
    "class ArbBaseModel(BaseModel):\n",
    "    class Config:\n",
    "        arbitrary_types_allowed = True\n",
    "\n",
    "class CircuitConvInput(BaseModel):\n",
    "    input: Optional[List[List[List[str]]]]\n",
    "    weights: Union[List[List[List[str]]], List[List[str]]]\n",
    "    bias: List[str]\n",
    "    out_str: List[List[List[str]]]\n",
    "    remainder: List[List[List[str]]]\n",
    "    \n",
    "    out: List[List[List[int]]]\n",
    "    \n",
    "    \n",
    "class CircuitBatchNormInput(BaseModel):\n",
    "    input: Optional[List[List[List[str]]]]\n",
    "    a: List[str]\n",
    "    b: List[str]\n",
    "    out_str: List[List[List[str]]]\n",
    "    out: List[List[List[int]]]\n",
    "    remainder: List[List[List[str]]]\n",
    "\n",
    "\n",
    "class ConvBN(BaseModel):\n",
    "    conv: CircuitConvInput\n",
    "    bn: CircuitBatchNormInput\n",
    "    \n",
    "    def to_dict(self, prefix: str):\n",
    "        return {\n",
    "            f\"{prefix}conv_weights\": self.conv.weights,\n",
    "            f\"{prefix}conv_bias\": self.conv.bias,\n",
    "            f\"{prefix}conv_remainder\": self.conv.remainder,\n",
    "            f\"{prefix}conv_out\": self.conv.out_str,\n",
    "            \n",
    "            f\"{prefix}bn_a\": self.bn.a,\n",
    "            f\"{prefix}bn_b\": self.bn.b,\n",
    "            f\"{prefix}bn_remainder\": self.bn.remainder,\n",
    "            f\"{prefix}bn_out\": self.bn.out_str,\n",
    "        }\n",
    "\n",
    "class CircuitLayerInput(BaseModel):\n",
    "    depthwise: ConvBN\n",
    "    pointwise: ConvBN\n",
    "\n",
    "    def input(self):\n",
    "        return self.depthwise.conv.input\n",
    "        \n",
    "    def out(self):\n",
    "        return self.pointwise.bn.out\n",
    "    \n",
    "    def to_dict(self, prefix: str):\n",
    "        return {\n",
    "            **self.depthwise.to_dict(f\"{prefix}dw_\"),\n",
    "            **self.pointwise.to_dict(f\"{prefix}pw_\")\n",
    "        }\n",
    "    def to_json(self, prefix: str, json_path: str):\n",
    "        with open(json_path, \"w\") as input_file:\n",
    "            json.dump(self.to_dict(prefix), input_file)\n",
    "            \n",
    "def TypedPaddedDepthwiseConv(nRows, nCols, nChannels, nFilters, kernelSize, strides, n, input, weights, bias):\n",
    "    Input, Weights, Bias, out_str, out, remainder = PaddedDepthwiseConv(nRows, nCols, nChannels, nFilters, kernelSize, strides, n, input, weights, bias)\n",
    "    return CircuitConvInput(\n",
    "        input=Input, \n",
    "        weights=Weights, \n",
    "        bias=Bias, \n",
    "        out_str=out_str, \n",
    "        out=out, \n",
    "        remainder=remainder, \n",
    "    )\n",
    "    \n",
    "def BatchNormalizationPadded(nRows, nCols, nChannels, n, X_in, a_in, b_in, padding):\n",
    "    p = CIRCOM_PRIME\n",
    "    X = [[[str(int(X_in[i][j][k]) % p) for k in range(nChannels)] for j in range(nCols)] for i in range(nRows)]\n",
    "    A = [str(int(a_in[k]) % p) for k in range(nChannels)]\n",
    "    B = [str(int(b_in[k]) % p) for k in range(nChannels)]\n",
    "    out = [[[0 for _ in range(nChannels)] for _ in range(nCols)] for _ in range(nRows)]\n",
    "    out_str = [[[str(0) for _ in range(nChannels)] for _ in range(nCols)] for _ in range(nRows)]\n",
    "    remainder = [[[str(n) for _ in range(nChannels)] for _ in range(nCols)] for _ in range(nRows)]\n",
    "    for i in range(padding, nRows-padding):\n",
    "        for j in range(padding, nCols-padding):\n",
    "            for k in range(nChannels):\n",
    "                out[i][j][k] = int(int(X_in[i][j][k]) * int(a_in[k]) + int(b_in[k]))\n",
    "                remainder[i][j][k] = str(int(out[i][j][k]) % n)\n",
    "                out[i][j][k] = int(out[i][j][k] // n)\n",
    "                out_str[i][j][k] = str(out[i][j][k] % p)\n",
    "    return X, A, B, out_str, out, remainder\n",
    "    \n",
    "def TypedBatchNormalizationInt(nRows, nCols, nChannels, n, X_in, a_in, b_in, padding):\n",
    "    X, A, B, out_str, out, remainder = BatchNormalizationPadded(nRows, nCols, nChannels, n, X_in, a_in, b_in, padding)\n",
    "    return CircuitBatchNormInput(\n",
    "        input=X, \n",
    "        a=A, \n",
    "        b=B, \n",
    "        out_str=out_str, \n",
    "        out=out, \n",
    "        remainder=remainder, \n",
    "    )\n",
    "    \n",
    "def TypedPointwiseConv2d(nRows, nCols, nChannels, nFilters, strides, n, input, weights, bias):\n",
    "    Input, Weights, Bias, out_str, out, remainder = PointwiseConv2d(nRows, nCols, nChannels, nFilters, strides, n, input, weights, bias)\n",
    "    return CircuitConvInput(\n",
    "        input=Input, \n",
    "        weights=Weights, \n",
    "        bias=Bias, \n",
    "        out_str=out_str, \n",
    "        out=out, \n",
    "        remainder=remainder, \n",
    "    )\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "8203d7d9-8fac-4f99-b7c0-ce9738f5578e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting proper inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "94b4067b-0823-4f70-a8b0-f68a60836e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "quantized_input = torch.Tensor([[[out if out > 0 else 0 for out in vec] for vec in matrix] for matrix in circuit_bn_out])\n",
    "pytorch_input = torch.Tensor([[[int(out) / 10**EXPONENT if out > 0 else 0 for out in vec] for vec in matrix] for matrix in circuit_bn_out])\n",
    "pytorch_input = torch.permute(pytorch_input, (2, 0, 1)).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "8a99c08e-0fdb-4ef9-9af5-4820cb1b5e06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input shape:  torch.Size([1, 1, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "class CircuitBackbone():\n",
    "    def __init__(self, model: ZkMobileNet, max_dims: List[int]):\n",
    "        self.model = model\n",
    "        self.model.eval()\n",
    "        self.max_rows = max_dims[0]\n",
    "        self.max_cols = max_dims[1]\n",
    "        self.max_channels = max_dims[2]\n",
    "        self.max_depth_filters = self.max_point_filters = max_dims[2]\n",
    "        self.dw_kernel_size = 3\n",
    "        self.stride = 1\n",
    "        self.scalar_factor = 10**EXPONENT\n",
    "\n",
    "    def _forward_module(self, module: nn.Module, input: torch.Tensor):\n",
    "        output = module(input)\n",
    "        expected = output.squeeze(0).detach().numpy().transpose((1,2,0))\n",
    "        return output, expected\n",
    "        \n",
    "    def _circuit_conv_bn(self, layer: int, pytorch_input: torch.Tensor, quantized_input: List[List[List[int]]], conv: nn.Sequential):\n",
    "        conv_output, conv_expected = self._forward_module(conv[0], pytorch_input)\n",
    "        \n",
    "        circuit_conv_input = self._get_conv_circuit_input(layer, quantized_input, conv_expected, conv[0])\n",
    "        \n",
    "        bn_output, bn_expected = self._forward_module(conv[1], conv_output)\n",
    "        \n",
    "        circuit_bn_input = self._get_bn_circuit_input(layer, circuit_conv_input.out, bn_expected, conv[1])\n",
    "        \n",
    "        return ConvBN(conv=circuit_conv_input, bn=circuit_bn_input), bn_output\n",
    "\n",
    "    def circuit_layer_inputs(self, layer: int, pytorch_input: torch.Tensor, quantized_input: List[List[List[int]]]):\n",
    "        check_pytorch_input(pytorch_input)\n",
    "        check_quantized_input(quantized_input)\n",
    "\n",
    "        depthwise, dw_output = self._circuit_conv_bn(layer, pytorch_input, quantized_input, self.model.features[layer].dw_conv)\n",
    "        pointwise, pw_output = self._circuit_conv_bn(layer, dw_output, depthwise.bn.out, self.model.features[layer].pw_conv)\n",
    "\n",
    "        layer_input = CircuitLayerInput(depthwise=depthwise, pointwise=pointwise)\n",
    "\n",
    "        layer_input.to_json(f\"l{layer}\", \"help.json\")\n",
    "        return layer_input, pw_output\n",
    "\n",
    "    def _get_conv_circuit_input(self, layer: int, quantized_input: List[List[List[int]]], expected: np.array, module: nn.Conv2d) -> CircuitConvInput:\n",
    "        check_quantized_input(quantized_input)\n",
    "\n",
    "        padding = layer+1\n",
    "        weights = module.weight.detach()\n",
    "        \n",
    "        if module.kernel_size == (3, 3): \n",
    "            filter_padding = self.max_depth_filters - weights.shape[0]\n",
    "            assert(filter_padding >= 0)\n",
    "            \n",
    "            bias = np.zeros(weights.shape[0] + filter_padding)\n",
    "\n",
    "            weights = weights.squeeze(1)\n",
    "            weights = torch.permute(weights, (1, 2, 0))\n",
    "            assert(filter_padding >= 0)\n",
    "        \n",
    "            if len(quantized_input[0][0]) != self.max_depth_filters:\n",
    "                quantized_input = pad(quantized_input, 0, filter_padding)\n",
    "                # quantized_input = F.pad(quantized_input, (0, filter_padding), \"constant\", 0)\n",
    "            \n",
    "            padded_weights = F.pad(weights, (0, filter_padding), \"constant\", 0)\n",
    "            quantized_weights = padded_weights * 10**EXPONENT\n",
    "            \n",
    "            conv_input = TypedPaddedDepthwiseConv(\n",
    "                self.max_rows, \n",
    "                self.max_cols, \n",
    "                self.max_channels, \n",
    "                self.max_depth_filters, \n",
    "                self.dw_kernel_size, \n",
    "                self.stride, \n",
    "                self.scalar_factor,\n",
    "                quantized_input, \n",
    "                quantized_weights.round(), \n",
    "                bias\n",
    "            )\n",
    "            test_output = dequantize(conv_input.out, padding, self.max_point_filters - filter_padding)\n",
    "            \n",
    "        elif module.kernel_size == (1, 1):\n",
    "            channel_padding = self.max_depth_filters - weights.shape[1]\n",
    "            filter_padding = self.max_point_filters - weights.shape[0]\n",
    "            \n",
    "            assert(filter_padding >= 0)\n",
    "            bias = np.zeros(weights.shape[0] + filter_padding)\n",
    "\n",
    "            weights = weights.squeeze(-1).squeeze(-1) # removing H x W\n",
    "            weights = torch.permute(weights, (1, 0))\n",
    "            assert(filter_padding >= 0)\n",
    "        \n",
    "            if len(quantized_input[0][0]) != self.max_point_filters:\n",
    "                quantized_input = F.pad(quantized_input, (0, filter_padding), \"constant\", 0)\n",
    "                \n",
    "            assert(channel_padding >= 0)\n",
    "            padded_weights = F.pad(weights, (0, filter_padding, 0, channel_padding), \"constant\", 0)\n",
    "            quantized_weights = padded_weights * 10**EXPONENT\n",
    "            conv_input = TypedPointwiseConv2d(\n",
    "                self.max_rows, \n",
    "                self.max_cols, \n",
    "                self.max_depth_filters,\n",
    "                self.max_point_filters,\n",
    "                self.stride, \n",
    "                self.scalar_factor,\n",
    "                quantized_input,\n",
    "                quantized_weights.round(),\n",
    "                bias\n",
    "            )\n",
    "            \n",
    "            test_output = dequantize(conv_input.out, padding, self.max_point_filters - filter_padding)\n",
    "        \n",
    "        assert(np.allclose(expected, test_output, atol=1e-4))\n",
    "        \n",
    "        return conv_input\n",
    "        \n",
    "    def _get_bn_circuit_input(self, layer: int, quantized_input: List[List[List[int]]], expected: np.array, batch_norm: nn.modules.batchnorm.BatchNorm2d) -> CircuitBatchNormInput:\n",
    "        check_quantized_input(quantized_input)\n",
    "        \n",
    "        padding = layer+1\n",
    "        assert(len(expected.shape) == 3)\n",
    "        channel_padding = self.max_point_filters - expected.shape[2]\n",
    "       \n",
    "        gamma = batch_norm.weight\n",
    "        beta = batch_norm.bias\n",
    "        mean = batch_norm.running_mean\n",
    "        var = batch_norm.running_var\n",
    "        eps = batch_norm.eps\n",
    "        \n",
    "        a = (gamma/(var+eps)**.5).detach()\n",
    "        b = (beta-gamma*mean/(var+eps)**.5).detach()\n",
    "        \n",
    "        channel_padding = self.max_point_filters - len(a)\n",
    "        a = F.pad(a, (0, channel_padding), \"constant\", 0).tolist()\n",
    "        b = F.pad(b, (0, channel_padding), \"constant\", 0).tolist()\n",
    "        \n",
    "        quantized_a = [ai * 10**(EXPONENT) for ai in a]\n",
    "        quantized_b = [bi * 10**(2*EXPONENT) for bi in b]\n",
    "        \n",
    "        bn_input = TypedBatchNormalizationInt(\n",
    "            self.max_rows, \n",
    "            self.max_cols, \n",
    "            self.max_point_filters, \n",
    "            self.scalar_factor,\n",
    "            quantized_input,\n",
    "            quantized_a,\n",
    "            quantized_b,\n",
    "            padding=(layer+1)\n",
    "        )\n",
    "        test_output = dequantize(bn_input.out, padding, self.max_point_filters - channel_padding)\n",
    "\n",
    "        assert(np.allclose(test_output, expected, atol=1e-4))\n",
    "        return bn_input\n",
    "        \n",
    "print(\"input shape: \", pytorch_input.shape)\n",
    "circuit = CircuitBackbone(model, (32, 32, 32))\n",
    "circuit_layer_input, pytorch_output = circuit.circuit_layer_inputs(0, pytorch_input, quantized_input)\n",
    "\n",
    "image, label = testset[0]\n",
    "image = image.unsqueeze(0)\n",
    "conv_output = model.conv(image)\n",
    "bn_output = model.bn(conv_output)\n",
    "relu_expected = model.relu(bn_output)\n",
    "layer0_expected = model.features[0](relu_expected)\n",
    "\n",
    "assert(torch.allclose(pytorch_output, layer0_expected, atol=1e-5))\n",
    "\n",
    "padding = (np.array(circuit_layer_input.out()).shape[0] - expected.shape[0]) // 2\n",
    "channel_padding = np.array(circuit_layer_input.out()).shape[2] - expected.shape[2]\n",
    "test_output = dequantize(circuit_layer_input.out(), padding, circuit.max_point_filters - channel_padding)\n",
    "expected = layer0_expected.squeeze().detach().numpy().transpose((1,2,0))\n",
    "assert(np.allclose(test_output, expected, atol=1e-5))\n",
    "circuit_layer_input, pytorch_output = circuit.circuit_layer_inputs(1, pytorch_output, circuit_layer_input.out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "087b703c-6f2a-444f-a81c-1e369e6be60f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BackboneCircuitInput():\n",
    "    layers: List[CircuitLayerInput]\n",
    "    pytorch_outputs: List[torch.Tensor]\n",
    "    quantized_input: List[List[List[int]]]\n",
    "    n_layers: int\n",
    "    circuit: CircuitBackbone\n",
    "    \n",
    "    def __init__(self, model: ZkMobileNet, n_layers: int, pytorch_input: torch.Tensor, quantized_input: List[List[List[int]]]):\n",
    "        self.n_layers = n_layers\n",
    "        self.quantized_input = quantized_input\n",
    "        self.circuit = CircuitBackbone(model, (32, 32, 32))\n",
    "        # self.circuit = CircuitMobilenet(model, (32, 32, 64))\n",
    "        # self.circuit = CircuitMobilenet(model, (32, 32, 96))\n",
    "        circuit_layer_input, pytorch_output = self.circuit.circuit_layer_inputs(0, pytorch_input, quantized_input)\n",
    "        self.layers = [circuit_layer_input]\n",
    "        self.pytorch_outputs = [pytorch_output]\n",
    "        for layer in range(1, n_layers):\n",
    "            circuit_layer_input, pytorch_output = self.circuit.circuit_layer_inputs(layer, pytorch_output, circuit_layer_input.out())\n",
    "            self.layers.append(circuit_layer_input)\n",
    "            self.pytorch_outputs.append(pytorch_output)\n",
    "\n",
    "    def to_dict(self):\n",
    "        return {\n",
    "                \"inp\": self.layers[0].depthwise.conv.input,\n",
    "                \"backbone\": [self.layers[i].to_dict(\"\") for i in range(len(self.layers))],\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "cf358d91-8320-4955-93ce-b1fadd1e8cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "backbone = BackboneCircuitInput(model, len(model.features), pytorch_input, quantized_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "9ce52c80-8bfb-432c-ab56-1314c1680fcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BACKBONE STARTED\n",
      "STEP_IN     RESULT 999999678953308669596737357956888609944281556423122835049532631282419362239\n",
      "HASH OUTPUT RESULT 999999678953308669596737357956888609944281556423122835049532631282419362239\n",
      "WEIGHTS HASH RESULT 4878235444861024761285566989102471308086608624286814920624920793618467057715\n",
      "START\n",
      "dw_conv done\n",
      "depth batch norm done\n",
      "pw_conv done\n",
      "point batch norm done\n",
      "END\n",
      "LAYER DONE\n",
      "step_in[0] 0\n",
      "step_in[1] 999999678953308669596737357956888609944281556423122835049532631282419362239\n",
      "step_out[0] 15571199703095799184197421513773915810687785943782493479351762044398380016696\n",
      "step_out[1] 27838787996768650461281416050134280483843308848965692542946456243578455289310\n",
      "END\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "d = {\n",
    "        \"step_in\": [\"0\", \n",
    "                    \"999999678953308669596737357956888609944281556423122835049532631282419362239\"],\n",
    "        \"in\": backbone.layers[0].depthwise.conv.input,\n",
    "        **backbone.layers[0].to_dict(\"\")\n",
    "        # \"in\": backbone.layers[0].pointwise.bn.out_str,\n",
    "}\n",
    "\n",
    "json_path = \"layer_test.json\"\n",
    "with open(json_path, \"w\") as f:\n",
    "    json.dump(d, f)\n",
    "    \n",
    "os.chdir(\"circuits\")\n",
    "!./backbone/backbone_cpp/backbone ../layer_test.json layer_test.wtns\n",
    "# !npx snarkjs groth16 prove ./backbone/circuit_final.zkey layer_test.wtns layer_proof.json layer_public_test.json\n",
    "os.chdir(\"../\")\n",
    "\n",
    "print(\"OK\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae116a0f-210d-4c9d-a969-a6e603642910",
   "metadata": {},
   "source": [
    "# Testing Tail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "f9998375-22b4-4706-bf68-40bccf0eefdd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 32, 32, 32])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quantized_input = torch.Tensor(backbone.layers[-1].pointwise.bn.out)\n",
    "pytorch_input = torch.Tensor([[[int(out) / 10**EXPONENT if out > 0 else 0 for out in vec] for vec in matrix] for matrix in backbone.layers[-1].pointwise.bn.out])\n",
    "pytorch_input = torch.permute(pytorch_input, (2, 0, 1)).unsqueeze(0)\n",
    "pytorch_input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "39ed369f-1bf0-4258-977a-77d09d195673",
   "metadata": {},
   "outputs": [],
   "source": [
    "quantized_input = torch.Tensor(backbone.layers[-1].pointwise.bn.out)\n",
    "pytorch_input = torch.Tensor([[[int(out) / 10**EXPONENT if out > 0 else 0 for out in vec] for vec in matrix] for matrix in backbone.layers[-1].pointwise.bn.out])\n",
    "pytorch_input = torch.permute(pytorch_input, (2, 0, 1)).unsqueeze(0)\n",
    "pytorch_input.shape\n",
    "\n",
    "test_output = dequantize(backbone.layers[-1].pointwise.bn.out, 13, 0)\n",
    "backbone.pytorch_outputs[-1].shape\n",
    "\n",
    "expected = torch.permute(backbone.pytorch_outputs[-1].squeeze(0), (1, 2, 0))\n",
    "assert(np.allclose(expected.detach(), test_output, atol=1e-6))\n",
    "\n",
    "test_output = torch.Tensor(test_output.transpose((2, 0, 1))).unsqueeze(0)\n",
    "input_expected = F.avg_pool2d(test_output, 6)\n",
    "output_expected = F.avg_pool2d(backbone.pytorch_outputs[-1], 6)\n",
    "assert(np.allclose(output_expected.detach(), test_output, atol=1e-6))\n",
    "\n",
    "quantized_in = np.array(backbone.layers[-1].pointwise.bn.out)[13:-13, 13:-13, :]\n",
    "input, out_str, out, remainder = AveragePooling2DInt(6, 6, 32, 6, 1, quantized_in)\n",
    "test_output = [[[int(o) / 10**EXPONENT for o in vec] for vec in matrix] for matrix in out]\n",
    "expected = torch.permute(output_expected.squeeze(0), (1, 2, 0))\n",
    "test_output = np.array([[[int(from_circom(int(value))) / 10**EXPONENT for value in vec] for vec in matrix] for matrix in out])\n",
    "assert(np.allclose(test_output, expected.detach(), atol=1e-6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd68a70-9466-4173-9d10-4d14f6d2c017",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "450f8994-b39b-4581-8768-11e637e60a96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QUANTIZED_INPUT SHAPE :  (6, 6, 32)\n",
      "out shape:  (32,)\n",
      "remainder shape:  (1, 1, 32)\n",
      "bias shape:  (10,)\n",
      "WEIGHTS shape:  (32, 10)\n",
      "TAIL STARTED\n",
      "end pooling\n",
      "MIMC_INPUT HASH :  12454065012971934911155132310689290609488170040964420067405519009722742678190\n",
      "end!!\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "quantized_in = np.array(backbone.layers[-1].pointwise.bn.out)[13:-13, 13:-13, :]\n",
    "# quantized_in = quantized_in[13:-13]\n",
    "print(\"QUANTIZED_INPUT SHAPE : \", np.array(quantized_in).shape)\n",
    "# a = dequantize(backbone.layers[-1].pointwise.bn.out, 13, 0)\n",
    "input, out_str, out, remainder = AveragePooling2DInt(6, 6, 32, 6, 1, quantized_in)\n",
    "\n",
    "out = out[0]\n",
    "out = out[0]\n",
    "print(\"out shape: \", np.array(out).shape)\n",
    "print(\"remainder shape: \", np.array(remainder).shape)\n",
    "\n",
    "weights = model.linear.weight.detach().numpy().transpose((1,0))\n",
    "weights = weights * 10**EXPONENT\n",
    "bias = torch.zeros(weights.shape[1]).tolist()\n",
    "print(\"bias shape: \", np.array(bias).shape)\n",
    "# weights = model.conv.weight.detach().numpy().transpose(2, 3, 1, 0)\n",
    "\n",
    "print(\"WEIGHTS shape: \", weights.shape)\n",
    "dense_input, dense_weights, dense_bias, dense_out_str, dense_out, dense_remainder = DenseInt(32, 10, 10**EXPONENT, out, weights.round(), bias)\n",
    "# print(\"dense weights shape: \", dense_weights)\n",
    "d = {\n",
    "        \"step_in\": [\"0\", \n",
    "                    \"12454065012971934911155132310689290609488170040964420067405519009722742678190\"],\n",
    "        \"in\": backbone.layers[-1].pointwise.bn.out,\n",
    "        # \"in\": input,\n",
    "        \"avg_pool_out\": out,\n",
    "        \"avg_pool_remainder\": remainder,\n",
    "    \n",
    "        \"dense_weights\": dense_weights,\n",
    "        \"dense_bias\": dense_bias,\n",
    "        \"dense_out\": dense_out_str,\n",
    "        \"dense_remainder\": dense_remainder,\n",
    "        # \"pw_bn_remainder\": self.bn.remainder,\n",
    "}\n",
    "\n",
    "json_path = \"tail_test.json\"\n",
    "with open(json_path, \"w\") as f:\n",
    "    json.dump(d, f)\n",
    "    \n",
    "os.chdir(\"circuits\")\n",
    "!./tail/tail_cpp/tail ../tail_test.json tail.wtns\n",
    "# !npx snarkjs groth16 prove ./tail/circuit_final.zkey tail.witns tail_proof.json tail_public_test.json\n",
    "os.chdir(\"../\")\n",
    "\n",
    "print(\"OK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "2e3cb4da-4519-4be8-854d-978d8725db21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate nova backbone circuit input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "32c5c201-1a39-4942-9c7f-fcf4188cc0a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keys;  dict_keys(['inp', 'backbone'])\n",
      "keys;  dict_keys(['step_in', 'in', 'dw_conv_weights', 'dw_conv_bias', 'dw_conv_remainder', 'dw_conv_out', 'dw_bn_a', 'dw_bn_b', 'dw_bn_remainder', 'dw_bn_out', 'pw_conv_weights', 'pw_conv_bias', 'pw_conv_remainder', 'pw_conv_out', 'pw_bn_a', 'pw_bn_b', 'pw_bn_remainder', 'pw_bn_out'])\n",
      "BACKBONE STARTED\n",
      "STEP_IN     RESULT 999999678953308669596737357956888609944281556423122835049532631282419362239\n",
      "HASH OUTPUT RESULT 999999678953308669596737357956888609944281556423122835049532631282419362239\n",
      "WEIGHTS HASH RESULT 4878235444861024761285566989102471308086608624286814920624920793618467057715\n",
      "START\n",
      "dw_conv done\n",
      "depth batch norm done\n",
      "pw_conv done\n",
      "point batch norm done\n",
      "END\n",
      "LAYER DONE\n",
      "step_in[0] 0\n",
      "step_in[1] 999999678953308669596737357956888609944281556423122835049532631282419362239\n",
      "step_out[0] 15571199703095799184197421513773915810687785943782493479351762044398380016696\n",
      "step_out[1] 27838787996768650461281416050134280483843308848965692542946456243578455289310\n",
      "END\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "    \n",
    "d = {\n",
    "        \"inp\": backbone.layers[0].depthwise.conv.input,\n",
    "        # \"backbone\": [backbone.layers[0].to_dict(\"\"), backbone.layers[1].to_dict(\"\"), backbone.layers[2].to_dict(\"\")],\n",
    "        \"backbone\": [backbone.layers[i].to_dict(\"\") for i in range(len(backbone.layers))],\n",
    "        # **backbone.layers[0].to_dict(\"\"),\n",
    "}\n",
    "\n",
    "print(\"keys; \", d.keys())\n",
    "json_path = \"nova_backbone_input.json\"\n",
    "with open(json_path, \"w\") as f:\n",
    "    json.dump(d, f)\n",
    "    \n",
    "d = {\n",
    "        \"step_in\": [\"0\", \n",
    "                    \"999999678953308669596737357956888609944281556423122835049532631282419362239\"],\n",
    "        \"in\": d[\"inp\"],\n",
    "        **d[\"backbone\"][0], \n",
    "}\n",
    "\n",
    "print(\"keys; \", d.keys())\n",
    "\n",
    "json_path = \"padded1_test.json\"\n",
    "with open(json_path, \"w\") as f:\n",
    "    json.dump(d, f)\n",
    "\n",
    "\n",
    "os.chdir(\"circuits\")\n",
    "# !./padded/padded_cpp/padded ../padded1_test.json head.wtns\n",
    "# !./model_test/model_test_cpp/model_test ../padded1_test.json head.wtns\n",
    "!./backbone/backbone_cpp/backbone ../padded1_test.json head.wtns\n",
    "# # # !npx snarkjs groth16 prove ./origDepthwiseConv2d/circuit_final.zkey head.wtns proof.json public_test.json\n",
    "os.chdir(\"../\")\n",
    "\n",
    "print(\"OK\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38d03710-0dee-467c-acae-ecd523049cad",
   "metadata": {},
   "source": [
    "# Testing hashing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "d3ce1f69-7466-43b7-b181-1ed7b32585ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODEL TEST STARTED\n",
      "STEP_IN     RESULT 999999678953308669596737357956888609944281556423122835049532631282419362239\n",
      "HASH OUTPUT RESULT 999999678953308669596737357956888609944281556423122835049532631282419362239\n",
      "WEIGHTS HASH RESULT 4878235444861024761285566989102471308086608624286814920624920793618467057715\n",
      "PARAMS HASH RESULT 26641664397551561746797824861551809283236976665550582276760575734004319122274\n",
      "POINTWISE WEIGHTS HASH RESULT 21011847256614247465027806478923474158642411986573067865077321300115485589574\n",
      "OUTPUT HASH RESULT 27838787996768650461281416050134280483843308848965692542946456243578455289310\n",
      "L0_STEP_OUT[0] RESULT 15571199703095799184197421513773915810687785943782493479351762044398380016696\n",
      "L0_STEP_OUT[1] RESULT 27838787996768650461281416050134280483843308848965692542946456243578455289310\n",
      "START\n",
      "dw_conv done\n",
      "depth batch norm done\n",
      "pw_conv done\n",
      "point batch norm done\n",
      "END\n",
      "LAYER 0 DONE\n",
      "WEIGHTS HASH RESULT 26464444025495705445076460101874314851266853698200008325954139974672143296303\n",
      "PARAMS HASH RESULT 13610154307546635984936163669142959430153454924605136147847814114611837568322\n",
      "POINTWISE WEIGHTS HASH RESULT 426020850812800278627945684430261871443812536454940813130149193606563711453\n",
      "OUTPUT HASH RESULT 27838787996768650461281416050134280483843308848965692542946456243578455289310\n",
      "STEP_OUT[0] RESULT 24609640066216945031129258912229937002460547325763774999926338371182973626534\n",
      "STEP_OUT[1] RESULT 27838787996768650461281416050134280483843308848965692542946456243578455289310\n",
      "START\n",
      "dw_conv done\n",
      "depth batch norm done\n",
      "pw_conv done\n",
      "point batch norm done\n",
      "END\n",
      "LAYER 1 DONE\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "d = {\n",
    "        \"step_in\": [\"0\", \n",
    "                    \"999999678953308669596737357956888609944281556423122835049532631282419362239\"],\n",
    "        # \"in\": d[\"inp\"],\n",
    "        \"in\": backbone.layers[0].depthwise.conv.input,\n",
    "        **backbone.layers[0].to_dict(\"l0_\"),\n",
    "        **backbone.layers[1].to_dict(\"l1_\"),\n",
    "}\n",
    "\n",
    "json_path = \"padded1_test.json\"\n",
    "with open(json_path, \"w\") as f:\n",
    "    json.dump(d, f)\n",
    "\n",
    "\n",
    "os.chdir(\"circuits\")\n",
    "!./model_test/model_test_cpp/model_test ../padded1_test.json head.wtns\n",
    "os.chdir(\"../\")\n",
    "\n",
    "print(\"OK\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba201e25-e3ef-4021-90a3-aa9e462bd445",
   "metadata": {},
   "source": [
    "# First layer manual input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "90d89050-bf81-42b4-9501-23ad15d04cfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BACKBONE STARTED\n",
      "STEP_IN     RESULT 23215198383721024358459492747258189658267674907025876403600822633491593575047\n",
      "HASH OUTPUT RESULT 23215198383721024358459492747258189658267674907025876403600822633491593575047\n",
      "WEIGHTS HASH RESULT 1097474930525066110050955196989148411541598006186766857249811439698220174137\n",
      "step_in[0] 0\n",
      "step_in[1] 23215198383721024358459492747258189658267674907025876403600822633491593575047\n",
      "step_out[0] 1259948989982022095763350961542173932484333119790749852996129985933180386885\n",
      "step_out[1] 1097015325429159465404141921846739455743486728058090106174067791419672700506\n",
      "END\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "# backbone = BackboneCircuitInput(model, 2, pytorch_input, quantized_input)\n",
    "    \n",
    "# backbone.to_json(\"padded_backbone_test_2.json\")\n",
    "\n",
    "json_path = \"send_help.json\"\n",
    "with open(json_path, \"w\") as f:\n",
    "    json.dump({\n",
    "        \"step_in\": [\"0\", \n",
    "                    \"23215198383721024358459492747258189658267674907025876403600822633491593575047\"],\n",
    "        \"in\": backbone.layers[0].depthwise.conv.input,\n",
    "        \"dw_conv_weights\": backbone.layers[0].depthwise.conv.weights,\n",
    "        \"dw_conv_bias\": backbone.layers[0].depthwise.conv.bias,\n",
    "        \"dw_conv_out\": backbone.layers[0].depthwise.conv.out_str,\n",
    "        # \"dw_conv_remainder\": backbone.layers[0].depthwise.conv.remainder,\n",
    "        \n",
    "        \"dw_bn_a\": backbone.layers[0].depthwise.bn.a,\n",
    "        \"dw_bn_b\": backbone.layers[0].depthwise.bn.b,\n",
    "        \"dw_bn_out\": backbone.layers[0].depthwise.bn.out_str,\n",
    "        # \"dw_bn_remainder\": backbone.layers[0].depthwise.bn.remainder,\n",
    "        \n",
    "        \"pw_conv_weights\": backbone.layers[0].pointwise.conv.weights,\n",
    "        \"pw_conv_bias\": backbone.layers[0].pointwise.conv.bias,\n",
    "        \"pw_conv_out\": backbone.layers[0].pointwise.conv.out_str,\n",
    "        # \"pw_conv_remainder\": backbone.layers[0].pointwise.conv.remainder,\n",
    "        \n",
    "        \"pw_bn_a\": backbone.layers[0].pointwise.bn.a,\n",
    "        \"pw_bn_b\": backbone.layers[0].pointwise.bn.b,\n",
    "        \"pw_bn_out\": backbone.layers[0].pointwise.bn.out_str,\n",
    "        # \"pw_bn_remainder\": backbone.layers[0].pointwise.bn.remainder,\n",
    "    }, f)\n",
    "\n",
    "os.chdir(\"circuits\")\n",
    "!./hashing/hashing_cpp/hashing ../send_help.json head.wtns\n",
    "# !npx snarkjs groth16 prove ./origDepthwiseConv2d/circuit_final.zkey head.wtns proof.json public_test.json\n",
    "os.chdir(\"../\")\n",
    "\n",
    "print(\"OK\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "694f31ff-aa6b-492e-9a00-24bcf074cfba",
   "metadata": {},
   "source": [
    "# Second layer manual input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84765837-d02d-4847-a9e2-15cd9bf7b3b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMAGE SHAPE:  torch.Size([3, 32, 32])\n",
    "# IMAGE SHAPE:  torch.Size([1, 3, 32, 32])\n",
    "# STARTING SHAPE:  torch.Size([1, 3, 32, 32])\n",
    "# CONV1 SHAPE:  torch.Size([1, 1, 32, 32])\n",
    "# PRE AVG-POOL SHAPE:  torch.Size([1, 32, 6, 6])\n",
    "# POST AVG-POOL SHAPE:  torch.Size([1, 32, 1, 1])\n",
    "# PRE-CLASSIFIER SHAPE:  torch.Size([1, 32])\n",
    "# POST-CLASSIFIER SHAPE:  torch.Size([1, 10])\n",
    "# Predicted cat - idx: 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "ff636407-3627-4fa9-8b7b-7eeae2a07c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial    SHAPE: 32x32x3\n",
    "# conv       SHAPE: 32x32x8\n",
    "      # (0): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), groups=8, bias=False)\n",
    "      # (0): Conv2d(8, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "# layer0_dw  SHAPE: 30x30x8\n",
    "# layer0_pw  SHAPE: 30x30x16\n",
    "\n",
    "      # (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), groups=16, bias=False)\n",
    "      # (0): Conv2d(16, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "# layer1_dw  SHAPE: 28x28x16\n",
    "# layer1_pw  SHAPE: 28x28x32\n",
    "\n",
    "      # (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), groups=32, bias=False)\n",
    "      # (0): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "# layer2_dw  SHAPE: 26x26x32\n",
    "# layer2_pw  SHAPE: 26x26x32\n",
    "\n",
    "      # (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), groups=32, bias=False)\n",
    "      # (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "# layer3_dw  SHAPE: 24x24x32\n",
    "# layer3_pw  SHAPE: 24x24x64\n",
    "\n",
    "      # (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), groups=64, bias=False)\n",
    "      # (0): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "# layer4_dw  SHAPE: 22x22x64\n",
    "# layer4_pw  SHAPE: 22x22x64\n",
    "\n",
    "      # (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), groups=64, bias=False)\n",
    "      # (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "# layer5_dw  SHAPE: 20x20x64\n",
    "# layer5_pw  SHAPE: 20x20x128\n",
    "\n",
    "      # (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), groups=128, bias=False)\n",
    "      # (0): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "# layer6_dw  SHAPE: 18x18x128\n",
    "# layer6_pw  SHAPE: 18x18x128\n",
    "\n",
    "      # (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), groups=128, bias=False)\n",
    "      # (0): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "# layer7_dw  SHAPE: 16x16x128\n",
    "# layer7_pw  SHAPE: 16x16x128\n",
    "\n",
    "      # (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), groups=128, bias=False)\n",
    "      # (0): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "# layer8_dw  SHAPE: 14x14x128\n",
    "# layer8_pw  SHAPE: 14x14x128\n",
    "\n",
    "      # (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), groups=128, bias=False)\n",
    "      # (0): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "# layer9_dw  SHAPE: 12x12x128\n",
    "# layer9_pw  SHAPE: 12x12x128\n",
    "\n",
    "      # (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), groups=128, bias=False)\n",
    "      # (0): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "# layer10_dw  SHAPE: 10x10x128\n",
    "# layer10_pw  SHAPE: 10x10x128\n",
    "\n",
    "      # (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), groups=128, bias=False)\n",
    "      # (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "# layer11_dw  SHAPE: 8x8x128\n",
    "# layer11_pw  SHAPE: 8x8x256\n",
    "\n",
    "      # (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), groups=256, bias=False)\n",
    "      # (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "# layer12_dw  SHAPE: 6x6x256\n",
    "# layer12_pw  SHAPE: 6x6x256"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
