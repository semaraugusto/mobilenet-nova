{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6e30ed7-19bd-495b-9c3b-37b7753f3934",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Manual mobilenet implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "69987bda-3e35-44f6-9117-a90513a9513b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from models.mobilenet import MyMobileNet\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pytorch_lightning as pl\n",
    "import torchvision\n",
    "import torchvision.transforms.v2 as transforms\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "1a79f141-4c15-4283-9a09-4825a99028f9",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Circom defines the range of positives are [0, p/2] and the range of negatives are [(p/2)+1, (p-1)].\n",
    "# CIRCOM_PRIME = 21888242871839275222246405745257275088548364400416034343698204186575808495617\n",
    "CIRCOM_PRIME = 28948022309329048855892746252171976963363056481941647379679742748393362948097\n",
    "MAX_POSITIVE = CIRCOM_PRIME // 2\n",
    "MAX_NEGATIVE = MAX_POSITIVE + 1 # The most positive number\n",
    "CIRCOM_NEGATIVE_1 = CIRCOM_PRIME - 1\n",
    "EXPONENT = 15\n",
    "\n",
    "def from_circom(x):\n",
    "    if type(x) != int:\n",
    "        x = int(x)\n",
    "    if x > MAX_POSITIVE: \n",
    "        return x - CIRCOM_PRIME\n",
    "    return x\n",
    "    \n",
    "def to_circom(x):\n",
    "    return x % CIRCOM_PRIME\n",
    "    \n",
    "def to_circom_input(array: np.array): \n",
    "    if type(array) != np.array:\n",
    "        array = np.array(array)\n",
    "    int_array = array.round().astype(int)\n",
    "    int_array = to_circom(int_array)\n",
    "    return int_array.astype(str).tolist()\n",
    "\n",
    "\n",
    "def DepthwiseConv(nRows, nCols, nChannels, nFilters, kernelSize, strides, n, input, weights, bias):\n",
    "    assert(nFilters % nChannels == 0)\n",
    "    outRows = (nRows - kernelSize)//strides + 1\n",
    "    outCols = (nCols - kernelSize)//strides + 1\n",
    "    \n",
    "    # out = np.zeros((outRows, outCols, nFilters))\n",
    "    out = [[[0 for _ in range(nFilters)] for _ in range(outCols)] for _ in range(outRows)]\n",
    "    remainder = [[[0 for _ in range(nFilters)] for _ in range(outCols)] for _ in range(outRows)]\n",
    "    # remainder = np.zeros((outRows, outCols, nFilters))\n",
    "    \n",
    "    for row in range(outRows):\n",
    "        for col in range(outCols):\n",
    "            for channel in range(nChannels):\n",
    "                for x in range(kernelSize):\n",
    "                    for y in range(kernelSize):\n",
    "                        out[row][col][channel] += int(input[row*strides+x, col*strides+y, channel]) * int(weights[x, y, channel])\n",
    "                \n",
    "                out[row][col][channel] += int(bias[channel])\n",
    "                remainder[row][col][channel] = str(int(out[row][col][channel] % n))\n",
    "                out[row][col][channel] = int(out[row][col][channel] // n)\n",
    "                            \n",
    "    return out, remainder\n",
    "\n",
    "def PointwiseConv2d(nRows, nCols, nChannels, nFilters, strides, n, input, weights, bias):\n",
    "    kernelSize = 1\n",
    "    outRows = (nRows - kernelSize)//strides + 1\n",
    "    outCols = (nCols - kernelSize)//strides + 1\n",
    "    out = [[[0 for _ in range(nFilters)] for _ in range(outCols)] for _ in range(outRows)]\n",
    "    str_out = [[[0 for _ in range(nFilters)] for _ in range(outCols)] for _ in range(outRows)]\n",
    "    remainder = [[[None for _ in range(nFilters)] for _ in range(outCols)] for _ in range(outRows)]\n",
    "    for row in range(outRows):\n",
    "        for col in range(outCols):\n",
    "            for filter in range(nFilters):\n",
    "                for k in range(nChannels):\n",
    "                    out[row][col][filter] += int(input[row*strides, col*strides, k]) * int(weights[k, filter])\n",
    "                    \n",
    "                out[row][col][filter] += int(bias[filter])\n",
    "                remainder[row][col][filter] = str(int(out[row][col][filter] % n))\n",
    "                out[row][col][filter] = int(out[row][col][filter] // n)\n",
    "                str_out[row][col][filter] = str(out[row][col][filter] % p)\n",
    "                            \n",
    "    return out, str_out, remainder\n",
    "\n",
    "def SeparableConvImpl(nRows, nCols, nChannels, nDepthFilters, nPointFilters, kernelSize, strides, n, input, depthWeights, pointWeights, depthBias, pointBias):\n",
    "    outRows = (nRows - kernelSize)//strides + 1\n",
    "    outCols = (nCols - kernelSize)//strides + 1\n",
    "\n",
    "    depth_out, depth_remainder = DepthwiseConv(nRows, nCols, nChannels, nDepthFilters, kernelSize, strides, n, input, depthWeights, depthBias)\n",
    "    point_out, point_str_out, point_remainder = PointwiseConv2d(outRows, outCols, nChannels, nPointFilters, strides, n, depth_out, pointWeights, pointBias)\n",
    "    return depth_out, depth_remainder, point_out, point_str_out, point_remainder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "faaa5941-253c-4676-94bd-c4dfbb1e524f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetWrapper(torch.utils.data.Dataset):\n",
    "    def __init__(self, subset, transform=None):\n",
    "        self.subset = subset\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        x, y = self.subset[index]\n",
    "        \n",
    "        if self.transform:\n",
    "            x = self.transform(x)\n",
    "        return x, y\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "622df7cd-d2f1-40bd-b801-27df6c3b0cdf",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(\n",
    "    root='./CIFAR10', train=False, download=True, transform=transform)\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(\n",
    "    root='./CIFAR10', train=True, download=True)\n",
    "\n",
    "# split the train set into train/validation\n",
    "train_set_size = int(len(trainset) * 0.8)\n",
    "valid_set_size = len(trainset) - train_set_size\n",
    "\n",
    "seed = torch.Generator().manual_seed(42)\n",
    "trainset, validset = torch.utils.data.random_split(trainset, [train_set_size, valid_set_size], generator=seed)\n",
    "\n",
    "trainset = DatasetWrapper(trainset, transform)\n",
    "validset = DatasetWrapper(validset, transform)\n",
    "\n",
    "# Create train dataloader\n",
    "trainloader = torch.utils.data.DataLoader(\n",
    "    trainset, batch_size=512, shuffle=True, num_workers=24)\n",
    "\n",
    "# Create validation dataloader\n",
    "validloader = torch.utils.data.DataLoader(\n",
    "    validset, batch_size=512, shuffle=False, num_workers=24)\n",
    "\n",
    "# Create test dataloader\n",
    "testloader = torch.utils.data.DataLoader(\n",
    "    testset, batch_size=512, shuffle=False, num_workers=24)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat', 'deer',\n",
    "           'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 518,
   "id": "0ef5c968-2309-4bcb-a680-9450536ef079",
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted horse - idx: 7\n"
     ]
    }
   ],
   "source": [
    "MODEL_WEIGHTS_PATH = './checkpoints/model_small_100epochs.pth'\n",
    "\n",
    "model = MyMobileNet(trainloader, num_classes=10, alpha=0.25, max_epochs=100)\n",
    "checkpoint = torch.load(MODEL_WEIGHTS_PATH)\n",
    "# model.load_state_dict(checkpoint['state_dict'])\n",
    "model.load_state_dict(checkpoint['net'])\n",
    "model.eval()\n",
    "\n",
    "image, label = validset[0]\n",
    "image = image.unsqueeze(0)\n",
    "logits = model(image)\n",
    "pred_idx = logits.argmax()\n",
    "\n",
    "print(f\"Predicted {classes[pred_idx]} - idx: {pred_idx}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "da689ff6-ad5a-421b-b388-bf0b26467475",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Conv2DInt(nRows, nCols, nChannels, nFilters, kernelSize, strides, n, input, weights, bias):\n",
    "    p = CIRCOM_PRIME\n",
    "    Input = [[[str(int(input[i][j][k]) % p) for k in range(nChannels)] for j in range(nCols)] for i in range(nRows)]\n",
    "    Weights = [[[[str(int(weights[i][j][k][l]) % p) for l in range(nFilters)] for k in range(nChannels)] for j in range(kernelSize)] for i in range(kernelSize)]\n",
    "    Bias = [str(int(bias[i]) % p) for i in range(nFilters)]\n",
    "    out = [[[0 for _ in range(nFilters)] for _ in range((nCols - kernelSize)//strides + 1)] for _ in range((nRows - kernelSize)//strides + 1)]\n",
    "    remainder = [[[None for _ in range(nFilters)] for _ in range((nCols - kernelSize)//strides + 1)] for _ in range((nRows - kernelSize)//strides + 1)]\n",
    "    for i in range((nRows - kernelSize)//strides + 1):\n",
    "        for j in range((nCols - kernelSize)//strides + 1):\n",
    "            for m in range(nFilters):\n",
    "                for k in range(nChannels):\n",
    "                    for x in range(kernelSize):\n",
    "                        for y in range(kernelSize):\n",
    "                            out[i][j][m] += int(input[i*strides+x][j*strides+y][k]) * int(weights[x][y][k][m])\n",
    "                out[i][j][m] += int(bias[m])\n",
    "                remainder[i][j][m] = str(int(out[i][j][m]) % n)\n",
    "                out[i][j][m] = str(int(out[i][j][m]) // n % p)\n",
    "    return Input, Weights, Bias, out, remainder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "247d5284-3c4f-4fd3-9781-43b47c592365",
   "metadata": {},
   "outputs": [],
   "source": [
    "def BatchNormalizationInt(nRows, nCols, nChannels, n, X_in, a_in, b_in):\n",
    "    p = CIRCOM_PRIME\n",
    "    X = [[[str(int(X_in[i][j][k]) % p) for k in range(nChannels)] for j in range(nCols)] for i in range(nRows)]\n",
    "    A = [str(int(a_in[k]) % p) for k in range(nChannels)]\n",
    "    B = [str(int(b_in[k]) % p) for k in range(nChannels)]\n",
    "    out = [[[0 for _ in range(nChannels)] for _ in range(nCols)] for _ in range(nRows)]\n",
    "    remainder = [[[None for _ in range(nChannels)] for _ in range(nCols)] for _ in range(nRows)]\n",
    "    for i in range(nRows):\n",
    "        for j in range(nCols):\n",
    "            for k in range(nChannels):\n",
    "                out[i][j][k] = int(int(X_in[i][j][k]) * int(a_in[k]) + int(b_in[k]))\n",
    "                remainder[i][j][k] = str(int(out[i][j][k]) % n)\n",
    "                out[i][j][k] = int(out[i][j][k] // n)\n",
    "    return X, A, B, out, remainder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 519,
   "id": "56e21b0a-fbb2-46db-80c1-5d8f57518c95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out.shape=torch.Size([1, 8, 32, 32])\n",
      "expected.shape=torch.Size([32, 32, 8])\n",
      "torch.Size([1, 8, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "gamma = model.bn.weight\n",
    "beta = model.bn.bias\n",
    "mean = model.bn.running_mean\n",
    "var = model.bn.running_var\n",
    "eps = model.bn.eps\n",
    "\n",
    "a = (gamma/(var+eps)**.5).detach()\n",
    "b = (beta-gamma*mean/(var+eps)**.5).detach().tolist()\n",
    "\n",
    "image, label = testset[0]\n",
    "image = image.unsqueeze(0)\n",
    "out = model.conv(image)\n",
    "expected = model.bn(out)\n",
    "# out = out.squeeze()\n",
    "print(f\"{out.shape=}\")\n",
    "\n",
    "expected = torch.permute(expected.squeeze(), (1, 2, 0))\n",
    "\n",
    "print(f\"{expected.shape=}\")\n",
    "\n",
    "quantized_in = torch.permute(out.squeeze(), (1, 2, 0)) * 10**EXPONENT\n",
    "quantized_a = (a * 10**EXPONENT).tolist()\n",
    "quantized_b = [bi * 10**(2*EXPONENT) for bi in b]\n",
    "print(out.shape)\n",
    "\n",
    "X, A, B, actual, remainder = BatchNormalizationInt(32, 32, 8, 10**EXPONENT, quantized_in, quantized_a, quantized_b)\n",
    "test_output = [[[from_circom(int(out)) / 10**EXPONENT for out in asdf] for asdf in asdfasdf] for asdfasdf in actual]\n",
    "\n",
    "assert(torch.allclose(torch.Tensor(test_output), expected, atol=1e-6))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea13b7cc-41ef-4127-b1fd-99ffcc4723b8",
   "metadata": {},
   "source": [
    "# Testing head layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fbe4fd19-94b7-4b11-9dac-e308c523b7a8",
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "something\n",
      "before conv2d\n",
      "after conv2d\n",
      "after bn\n",
      "after relu\n",
      "end\n",
      "ok\n"
     ]
    }
   ],
   "source": [
    "# CONVOLUTION LAYER\n",
    "weights = model.conv.weight.detach().numpy().transpose(2, 3, 1, 0)\n",
    "image, label = testset[0]\n",
    "\n",
    "expected = model.conv(image).detach().numpy()\n",
    "bias = torch.zeros(weights.shape[3]).numpy()\n",
    "\n",
    "# padded = pad(image, 1).transpose(1,2,0)\n",
    "padded = F.pad(image, (1,1,1,1), \"constant\", 0).numpy()\n",
    "padded = padded.transpose(1,2,0)\n",
    "\n",
    "quantized_image = (padded * 10**EXPONENT).round()\n",
    "quantized_weights = (weights * 10**EXPONENT).round() # .transpose(0, 3, 1, 0) # [nFilters, nChannels, H, W] -> \n",
    "\n",
    "circuit_in, circuit_conv_weights, circuit_conv_bias, circuit_conv_out, circuit_conv_remainder = Conv2DInt(34, 34, 3, 8, 3, 1, 10**EXPONENT, quantized_image, quantized_weights, bias)\n",
    "# output, remainder = Conv2DInt(34, 34, 3, 8, 3, 1, 10**EXPONENT, quantized_image, quantized_weights, bias)\n",
    "# test_output = output / 10**(EXPONENT)\n",
    "test_output = [[[from_circom(int(out)) / 10**EXPONENT for out in asdf] for asdf in asdfasdf] for asdfasdf in circuit_conv_out]\n",
    "\n",
    "expected = expected.transpose((1, 2, 0))\n",
    "\n",
    "assert(np.allclose(test_output, expected, atol=1e-6))\n",
    "\n",
    "# BATCH NORM CONSTANTS\n",
    "gamma = model.bn.weight\n",
    "beta = model.bn.bias\n",
    "mean = model.bn.running_mean\n",
    "var = model.bn.running_var\n",
    "eps = model.bn.eps\n",
    "\n",
    "a = (gamma/(var+eps)**.5).detach()\n",
    "b = (beta-gamma*mean/(var+eps)**.5).detach().tolist()\n",
    "\n",
    "quantized_a = (a * 10**EXPONENT).tolist()\n",
    "quantized_b = [bi * 10**(2*EXPONENT) for bi in b]\n",
    "\n",
    "# BATCH NORM USING PYTORCH OUTPUT\n",
    "image, label = testset[0]\n",
    "image = image.unsqueeze(0)\n",
    "out = model.conv(image)\n",
    "expected = model.bn(out)\n",
    "\n",
    "expected = torch.permute(expected.squeeze(), (1, 2, 0))\n",
    "\n",
    "quantized_in = torch.permute(out.squeeze(), (1, 2, 0)) * 10**EXPONENT\n",
    "\n",
    "X, A, B, actual, remainder = BatchNormalizationInt(32, 32, 8, 10**EXPONENT, quantized_in, quantized_a, quantized_b)\n",
    "test_output = [[[from_circom(int(out)) / 10**EXPONENT for out in vec] for vec in matrix] for matrix in actual]\n",
    "assert(torch.allclose(torch.Tensor(test_output), expected, atol=1e-6))\n",
    "\n",
    "# BATCH NORM USING CIRCUIT CONV OUTPUT\n",
    "quantized_in = [[[from_circom(int(out)) for out in vec] for vec in matrix] for matrix in circuit_conv_out]\n",
    "\n",
    "_, circuit_bn_a, circuit_bn_b, circuit_bn_out, circuit_bn_remainder = BatchNormalizationInt(32, 32, 8, 10**EXPONENT, quantized_in, quantized_a, quantized_b)\n",
    "\n",
    "test_output = [[[int(from_circom(int(out))) / 10**EXPONENT for out in vec] for vec in matrix] for matrix in actual]\n",
    "\n",
    "assert(torch.allclose(torch.Tensor(test_output), expected, atol=1e-6))\n",
    "\n",
    "# RELU USING CIRCUIT OUTPUT\n",
    "relu_in = [[[to_circom(bn_out) if bn_out < MAX_POSITIVE else 0 for bn_out in vec] for vec in matrix] for matrix in circuit_bn_out]\n",
    "relu_out = [[[str(bn_out) if bn_out < MAX_POSITIVE else 0 for bn_out in vec] for vec in matrix] for matrix in relu_in]\n",
    "\n",
    "input_json_path = \"head_input1.json\"\n",
    "with open(input_json_path, \"w\") as input_file:\n",
    "    json.dump({\"in\": circuit_in,\n",
    "               \"conv2d_weights\": circuit_conv_weights,\n",
    "               \"conv2d_bias\": circuit_conv_bias,\n",
    "               \"conv2d_out\": circuit_conv_out,\n",
    "               \"conv2d_remainder\": circuit_conv_remainder,\n",
    "               \n",
    "               \"bn_a\": circuit_bn_a,\n",
    "               \"bn_b\": circuit_bn_b,\n",
    "               \"bn_out\": circuit_bn_out,\n",
    "               \"bn_remainder\": circuit_bn_remainder,\n",
    "               \n",
    "               \"relu_out\": relu_out,\n",
    "               }, input_file)\n",
    "\n",
    "os.chdir(\"circuits\")\n",
    "!./head/head_cpp/head ../head_input1.json head.wtns\n",
    "!npx snarkjs groth16 prove head/circuit_final.zkey head.wtns proof.json public_test.json\n",
    "os.chdir(\"../\")\n",
    "\n",
    "print(\"ok\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f410f561-1bc2-4fc4-94cb-cf4c1d4ed258",
   "metadata": {},
   "source": [
    "# Testing padding over highly padded input (to try and fold the circuit using nova)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 520,
   "id": "ecdd2b74-e553-4f68-b755-7f50d91a58a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SeparableConv2d(nn.Module):\n",
    "    '''Separable convolution'''\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super(SeparableConv2d, self).__init__()\n",
    "        self.dw_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, in_channels, kernel_size=3, stride=stride, padding=0, groups=in_channels, bias=False),\n",
    "            nn.BatchNorm2d(in_channels),\n",
    "            # nn.ReLU(inplace=False),\n",
    "        )\n",
    "        self.pw_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            # nn.ReLU(inplace=False),\n",
    "        )\n",
    "        \n",
    "input = torch.randn((1, 3, 5, 5))\n",
    "test_model = SeparableConv2d(3, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "id": "2288f8d3-b0d8-4954-9d15-b7ca41f5b065",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 3, 3, 3)\n",
      "test_output shape:  (3, 3, 3)\n",
      "expected shape:  (3, 3, 3)\n"
     ]
    }
   ],
   "source": [
    "weights = test_model.dw_conv[0].weight.squeeze().detach().numpy()\n",
    "bias = torch.zeros(weights.shape[0]).numpy()\n",
    "\n",
    "expected = test_model.dw_conv[0](input).detach().numpy()\n",
    "print(expected.shape)\n",
    "\n",
    "padded = F.pad(input, (1,1,1,1), \"constant\", 0)\n",
    "padded = padded.squeeze().numpy().transpose((1, 2, 0))\n",
    "weights = weights.transpose((1, 2, 0))\n",
    "\n",
    "quantized_image = padded * 10**EXPONENT\n",
    "quantized_weights = weights * 10**EXPONENT\n",
    "\n",
    "actual, rem = DepthwiseConv(7, 7, 3, 3, 3, 1, 10**EXPONENT, quantized_image.round(), quantized_weights.round(), bias)\n",
    "\n",
    "test_output = np.array([[[int(out) / 10**EXPONENT for out in asdf] for asdf in asdfasdf] for asdfasdf in actual])\n",
    "test_output = test_output[1:4, 1:4, :]\n",
    "expected = expected.squeeze().transpose((1, 2, 0))\n",
    "# expected = expected * 10**EXPONENT\n",
    "\n",
    "print(\"test_output shape: \", test_output.shape)\n",
    "print(\"expected shape: \", expected.shape)\n",
    "assert(np.allclose(expected, test_output, atol=0.00001))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "abef6b48-7498-40f8-ace1-84603933c80c",
   "metadata": {},
   "outputs": [],
   "source": [
    "input = torch.randn((1, 3, 5, 5))\n",
    "model = SeparableConv2d(3, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "id": "811eec43-c91a-4750-99a9-21b0789928cc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 3, 3, 3)\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "def DepthwiseConv(nRows, nCols, nChannels, nFilters, kernelSize, strides, n, input, weights, bias):\n",
    "    assert(nFilters % nChannels == 0)\n",
    "    outRows = (nRows - kernelSize)//strides + 1\n",
    "    outCols = (nCols - kernelSize)//strides + 1\n",
    "    \n",
    "    # out = np.zeros((outRows, outCols, nFilters))\n",
    "    out = [[[0 for _ in range(nFilters)] for _ in range(outCols)] for _ in range(outRows)]\n",
    "    remainder = [[[0 for _ in range(nFilters)] for _ in range(outCols)] for _ in range(outRows)]\n",
    "    # remainder = np.zeros((outRows, outCols, nFilters))\n",
    "    \n",
    "    for row in range(outRows):\n",
    "        for col in range(outCols):\n",
    "            for channel in range(nChannels):\n",
    "                for x in range(kernelSize):\n",
    "                    for y in range(kernelSize):\n",
    "                        out[row][col][channel] += int(input[row*strides+x, col*strides+y, channel]) * int(weights[x, y, channel])\n",
    "                \n",
    "                out[row][col][channel] += int(bias[channel])\n",
    "                remainder[row][col][channel] = str(int(out[row][col][channel] % n))\n",
    "                out[row][col][channel] = int(out[row][col][channel] // n)\n",
    "                            \n",
    "    return out, remainder\n",
    "    \n",
    "weights = test_model.dw_conv[0].weight.squeeze().detach().numpy()\n",
    "bias = torch.zeros(weights.shape[0]).numpy()\n",
    "\n",
    "expected = test_model.dw_conv[0](input).detach().numpy()\n",
    "print(expected.shape)\n",
    "\n",
    "padded = F.pad(input, (1,1,1,1), \"constant\", 0)\n",
    "padded = padded.squeeze().numpy().transpose((1, 2, 0))\n",
    "weights = weights.transpose((1, 2, 0))\n",
    "\n",
    "quantized_image = padded * 10**EXPONENT\n",
    "quantized_weights = weights * 10**EXPONENT\n",
    "\n",
    "actual, rem = DepthwiseConv(7, 7, 3, 3, 3, 1, 10**EXPONENT, quantized_image.round(), quantized_weights.round(), bias)\n",
    "\n",
    "# test_output = np.array([[[int(out) / 10**EXPONENT for out in asdf] for asdf in asdfasdf] for asdfasdf in actual])\n",
    "# test_output = test_output[1:6, 1:6, :]\n",
    "test_output = np.array([[[int(out) / 10**EXPONENT for out in asdf] for asdf in asdfasdf] for asdfasdf in actual])\n",
    "test_output = test_output[1:4, 1:4, :]\n",
    "\n",
    "expected = expected.squeeze().transpose((1, 2, 0))\n",
    "# expected = expected * 10**EXPONENT\n",
    "assert(np.allclose(expected, test_output, atol=0.00001))\n",
    "\n",
    "circuit_in = quantized_image.round().astype(int).astype(str).tolist()\n",
    "# circuit_weights = quantized_weights.round().astype(int).astype(str).tolist()\n",
    "circuit_bias = bias.round().astype(int).astype(str).tolist()\n",
    "\n",
    "p = CIRCOM_PRIME\n",
    "Input = [[[str(int(quantized_image[i][j][k].round()) % p) for k in range(quantized_image.shape[2])] for j in range(quantized_image.shape[1])] for i in range(quantized_image.shape[0])]\n",
    "Weights = [[[str(int(quantized_weights[i][j][k].round()) % p) for k in range(quantized_weights.shape[2])] for j in range(quantized_weights.shape[1])] for i in range(quantized_weights.shape[0])]\n",
    "# Out = [[[str(int(actual[i][j][k]) % p) for k in range(3)] for j in range(7)] for i in range(7)]\n",
    "\n",
    "test_output = np.array([[[int(out) for out in asdf] for asdf in asdfasdf] for asdfasdf in actual])\n",
    "# test_output = test_output[1:6, 1:6, :]\n",
    "Out = [[[str(int(test_output[i][j][k]) % p) for k in range(3)] for j in range(5)] for i in range(5)]\n",
    "Rem = [[[str(rem[i][j][k]) for k in range(3)] for j in range(5)] for i in range(5)]\n",
    "\n",
    "input_json_path = \"backbone_input.json\"\n",
    "with open(input_json_path, \"w\") as input_file:\n",
    "    json.dump({\n",
    "               \"in\": Input,\n",
    "               \"weights\": Weights,\n",
    "               \"bias\": circuit_bias,\n",
    "               \"remainder\": Rem,\n",
    "               \"out\": Out,\n",
    "              },\n",
    "              input_file)\n",
    "\n",
    "\n",
    "# os.chdir(\"circuits\")\n",
    "# !./origDepthwiseConv2d/origDepthwiseConv2d_cpp/origDepthwiseConv2d ../backbone_input.json head.wtns\n",
    "# !npx snarkjs groth16 prove ./origDepthwiseConv2d/circuit_final.zkey head.wtns proof.json public_test.json\n",
    "# os.chdir(\"../\")\n",
    "\n",
    "print(\"OK\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f933a59-1605-4a64-ac94-f4e7107a79e7",
   "metadata": {},
   "source": [
    "# Padded Convolution test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c6fd691-e729-404e-a858-2d3e45e26846",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 524,
   "id": "b8a3222d-4eee-4060-b336-07da6efa1ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "input = torch.randn((1, 3, 7, 7))\n",
    "model = SeparableConv2d(3, 6)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "id": "1dd45bb1-31a1-42e1-80a8-a7ac0452b45b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def PointwiseConv2d(nRows, nCols, nChannels, nFilters, strides, n, input, weights, bias):\n",
    "    kernelSize = 1\n",
    "    outRows = (nRows - kernelSize)//strides + 1\n",
    "    outCols = (nCols - kernelSize)//strides + 1\n",
    "    out = [[[0 for _ in range(nFilters)] for _ in range(nCols)] for _ in range(nRows)]\n",
    "    remainder = [[[None for _ in range(nFilters)] for _ in range(nCols)] for _ in range(nRows)]\n",
    "    for row in range(outRows):\n",
    "        for col in range(outCols):\n",
    "            for filter in range(nFilters):\n",
    "                for k in range(nChannels):\n",
    "                    out[row][col][filter] += int(input[row*strides][col*strides][k]) * int(weights[k, filter])\n",
    "                    \n",
    "                out[row][col][filter] += int(bias[filter])\n",
    "                remainder[row][col][filter] = str(int(out[row][col][filter] % n))\n",
    "                out[row][col][filter] = int(out[row][col][filter] // n)\n",
    "                            \n",
    "    return out, remainder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "id": "b70bb0a6-f586-4719-a200-501ad9bb708d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape:  torch.Size([1, 3, 5, 5])\n",
      "Expected shape:  torch.Size([1, 3, 3, 3])\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 5 is out of bounds for axis 0 with size 5",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[415], line 43\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# quantized_image = padded * 10**EXPONENT\u001b[39;00m\n\u001b[1;32m     41\u001b[0m quantized_weights \u001b[38;5;241m=\u001b[39m weights \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m10\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mEXPONENT\n\u001b[0;32m---> 43\u001b[0m actual, rem \u001b[38;5;241m=\u001b[39m \u001b[43mPaddedDepthwiseConv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m7\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m7\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mEXPONENT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquantized_image\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mround\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquantized_weights\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mround\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     45\u001b[0m test_output \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([[[\u001b[38;5;28mint\u001b[39m(out) \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m10\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mEXPONENT \u001b[38;5;28;01mfor\u001b[39;00m out \u001b[38;5;129;01min\u001b[39;00m asdf] \u001b[38;5;28;01mfor\u001b[39;00m asdf \u001b[38;5;129;01min\u001b[39;00m asdfasdf] \u001b[38;5;28;01mfor\u001b[39;00m asdfasdf \u001b[38;5;129;01min\u001b[39;00m actual])\n\u001b[1;32m     46\u001b[0m test_output \u001b[38;5;241m=\u001b[39m test_output[\u001b[38;5;241m1\u001b[39m:\u001b[38;5;241m6\u001b[39m, \u001b[38;5;241m1\u001b[39m:\u001b[38;5;241m6\u001b[39m, :]\n",
      "Cell \u001b[0;32mIn[415], line 7\u001b[0m, in \u001b[0;36mPaddedDepthwiseConv\u001b[0;34m(nRows, nCols, nChannels, nFilters, kernelSize, strides, n, input, weights, bias)\u001b[0m\n\u001b[1;32m      4\u001b[0m outCols \u001b[38;5;241m=\u001b[39m (nCols \u001b[38;5;241m-\u001b[39m kernelSize)\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39mstrides \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# out = np.zeros((outRows, outCols, nFilters))\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m Input \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mj\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m%\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mnChannels\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mnCols\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mnRows\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m      8\u001b[0m Weights \u001b[38;5;241m=\u001b[39m [[[\u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mint\u001b[39m(weights[i][j][k]\u001b[38;5;241m.\u001b[39mround()) \u001b[38;5;241m%\u001b[39m p) \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(weights\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m])] \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(weights\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m])] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(weights\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m])]\n\u001b[1;32m      9\u001b[0m out \u001b[38;5;241m=\u001b[39m [[[\u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(nFilters)] \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(nCols)] \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(nRows)]\n",
      "Cell \u001b[0;32mIn[415], line 7\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      4\u001b[0m outCols \u001b[38;5;241m=\u001b[39m (nCols \u001b[38;5;241m-\u001b[39m kernelSize)\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39mstrides \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# out = np.zeros((outRows, outCols, nFilters))\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m Input \u001b[38;5;241m=\u001b[39m [\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mj\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m%\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mnChannels\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mnCols\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(nRows)]\n\u001b[1;32m      8\u001b[0m Weights \u001b[38;5;241m=\u001b[39m [[[\u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mint\u001b[39m(weights[i][j][k]\u001b[38;5;241m.\u001b[39mround()) \u001b[38;5;241m%\u001b[39m p) \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(weights\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m])] \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(weights\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m])] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(weights\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m])]\n\u001b[1;32m      9\u001b[0m out \u001b[38;5;241m=\u001b[39m [[[\u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(nFilters)] \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(nCols)] \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(nRows)]\n",
      "Cell \u001b[0;32mIn[415], line 7\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      4\u001b[0m outCols \u001b[38;5;241m=\u001b[39m (nCols \u001b[38;5;241m-\u001b[39m kernelSize)\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39mstrides \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# out = np.zeros((outRows, outCols, nFilters))\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m Input \u001b[38;5;241m=\u001b[39m [[\u001b[43m[\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mj\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m%\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mnChannels\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(nCols)] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(nRows)]\n\u001b[1;32m      8\u001b[0m Weights \u001b[38;5;241m=\u001b[39m [[[\u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mint\u001b[39m(weights[i][j][k]\u001b[38;5;241m.\u001b[39mround()) \u001b[38;5;241m%\u001b[39m p) \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(weights\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m])] \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(weights\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m])] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(weights\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m])]\n\u001b[1;32m      9\u001b[0m out \u001b[38;5;241m=\u001b[39m [[[\u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(nFilters)] \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(nCols)] \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(nRows)]\n",
      "Cell \u001b[0;32mIn[415], line 7\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      4\u001b[0m outCols \u001b[38;5;241m=\u001b[39m (nCols \u001b[38;5;241m-\u001b[39m kernelSize)\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39mstrides \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# out = np.zeros((outRows, outCols, nFilters))\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m Input \u001b[38;5;241m=\u001b[39m [[[\u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mint\u001b[39m(\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mj\u001b[49m\u001b[43m]\u001b[49m[k]) \u001b[38;5;241m%\u001b[39m p) \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(nChannels)] \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(nCols)] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(nRows)]\n\u001b[1;32m      8\u001b[0m Weights \u001b[38;5;241m=\u001b[39m [[[\u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mint\u001b[39m(weights[i][j][k]\u001b[38;5;241m.\u001b[39mround()) \u001b[38;5;241m%\u001b[39m p) \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(weights\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m])] \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(weights\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m])] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(weights\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m])]\n\u001b[1;32m      9\u001b[0m out \u001b[38;5;241m=\u001b[39m [[[\u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(nFilters)] \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(nCols)] \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(nRows)]\n",
      "\u001b[0;31mIndexError\u001b[0m: index 5 is out of bounds for axis 0 with size 5"
     ]
    }
   ],
   "source": [
    "def PaddedDepthwiseConv(nRows, nCols, nChannels, nFilters, kernelSize, strides, n, input, weights, bias):\n",
    "    assert(nFilters % nChannels == 0)\n",
    "    outRows = (nRows - kernelSize)//strides + 1\n",
    "    outCols = (nCols - kernelSize)//strides + 1\n",
    "    \n",
    "    # out = np.zeros((outRows, outCols, nFilters))\n",
    "    Input = [[[str(int(input[i][j][k]) % p) for k in range(nChannels)] for j in range(nCols)] for i in range(nRows)]\n",
    "    Weights = [[[str(int(weights[i][j][k].round()) % p) for k in range(weights.shape[2])] for j in range(weights.shape[1])] for i in range(weights.shape[0])]\n",
    "    out = [[[0 for _ in range(nFilters)] for _ in range(nCols)] for _ in range(nRows)]\n",
    "    Bias = bias.round().astype(int).astype(str).tolist()\n",
    "    out_str = [[[str(0) for _ in range(nFilters)] for _ in range(nCols)] for _ in range(nRows)]\n",
    "    remainder = [[[str(0) for _ in range(nFilters)] for _ in range(nCols)] for _ in range(nRows)]\n",
    "    \n",
    "    for row in range(outRows):\n",
    "        for col in range(outCols):\n",
    "            for channel in range(nChannels):\n",
    "                for x in range(kernelSize):\n",
    "                    for y in range(kernelSize):\n",
    "                        out[row+1][col+1][channel] += int(input[row*strides+x][col*strides+y][channel]) * int(weights[x, y, channel])\n",
    "                \n",
    "                out[row+1][col+1][channel] += int(bias[channel])\n",
    "                remainder[row+1][col+1][channel] = str(int(out[row+1][col+1][channel] % n))\n",
    "                out[row+1][col+1][channel] = int(out[row+1][col+1][channel] // n)\n",
    "                out_str[row+1][col+1][channel] = str(out[row+1][col+1][channel] % p)\n",
    "                            \n",
    "    return out, remainder\n",
    "    \n",
    "weights = model.dw_conv[0].weight.squeeze().detach().numpy()\n",
    "bias = torch.zeros(weights.shape[0]).numpy()\n",
    "\n",
    "print(\"Input shape: \", input.shape)\n",
    "expected = model.dw_conv(input).detach()\n",
    "print(\"Expected shape: \", expected.shape)\n",
    "\n",
    "padded = F.pad(input, (1,1,1,1), \"constant\", 0)\n",
    "padded = padded.squeeze().numpy().transpose((1, 2, 0))\n",
    "weights = weights.transpose((1, 2, 0))\n",
    "\n",
    "quantized_image = input.squeeze().numpy().transpose((1,2,0)) * 10**EXPONENT\n",
    "# quantized_image = padded * 10**EXPONENT\n",
    "quantized_weights = weights * 10**EXPONENT\n",
    "\n",
    "actual, rem = PaddedDepthwiseConv(7, 7, 3, 3, 3, 1, 10**EXPONENT, quantized_image.round(), quantized_weights.round(), bias)\n",
    "\n",
    "test_output = np.array([[[int(out) / 10**EXPONENT for out in asdf] for asdf in asdfasdf] for asdfasdf in actual])\n",
    "test_output = test_output[1:6, 1:6, :]\n",
    "\n",
    "# expected = expected.squeeze().transpose((1, 2, 0))\n",
    "# expected = F.pad(expected.squeeze(), (1,1,1,1)).numpy().transpose((1,2,0))\n",
    "expected = expected.squeeze().numpy().transpose((1,2,0))\n",
    "print(\"Expected shape: \", expected.shape)\n",
    "print(\"test_output shape: \", test_output.shape)\n",
    "\n",
    "# expected = expected * 10**EXPONENT\n",
    "assert(np.allclose(expected, test_output, atol=0.00001))\n",
    "print(\"--------------------------------\")\n",
    "\n",
    "print(\"Input shape: \", input.shape)\n",
    "expected = model.dw_conv(input).detach()\n",
    "print(\"expected shape: \", expected.shape)\n",
    "expected = model.dw_conv(expected).detach()\n",
    "expected = expected.squeeze().numpy().transpose((1,2,0))\n",
    "print(\"expected shape: \", expected.shape)\n",
    "dw_input, rem = PaddedDepthwiseConv(7, 7, 3, 3, 3, 1, 10**EXPONENT, quantized_image.round(), quantized_weights.round(), bias)\n",
    "print(\"1st shape: \", np.array(dw_input).shape)\n",
    "actual, rem = PaddedDepthwiseConv(7, 7, 3, 3, 3, 1, 10**EXPONENT, dw_input, quantized_weights.round(), bias)\n",
    "print(\"2nd shape: \", np.array(actual).shape)\n",
    "test_output = np.array([[[int(out) / 10**EXPONENT for out in asdf] for asdf in asdfasdf] for asdfasdf in actual])\n",
    "test_output = test_output[2:5, 2:5, :]\n",
    "\n",
    "\n",
    "# expected = F.pad(expected.squeeze(), (2,2,2,2)).numpy().transpose((1,2,0))\n",
    "print(\"actual shape: \", np.array(actual).shape)\n",
    "print(\"expected shape: \", np.array(expected).shape)\n",
    "assert(np.allclose(expected, test_output, atol=0.00001))\n",
    "\n",
    "p = CIRCOM_PRIME\n",
    "# quantized_image = np.array(dw_input)\n",
    "# Input = [[[str(int(dw_input[i][j][k]) % p) for k in range(quantized_image.shape[2])] for j in range(quantized_image.shape[1])] for i in range(quantized_image.shape[0])]\n",
    "# Weights = [[[str(int(quantized_weights[i][j][k].round()) % p) for k in range(quantized_weights.shape[2])] for j in range(quantized_weights.shape[1])] for i in range(quantized_weights.shape[0])]\n",
    "# # Out = [[[str(int(actual[i][j][k]) % p) for k in range(3)] for j in range(7)] for i in range(7)]\n",
    "\n",
    "# test_output = np.array([[[int(out) for out in asdf] for asdf in asdfasdf] for asdfasdf in actual])\n",
    "# print(\"test_output shape: \", np.array(test_output).shape)\n",
    "# # # test_output = test_output[1:6, 1:6, :]\n",
    "# Out = [[[str(int(test_output[i][j][k]) % p) for k in range(test_output.shape[2])] for j in range(test_output.shape[1])] for i in range(test_output.shape[0])]\n",
    "# Rem = [[[str(rem[i][j][k]) for k in range(test_output.shape[2])] for j in range(test_output.shape[1])] for i in range(test_output.shape[0])]\n",
    "# print(\"OUT SHAPE: \", np.array(Out).shape)\n",
    "# print(\"REM SHAPE: \", np.array(Rem).shape)\n",
    "\n",
    "# input_json_path = \"padded_input.json\"\n",
    "# with open(input_json_path, \"w\") as input_file:\n",
    "#     json.dump({\n",
    "#                \"in\": Input,\n",
    "#                \"dw_conv_weights\": Weights,\n",
    "#                \"dw_conv_bias\": circuit_bias,\n",
    "#                \"dw_conv_remainder\": Rem,\n",
    "#                \"dw_conv_out\": Out,\n",
    "#               },\n",
    "#               input_file)\n",
    "\n",
    "\n",
    "# os.chdir(\"circuits\")\n",
    "# !./padded/padded_cpp/padded ../padded_input.json head.wtns\n",
    "# # # !npx snarkjs groth16 prove ./origDepthwiseConv2d/circuit_final.zkey head.wtns proof.json public_test.json\n",
    "# os.chdir(\"../\")\n",
    "\n",
    "# # print(\"OK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "id": "48256a72-45ff-42bd-a29a-0aec9d0faa5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def PaddedDepthwiseConv(nRows, nCols, nChannels, nFilters, kernelSize, strides, n, input, weights, bias):\n",
    "    assert(nFilters % nChannels == 0)\n",
    "    outRows = (nRows - kernelSize)//strides + 1\n",
    "    outCols = (nCols - kernelSize)//strides + 1\n",
    "    \n",
    "    # out = np.zeros((outRows, outCols, nFilters))\n",
    "    Input = [[[str(int(input[i][j][k]) % p) for k in range(nChannels)] for j in range(nCols)] for i in range(nRows)]\n",
    "    Weights = [[[str(int(weights[i][j][k].round()) % p) for k in range(weights.shape[2])] for j in range(weights.shape[1])] for i in range(weights.shape[0])]\n",
    "    out = [[[0 for _ in range(nFilters)] for _ in range(nCols)] for _ in range(nRows)]\n",
    "    Bias = bias.round().astype(int).astype(str).tolist()\n",
    "    out_str = [[[str(0) for _ in range(nFilters)] for _ in range(nCols)] for _ in range(nRows)]\n",
    "    remainder = [[[str(0) for _ in range(nFilters)] for _ in range(nCols)] for _ in range(nRows)]\n",
    "    \n",
    "    for row in range(outRows):\n",
    "        for col in range(outCols):\n",
    "            for channel in range(nChannels):\n",
    "                for x in range(kernelSize):\n",
    "                    for y in range(kernelSize):\n",
    "                        out[row+1][col+1][channel] += int(input[row*strides+x][col*strides+y][channel]) * int(weights[x, y, channel])\n",
    "                \n",
    "                out[row+1][col+1][channel] += int(bias[channel])\n",
    "                remainder[row+1][col+1][channel] = str(int(out[row+1][col+1][channel] % n))\n",
    "                out[row+1][col+1][channel] = int(out[row+1][col+1][channel] // n)\n",
    "                out_str[row+1][col+1][channel] = str(out[row+1][col+1][channel] % p)\n",
    "                            \n",
    "    return Input, Weights, Bias, out_str, out, remainder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "id": "bcc591b9-c270-4a6d-a1bd-ac926f5b8641",
   "metadata": {},
   "outputs": [],
   "source": [
    "def PointwiseConv2d(nRows, nCols, nChannels, nFilters, strides, n, input, weights, bias):\n",
    "    kernelSize = 1\n",
    "    outRows = (nRows - kernelSize)//strides + 1\n",
    "    outCols = (nCols - kernelSize)//strides + 1\n",
    "    # out = [[[0 for _ in range(nFilters)] for _ in range(nCols)] for _ in range(nRows)]\n",
    "    # remainder = [[[None for _ in range(nFilters)] for _ in range(nCols)] for _ in range(nRows)]\n",
    "    \n",
    "    Input = [[[str(int(input[i][j][k]) % p) for k in range(nChannels)] for j in range(nCols)] for i in range(nRows)]\n",
    "    # Weights = [[[str(int(weights[i][j][k].round()) % p) for k in range(weights.shape[2])] for j in range(weights.shape[1])] for i in range(weights.shape[0])]\n",
    "    Weights = [[str(int(weights[i][j].round()) % p)for j in range(weights.shape[1])] for i in range(weights.shape[0])]\n",
    "    out = [[[0 for _ in range(nFilters)] for _ in range(nCols)] for _ in range(nRows)]\n",
    "    Bias = bias.round().astype(int).astype(str).tolist()\n",
    "    out_str = [[[str(0) for _ in range(nFilters)] for _ in range(nCols)] for _ in range(nRows)]\n",
    "    remainder = [[[str(0) for _ in range(nFilters)] for _ in range(nCols)] for _ in range(nRows)]\n",
    "    \n",
    "    for row in range(outRows):\n",
    "        for col in range(outCols):\n",
    "            for filter in range(nFilters):\n",
    "                for k in range(nChannels):\n",
    "                    out[row][col][filter] += int(input[row*strides][col*strides][k]) * int(weights[k, filter])\n",
    "                    \n",
    "                out[row][col][filter] += int(bias[filter])\n",
    "                remainder[row][col][filter] = str(int(out[row][col][filter] % n))\n",
    "                out[row][col][filter] = int(out[row][col][filter] // n)\n",
    "                out_str[row][col][filter] = str(out[row][col][filter] % p)\n",
    "                            \n",
    "    # return out, remainder\n",
    "    return Input, Weights, Bias, out_str, out, remainder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "id": "8bcc73b0-e18d-403c-a05a-664406df0b99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape:  torch.Size([1, 3, 7, 7])\n",
      "Expected shape:  torch.Size([1, 3, 5, 5])\n",
      "point weights shape:  (6, 3, 1, 1)\n",
      "Input shape:  torch.Size([1, 3, 7, 7])\n",
      "Depth Expected shape:  torch.Size([1, 3, 5, 5])\n",
      "Point Expected shape:  (5, 5, 6)\n",
      "point weights shape:  (3, 6)\n",
      "START\n",
      "PRIME - 1 28948022309329048855892746252171976963363056481941647379679742748393362948096\n",
      "------------------------------------------------\n",
      "------------------------------------------------\n",
      "dw_conv done\n",
      "END\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "depth_weights = model.dw_conv[0].weight.squeeze().detach().numpy()\n",
    "depth_bias = torch.zeros(depth_weights.shape[0]).numpy()\n",
    "\n",
    "print(\"Input shape: \", input.shape)\n",
    "expected = model.dw_conv[0](input).detach()\n",
    "print(\"Expected shape: \", expected.shape)\n",
    "\n",
    "depth_weights = depth_weights.transpose((1, 2, 0))\n",
    "\n",
    "quantized_image = input.squeeze().numpy().transpose((1,2,0)) * 10**EXPONENT\n",
    "# quantized_image = padded * 10**EXPONENT\n",
    "quantized_depth_weights = depth_weights * 10**EXPONENT\n",
    "\n",
    "circuit_in, circuit_depth_weights, circuit_depth_bias, circuit_depth_out, depth_out, circuit_depth_remainder = PaddedDepthwiseConv(7, 7, 3, 3, 3, 1, 10**EXPONENT, quantized_image.round(), quantized_depth_weights.round(), depth_bias)\n",
    "\n",
    "test_output = np.array([[[int(out) / 10**EXPONENT for out in asdf] for asdf in asdfasdf] for asdfasdf in depth_out])\n",
    "test_output = test_output[1:6, 1:6, :]\n",
    "\n",
    "expected = expected.squeeze().numpy().transpose((1,2,0))\n",
    "\n",
    "assert(np.allclose(expected, test_output, atol=0.00001))\n",
    "\n",
    "# point_weights = model.pw_conv[0].weight.squeeze().detach().numpy()\n",
    "point_weights = model.pw_conv[0].weight.detach().numpy()\n",
    "print(\"point weights shape: \", point_weights.shape)\n",
    "point_bias = torch.zeros(point_weights.shape[0]).numpy()\n",
    "\n",
    "print(\"Input shape: \", input.shape)\n",
    "depth_expected = model.dw_conv[0](input)\n",
    "point_expected = model.pw_conv[0](depth_expected)\n",
    "print(\"Depth Expected shape: \", depth_expected.shape)\n",
    "# print(\"Point Expected shape: \", point_expected.shape)\n",
    "\n",
    "point_expected = point_expected.squeeze().detach().numpy().transpose((1,2,0))\n",
    "print(\"Point Expected shape: \", point_expected.shape)\n",
    "\n",
    "point_weights = point_weights.transpose((2, 3, 1, 0)).squeeze()\n",
    "quantized_point_weights = point_weights * 10**EXPONENT\n",
    "print(\"point weights shape: \", quantized_point_weights.shape)\n",
    "\n",
    "# circuit_in, circuit_depth_weights, circuit_depth_bias, circuit_depth_out, depth_out, circuit_depth_remainder = PaddedDepthwiseConv(7, 7, 3, 3, 3, 1, 10**EXPONENT, quantized_image.round(), quantized_depth_weights.round(), depth_bias)\n",
    "point_input, circuit_point_weights, circuit_point_bias, circuit_point_out, point_out, circuit_point_remainder = PointwiseConv2d(7, 7, 3, 6, 1, 10**EXPONENT, depth_out, quantized_point_weights.round(), point_bias)\n",
    "\n",
    "test_output = np.array([[[int(out) / 10**EXPONENT for out in asdf] for asdf in asdfasdf] for asdfasdf in point_out])\n",
    "test_output = test_output[1:6, 1:6, :]\n",
    "\n",
    "assert(np.allclose(point_expected, test_output, atol=0.00001))\n",
    "\n",
    "input_json_path = \"padded_input.json\"\n",
    "with open(input_json_path, \"w\") as input_file:\n",
    "    json.dump({\n",
    "               \"in\": circuit_in,\n",
    "               \"dw_conv_weights\": circuit_depth_weights,\n",
    "               \"dw_conv_bias\": circuit_depth_bias,\n",
    "               \"dw_conv_remainder\": circuit_depth_remainder,\n",
    "               \"dw_conv_out\": circuit_depth_out,\n",
    "        \n",
    "               \"pw_conv_weights\": circuit_point_weights,\n",
    "               \"pw_conv_bias\": circuit_point_bias,\n",
    "               \"pw_conv_remainder\": circuit_point_remainder,\n",
    "               \"pw_conv_out\": circuit_point_out,\n",
    "              },\n",
    "              input_file)\n",
    "\n",
    "\n",
    "os.chdir(\"circuits\")\n",
    "!./padded/padded_cpp/padded ../padded_input.json head.wtns\n",
    "# # !npx snarkjs groth16 prove ./origDepthwiseConv2d/circuit_final.zkey head.wtns proof.json public_test.json\n",
    "os.chdir(\"../\")\n",
    "\n",
    "print(\"OK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 581,
   "id": "ad9186ba-a226-4c99-b60c-929a6b4b2f05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape:  torch.Size([1, 3, 7, 7])\n",
      "Expected shape:  torch.Size([1, 3, 5, 5])\n",
      "a shape:  torch.Size([3])\n",
      "b shape:  torch.Size([3])\n",
      "point weights shape:  (6, 3, 1, 1)\n",
      "Input shape:  torch.Size([1, 3, 7, 7])\n",
      "Depth Expected shape:  torch.Size([1, 3, 5, 5])\n",
      "Point Expected shape:  (5, 5, 6)\n",
      "point weights shape:  (3, 6)\n",
      "START\n",
      "PRIME - 1 28948022309329048855892746252171976963363056481941647379679742748393362948096\n",
      "------------------------------------------------\n",
      "------------------------------------------------\n",
      "dw_conv done\n",
      "dw batch norm done\n",
      "END\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "# Depthwise convolution\n",
    "depth_weights = model.dw_conv[0].weight.squeeze().detach().numpy()\n",
    "depth_bias = torch.zeros(depth_weights.shape[0]).numpy()\n",
    "\n",
    "print(\"Input shape: \", input.shape)\n",
    "expected = model.dw_conv[0](input).detach()\n",
    "print(\"Expected shape: \", expected.shape)\n",
    "\n",
    "depth_weights = depth_weights.transpose((1, 2, 0))\n",
    "\n",
    "quantized_image = input.squeeze().numpy().transpose((1,2,0)) * 10**EXPONENT\n",
    "# quantized_image = padded * 10**EXPONENT\n",
    "quantized_depth_weights = depth_weights * 10**EXPONENT\n",
    "\n",
    "circuit_in, circuit_depth_weights, circuit_depth_bias, circuit_depth_out, depth_out, circuit_depth_remainder = PaddedDepthwiseConv(7, 7, 3, 3, 3, 1, 10**EXPONENT, quantized_image.round(), quantized_depth_weights.round(), depth_bias)\n",
    "\n",
    "test_output = np.array([[[int(out) / 10**EXPONENT for out in asdf] for asdf in asdfasdf] for asdfasdf in depth_out])\n",
    "test_output = test_output[1:6, 1:6, :]\n",
    "\n",
    "expected = expected.squeeze().numpy().transpose((1,2,0))\n",
    "\n",
    "assert(np.allclose(expected, test_output, atol=0.00001))\n",
    "\n",
    "# Batch normalization step\n",
    "# BATCH NORM CONSTANTS\n",
    "model.eval()\n",
    "gamma = model.dw_conv[1].weight\n",
    "beta = model.dw_conv[1].bias\n",
    "mean = model.dw_conv[1].running_mean\n",
    "var = model.dw_conv[1].running_var\n",
    "eps = model.dw_conv[1].eps\n",
    "\n",
    "a = (gamma/(var+eps)**.5).detach()\n",
    "print('a shape: ', a.shape)\n",
    "b = (beta-gamma*mean/(var+eps)**.5).detach()\n",
    "print('b shape: ', b.shape)\n",
    "\n",
    "b = b.tolist()\n",
    "\n",
    "quantized_a = (a * 10**EXPONENT).tolist()\n",
    "quantized_b = [bi * 10**(2*EXPONENT) for bi in b]\n",
    "\n",
    "out = model.dw_conv[0](input)\n",
    "expected = model.dw_conv[1](out)\n",
    "\n",
    "expected = torch.permute(expected.squeeze(), (1, 2, 0))\n",
    "\n",
    "quantized_in = torch.permute(out.squeeze(), (1, 2, 0)) * 10**EXPONENT\n",
    "\n",
    "X, A, B, actual, remainder = BatchNormalizationInt(5, 5, 3, 10**EXPONENT, quantized_in, quantized_a, quantized_b)\n",
    "test_output = [[[from_circom(int(out)) / 10**EXPONENT for out in vec] for vec in matrix] for matrix in actual]\n",
    "\n",
    "assert(torch.allclose(torch.Tensor(test_output), expected, atol=1e-6))\n",
    "\n",
    "# quantized_in = [[[from_circom(int(out)) for out in vec] for vec in matrix] for matrix in circuit_conv_out]\n",
    "\n",
    "_, circuit_bn_a, circuit_bn_b, circuit_bn_out, circuit_bn_remainder = BatchNormalizationInt(7, 7, 3, 10**EXPONENT, depth_out, quantized_a, quantized_b)\n",
    "\n",
    "test_output = np.array([[[int(out) / 10**EXPONENT for out in asdf] for asdf in asdfasdf] for asdfasdf in circuit_bn_out])\n",
    "test_output = test_output[1:6, 1:6, :]\n",
    "\n",
    "assert(torch.allclose(torch.Tensor(test_output), expected, atol=1e-6))\n",
    "\n",
    "# Pointwise convolution\n",
    "# point_weights = model.pw_conv[0].weight.squeeze().detach().numpy()\n",
    "point_weights = model.pw_conv[0].weight.detach().numpy()\n",
    "print(\"point weights shape: \", point_weights.shape)\n",
    "point_bias = torch.zeros(point_weights.shape[0]).numpy()\n",
    "\n",
    "print(\"Input shape: \", input.shape)\n",
    "depth_expected = model.dw_conv[0](input)\n",
    "bn_expected = model.dw_conv[1](depth_expected)\n",
    "point_expected = model.pw_conv[0](bn_expected)\n",
    "print(\"Depth Expected shape: \", depth_expected.shape)\n",
    "# print(\"Point Expected shape: \", point_expected.shape)\n",
    "\n",
    "point_expected = point_expected.squeeze().detach().numpy().transpose((1,2,0))\n",
    "print(\"Point Expected shape: \", point_expected.shape)\n",
    "\n",
    "point_weights = point_weights.transpose((2, 3, 1, 0)).squeeze()\n",
    "quantized_point_weights = point_weights * 10**EXPONENT\n",
    "print(\"point weights shape: \", quantized_point_weights.shape)\n",
    "\n",
    "# circuit_in, circuit_depth_weights, circuit_depth_bias, circuit_depth_out, depth_out, circuit_depth_remainder = PaddedDepthwiseConv(7, 7, 3, 3, 3, 1, 10**EXPONENT, quantized_image.round(), quantized_depth_weights.round(), depth_bias)\n",
    "point_input, circuit_point_weights, circuit_point_bias, circuit_point_out, point_out, circuit_point_remainder = PointwiseConv2d(7, 7, 3, 6, 1, 10**EXPONENT, circuit_bn_out, quantized_point_weights.round(), point_bias)\n",
    "\n",
    "test_output = np.array([[[int(out) / 10**EXPONENT for out in asdf] for asdf in asdfasdf] for asdfasdf in point_out])\n",
    "test_output = test_output[1:6, 1:6, :]\n",
    "\n",
    "assert(np.allclose(point_expected, test_output, atol=0.00001))\n",
    "\n",
    "input_json_path = \"padded_input.json\"\n",
    "with open(input_json_path, \"w\") as input_file:\n",
    "    json.dump({\n",
    "               \"in\": circuit_in,\n",
    "               \"dw_conv_weights\": circuit_depth_weights,\n",
    "               \"dw_conv_bias\": circuit_depth_bias,\n",
    "               \"dw_conv_remainder\": circuit_depth_remainder,\n",
    "               \"dw_conv_out\": circuit_depth_out,\n",
    "        \n",
    "               \"dw_bn_a\": circuit_bn_a,\n",
    "               \"dw_bn_b\": circuit_bn_b,\n",
    "               \"dw_bn_remainder\": circuit_bn_remainder,\n",
    "               \"dw_bn_out\": circuit_bn_out,\n",
    "        \n",
    "               \"pw_conv_weights\": circuit_point_weights,\n",
    "               \"pw_conv_bias\": circuit_point_bias,\n",
    "               \"pw_conv_remainder\": circuit_point_remainder,\n",
    "               \"pw_conv_out\": circuit_point_out,\n",
    "              },\n",
    "              input_file)\n",
    "\n",
    "\n",
    "os.chdir(\"circuits\")\n",
    "!./padded/padded_cpp/padded ../padded_input.json head.wtns\n",
    "# # !npx snarkjs groth16 prove ./origDepthwiseConv2d/circuit_final.zkey head.wtns proof.json public_test.json\n",
    "os.chdir(\"../\")\n",
    "\n",
    "print(\"OK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 569,
   "id": "7f63e50a-6e6f-4897-a5be-97e9330c0c5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array :  (7, 7, 3)\n",
      "out:  (5, 5, 3)\n",
      "expected:  torch.Size([5, 5, 3])\n"
     ]
    }
   ],
   "source": [
    "print(\"array : \", np.array(depth_out).shape)\n",
    "\n",
    "_, circuit_bn_a, circuit_bn_b, circuit_bn_out, circuit_bn_remainder = BatchNormalizationInt(7, 7, 3, 10**EXPONENT, depth_out, quantized_a, quantized_b)\n",
    "\n",
    "test_output = np.array([[[int(out) / 10**EXPONENT for out in asdf] for asdf in asdfasdf] for asdfasdf in circuit_bn_out])\n",
    "test_output = test_output[1:6, 1:6, :]\n",
    "\n",
    "assert(torch.allclose(torch.Tensor(test_output), expected, atol=1e-6))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb20792-ff40-4c22-8acc-73b7a96f594f",
   "metadata": {},
   "source": [
    "# Padded convolution 2 iterations test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "id": "8bcbe0be-2d60-4ebf-8d0f-814d63b76dfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape:  torch.Size([1, 3, 7, 7])\n",
      "Expected shape:  torch.Size([1, 3, 5, 5])\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[478], line 50\u001b[0m\n\u001b[1;32m     47\u001b[0m test_output \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([[[\u001b[38;5;28mint\u001b[39m(out) \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m10\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mEXPONENT \u001b[38;5;28;01mfor\u001b[39;00m out \u001b[38;5;129;01min\u001b[39;00m vec] \u001b[38;5;28;01mfor\u001b[39;00m vec \u001b[38;5;129;01min\u001b[39;00m matrix] \u001b[38;5;28;01mfor\u001b[39;00m matrix \u001b[38;5;129;01min\u001b[39;00m actual])\n\u001b[1;32m     48\u001b[0m test_output \u001b[38;5;241m=\u001b[39m test_output[\u001b[38;5;241m1\u001b[39m:\u001b[38;5;241m6\u001b[39m, \u001b[38;5;241m1\u001b[39m:\u001b[38;5;241m6\u001b[39m, :]\n\u001b[0;32m---> 50\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m(np\u001b[38;5;241m.\u001b[39mallclose(test_output, expected, atol\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-6\u001b[39m))\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Depthwise convolution\n",
    "depth_weights = model.dw_conv[0].weight.squeeze().detach().numpy()\n",
    "depth_bias = torch.zeros(depth_weights.shape[0]).numpy()\n",
    "\n",
    "print(\"Input shape: \", input.shape)\n",
    "expected = model.dw_conv[0](input).detach()\n",
    "print(\"Expected shape: \", expected.shape)\n",
    "\n",
    "depth_weights = depth_weights.transpose((1, 2, 0))\n",
    "\n",
    "quantized_image = input.squeeze().numpy().transpose((1,2,0)) * 10**EXPONENT\n",
    "# quantized_image = padded * 10**EXPONENT\n",
    "quantized_depth_weights = depth_weights * 10**EXPONENT\n",
    "\n",
    "circuit_in, circuit_depth_weights, circuit_depth_bias, circuit_depth_out, depth_out, circuit_depth_remainder = PaddedDepthwiseConv(7, 7, 3, 3, 3, 1, 10**EXPONENT, quantized_image.round(), quantized_depth_weights.round(), depth_bias)\n",
    "\n",
    "test_output = np.array([[[int(out) / 10**EXPONENT for out in asdf] for asdf in asdfasdf] for asdfasdf in depth_out])\n",
    "test_output = test_output[1:6, 1:6, :]\n",
    "\n",
    "expected = expected.squeeze().numpy().transpose((1,2,0))\n",
    "\n",
    "assert(np.allclose(expected, test_output, atol=0.00001))\n",
    "\n",
    "# Batch normalization step\n",
    "gamma = model.dw_conv[1].weight\n",
    "beta = model.dw_conv[1].bias\n",
    "mean = model.dw_conv[1].running_mean\n",
    "var = model.dw_conv[1].running_var\n",
    "eps = model.dw_conv[1].eps\n",
    "\n",
    "a = (gamma/(var+eps)**.5).detach()\n",
    "b = (beta-gamma*mean/(var+eps)**.5).detach().tolist()\n",
    "\n",
    "quantized_a = (a * 10**EXPONENT).tolist()\n",
    "quantized_b = [bi * 10**(2*EXPONENT) for bi in b]\n",
    "\n",
    "# # BATCH NORM USING PYTORCH OUTPUT\n",
    "depth_conv_expected = model.dw_conv[0](input)\n",
    "depth_bn_expected = model.dw_conv[1](depth_conv_expected)\n",
    "\n",
    "depth_bn_expected = torch.permute(depth_bn_expected.squeeze(), (1, 2, 0))\n",
    "\n",
    "# quantized_in = [[[from_circom(int(out)) for out in vec] for vec in matrix] for matrix in depth_out]\n",
    "\n",
    "_, circuit_bn_a, circuit_bn_b, circuit_bn_out, circuit_bn_remainder = BatchNormalizationInt(7, 7, 3, 10**EXPONENT, depth_out, quantized_a, quantized_b)\n",
    "\n",
    "test_output = np.array([[[int(out) / 10**EXPONENT for out in vec] for vec in matrix] for matrix in actual])\n",
    "test_output = test_output[1:6, 1:6, :]\n",
    "\n",
    "assert(np.allclose(test_output, expected, atol=1e-6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 557,
   "id": "5b7d7f64-c2c6-420d-aae5-084443e83cc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a shape:  torch.Size([3])\n",
      "b shape:  torch.Size([3])\n",
      "Input shape:  torch.Size([1, 3, 7, 7])\n",
      "out shape:  torch.Size([1, 3, 5, 5])\n",
      "expected shape:  torch.Size([1, 3, 5, 5])\n",
      "expected shape:  torch.Size([5, 5, 3])\n",
      "torch.Size([5, 5, 3])\n"
     ]
    }
   ],
   "source": [
    "# BATCH NORM CONSTANTS\n",
    "model.eval()\n",
    "gamma = model.dw_conv[1].weight\n",
    "beta = model.dw_conv[1].bias\n",
    "mean = model.dw_conv[1].running_mean\n",
    "var = model.dw_conv[1].running_var\n",
    "eps = model.dw_conv[1].eps\n",
    "\n",
    "a = (gamma/(var+eps)**.5).detach()\n",
    "print('a shape: ', a.shape)\n",
    "b = (beta-gamma*mean/(var+eps)**.5).detach()\n",
    "print('b shape: ', b.shape)\n",
    "\n",
    "b = b.tolist()\n",
    "\n",
    "quantized_a = (a * 10**EXPONENT).tolist()\n",
    "quantized_b = [bi * 10**(2*EXPONENT) for bi in b]\n",
    "\n",
    "# BATCH NORM USING PYTORCH OUTPUT\n",
    "# image, label = testset[0]\n",
    "# image = image.unsqueeze(0)\n",
    "print(\"Input shape: \", input.shape)\n",
    "out = model.dw_conv[0](input)\n",
    "print(\"out shape: \", out.shape)\n",
    "expected = model.dw_conv[1](out)\n",
    "print(\"expected shape: \", expected.shape)\n",
    "\n",
    "expected = torch.permute(expected.squeeze(), (1, 2, 0))\n",
    "\n",
    "print(\"expected shape: \", expected.shape)\n",
    "\n",
    "quantized_in = torch.permute(out.squeeze(), (1, 2, 0)) * 10**EXPONENT\n",
    "print(quantized_in.shape)\n",
    "\n",
    "X, A, B, actual, remainder = BatchNormalizationInt(5, 5, 3, 10**EXPONENT, quantized_in, quantized_a, quantized_b)\n",
    "test_output = [[[from_circom(int(out)) / 10**EXPONENT for out in vec] for vec in matrix] for matrix in actual]\n",
    "assert(torch.allclose(torch.Tensor(test_output), expected, atol=1e-6))\n",
    "\n",
    "# # BATCH NORM USING CIRCUIT CONV OUTPUT\n",
    "quantized_in = [[[from_circom(int(out)) for out in vec] for vec in matrix] for matrix in circuit_conv_out]\n",
    "\n",
    "_, circuit_bn_a, circuit_bn_b, circuit_bn_out, circuit_bn_remainder = BatchNormalizationInt(5, 5, 3, 10**EXPONENT, quantized_in, quantized_a, quantized_b)\n",
    "\n",
    "test_output = [[[int(from_circom(int(out))) / 10**EXPONENT for out in vec] for vec in matrix] for matrix in actual]\n",
    "\n",
    "assert(torch.allclose(torch.Tensor(test_output), expected, atol=1e-6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93546612-0766-481b-8655-ff2c3c1eb0f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 548,
   "id": "09269d35-40c6-4f44-856c-866c02abb16b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape:  torch.Size([1, 3, 7, 7])\n",
      "out shape:  torch.Size([1, 3, 5, 5])\n",
      "expected shape:  torch.Size([1, 3, 5, 5])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([-0.2848,  0.5557, -0.4013], grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 548,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gamma = model.dw_conv[1].weight\n",
    "beta = model.dw_conv[1].bias\n",
    "mean = model.dw_conv[1].running_mean\n",
    "var = model.dw_conv[1].running_var\n",
    "eps = model.dw_conv[1].eps\n",
    "\n",
    "inp = torch.permute(out.squeeze(), (1, 2, 0))\n",
    "\n",
    "print(\"Input shape: \", input.shape)\n",
    "out = model.dw_conv[0](input)\n",
    "print(\"out shape: \", out.shape)\n",
    "expected = model.dw_conv[1](out)\n",
    "print(\"expected shape: \", out.shape)\n",
    "\n",
    "expected = torch.permute(expected.squeeze(), (1, 2, 0))\n",
    "\n",
    "\n",
    "def batch_norm(X, gamma, beta, moving_mean, moving_var, eps):\n",
    "    X_hat = (X - moving_mean) / torch.sqrt(moving_var + eps)\n",
    "    Y = gamma * X_hat + beta  # Scale and shift\n",
    "    return Y\n",
    "\n",
    "# X, A, B, actual, remainder = BatchNormalizationInt(5, 5, 3, 10**EXPONENT, quantized_in, quantized_a, quantized_b)\n",
    "Y = batch_norm(inp, gamma, beta, mean, var, eps)\n",
    "Y[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 536,
   "id": "30e4be98-de14-4dfb-b60e-578a0847a98b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Conv2d(3, 3, kernel_size=(3, 3), stride=(1, 1), groups=3, bias=False)"
      ]
     },
     "execution_count": 536,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.dw_conv[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 537,
   "id": "9e411e1b-55ea-4362-82d8-98a8f4fdb1f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)"
      ]
     },
     "execution_count": 537,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.dw_conv[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 549,
   "id": "3c9f786a-7049-472e-9b35-5f1f8ebcb860",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape:  torch.Size([1, 3, 7, 7])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[[-0.2286, -0.3418, -0.2930,  1.2544,  0.2140],\n",
       "          [ 1.3454, -0.0814, -1.0314, -1.2243, -0.0749],\n",
       "          [ 0.5663, -0.0538,  0.0425,  1.5004,  0.1065],\n",
       "          [ 0.1701, -1.2069, -0.5410, -1.0333,  1.0874],\n",
       "          [-0.6385,  0.2057,  0.2992,  0.2291,  0.1995]],\n",
       "\n",
       "         [[ 0.4244, -0.2598, -0.5925, -0.5645, -1.0676],\n",
       "          [ 0.9458,  0.2964,  0.8387, -0.2701, -0.1671],\n",
       "          [-0.2637, -0.3243,  0.5020, -0.2796, -0.0948],\n",
       "          [ 0.1807, -0.5408,  0.2431,  0.3024,  0.2439],\n",
       "          [ 0.6838,  0.5855,  0.3536,  0.1841, -0.2822]],\n",
       "\n",
       "         [[-0.5071, -0.9027, -0.0069, -1.1867, -1.1653],\n",
       "          [-0.0524, -0.3024,  0.1501,  0.3752,  0.6453],\n",
       "          [-0.4639, -1.0154,  0.1621, -0.1855, -1.2875],\n",
       "          [-0.5402, -0.0055,  0.8623,  0.1427, -0.0833],\n",
       "          [-0.1914, -0.4314, -0.2682, -0.4835, -1.1620]]]],\n",
       "       grad_fn=<ConvolutionBackward0>)"
      ]
     },
     "execution_count": 549,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Input shape: \", input.shape)\n",
    "out = model.dw_conv[0](input)\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 550,
   "id": "1d300e24-ce0e-41ea-b41a-d59df9b28c6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.2848,  0.5557, -0.4013],\n",
       "         [-0.4188, -0.4036, -0.9281],\n",
       "         [-0.3611, -0.8701,  0.2651],\n",
       "         [ 1.4689, -0.8308, -1.3064],\n",
       "         [ 0.2385, -1.5362, -1.2780]],\n",
       "\n",
       "        [[ 1.5766,  1.2868,  0.2045],\n",
       "         [-0.1108,  0.3762, -0.1285],\n",
       "         [-1.2343,  1.1366,  0.4742],\n",
       "         [-1.4624, -0.4180,  0.7740],\n",
       "         [-0.1031, -0.2737,  1.1338]],\n",
       "\n",
       "        [[ 0.6552, -0.4091, -0.3436],\n",
       "         [-0.0782, -0.4941, -1.0783],\n",
       "         [ 0.0357,  0.6645,  0.4902],\n",
       "         [ 1.7598, -0.4314,  0.0272],\n",
       "         [ 0.1114, -0.1723, -1.4407]],\n",
       "\n",
       "        [[ 0.1866,  0.2140, -0.4453],\n",
       "         [-1.4419, -0.7976,  0.2669],\n",
       "         [-0.6543,  0.3015,  1.4229],\n",
       "         [-1.2365,  0.3846,  0.4644],\n",
       "         [ 1.2714,  0.3026,  0.1633]],\n",
       "\n",
       "        [[-0.7696,  0.9195,  0.0193],\n",
       "         [ 0.2287,  0.7816, -0.3003],\n",
       "         [ 0.3393,  0.4564, -0.0830],\n",
       "         [ 0.2564,  0.2188, -0.3697],\n",
       "         [ 0.2214, -0.4351, -1.2735]]], grad_fn=<PermuteBackward0>)"
      ]
     },
     "execution_count": 550,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expected = model.dw_conv[1](out)\n",
    "expected = torch.permute(expected.squeeze(), (1, 2, 0))\n",
    "expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 551,
   "id": "c8a62b5d-f4b3-4e94-bb31-df7aabeafff7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape:  torch.Size([5, 5, 3])\n",
      "out shape:  torch.Size([1, 3, 5, 5])\n",
      "expected shape:  torch.Size([1, 3, 5, 5])\n",
      "inp shape:  torch.Size([1, 3, 5, 5])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.2848,  0.5557, -0.4013],\n",
       "         [-0.4188, -0.4036, -0.9281],\n",
       "         [-0.3611, -0.8701,  0.2651],\n",
       "         [ 1.4689, -0.8308, -1.3064],\n",
       "         [ 0.2385, -1.5362, -1.2780]],\n",
       "\n",
       "        [[ 1.5766,  1.2868,  0.2045],\n",
       "         [-0.1108,  0.3762, -0.1285],\n",
       "         [-1.2343,  1.1366,  0.4742],\n",
       "         [-1.4624, -0.4180,  0.7740],\n",
       "         [-0.1031, -0.2737,  1.1338]],\n",
       "\n",
       "        [[ 0.6552, -0.4091, -0.3436],\n",
       "         [-0.0782, -0.4941, -1.0783],\n",
       "         [ 0.0357,  0.6645,  0.4902],\n",
       "         [ 1.7598, -0.4314,  0.0272],\n",
       "         [ 0.1114, -0.1723, -1.4407]],\n",
       "\n",
       "        [[ 0.1866,  0.2140, -0.4453],\n",
       "         [-1.4419, -0.7976,  0.2669],\n",
       "         [-0.6543,  0.3015,  1.4229],\n",
       "         [-1.2365,  0.3846,  0.4644],\n",
       "         [ 1.2714,  0.3026,  0.1633]],\n",
       "\n",
       "        [[-0.7696,  0.9195,  0.0193],\n",
       "         [ 0.2287,  0.7816, -0.3003],\n",
       "         [ 0.3393,  0.4564, -0.0830],\n",
       "         [ 0.2564,  0.2188, -0.3697],\n",
       "         [ 0.2214, -0.4351, -1.2735]]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 551,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()\n",
    "gamma = model.dw_conv[1].weight\n",
    "beta = model.dw_conv[1].bias\n",
    "mean = model.dw_conv[1].running_mean\n",
    "var = model.dw_conv[1].running_var\n",
    "eps = model.dw_conv[1].eps\n",
    "\n",
    "print(\"Input shape: \", inp.shape)\n",
    "out = model.dw_conv[0](input)\n",
    "print(\"out shape: \", out.shape)\n",
    "expected = model.dw_conv[1](out)\n",
    "print(\"expected shape: \", expected.shape)\n",
    "\n",
    "inp = torch.permute(out.squeeze(), (1, 2, 0))\n",
    "\n",
    "print(\"inp shape: \", expected.shape)\n",
    "\n",
    "expected = torch.permute(expected.squeeze(), (1, 2, 0))\n",
    "\n",
    "\n",
    "def batch_norm(X, gamma, beta, moving_mean, moving_var, eps):\n",
    "    X_hat = (X - moving_mean) / torch.sqrt(moving_var + eps)\n",
    "    Y = gamma * X_hat + beta  # Scale and shift\n",
    "    return Y\n",
    "\n",
    "# X, A, B, actual, remainder = BatchNormalizationInt(5, 5, 3, 10**EXPONENT, quantized_in, quantized_a, quantized_b)\n",
    "Y = batch_norm(inp, gamma, beta, mean, var, eps)\n",
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 546,
   "id": "5b772a17-e7b9-4bd6-8d8f-8df8732d7370",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[[-0.280648906516995, 0.542647893714028, -0.408273406971993],\n",
       "  [-0.413034389858258, -0.390205524472173, -0.923371342025755],\n",
       "  [-0.355992831786255, -0.843847884104955, 0.243282119438853],\n",
       "  [1.452674437088021, -0.805660201366962, -1.293269827728386],\n",
       "  [0.236594536713121, -1.491638978956749, -1.265472765669491]],\n",
       " [[1.559073541279003, 1.253596586012221, 0.184012997367891],\n",
       "  [-0.108627698219889, 0.368079008779547, -0.141585330013072],\n",
       "  [-1.219040540851403, 1.107605510267846, 0.447675642422697],\n",
       "  [-1.444541555775974, -0.404228274791963, 0.74083063168836],\n",
       "  [-0.101048000998199, -0.263837167350262, 1.092588554474058]],\n",
       " [[0.648462079822458, -0.395526276695443, -0.35188498618901],\n",
       "  [-0.07641596807634, -0.4781889679011, -1.070248227418558],\n",
       "  [0.036171287174584, 0.648427141709583, 0.46334672319825],\n",
       "  [1.740194270958422, -0.41723000835867, 0.010649627396178],\n",
       "  [0.110916689636747, -0.165309382369868, -1.424574198373823]],\n",
       " [[0.185283062625359, 0.210380095839048, -0.451330482928078],\n",
       "  [-1.424207683943491, -0.773333265508181, 0.245019586720135],\n",
       "  [-0.645805060072058, 0.295481775870049, 1.375224356961999],\n",
       "  [-1.221225080952991, 0.376325056122774, 0.438110127306753],\n",
       "  [1.257467541827159, 0.296526605824726, 0.143735801326391]],\n",
       " [[-0.759757731325975, 0.896400483624512, 0.002897237755524],\n",
       "  [0.226880549695142, 0.762364138337363, -0.309605950386264],\n",
       "  [0.336233567832827, 0.446066672117941, -0.097076311566833],\n",
       "  [0.254302901179524, 0.215002762113181, -0.37741089992117],\n",
       "  [0.219660409340891, -0.420829987981817, -1.261146086299585]]]"
      ]
     },
     "execution_count": 546,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, A, B, actual, remainder = BatchNormalizationInt(5, 5, 3, 10**EXPONENT, quantized_in, quantized_a, quantized_b)\n",
    "test_output = [[[from_circom(int(out)) / 10**EXPONENT for out in vec] for vec in matrix] for matrix in actual]\n",
    "test_output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
