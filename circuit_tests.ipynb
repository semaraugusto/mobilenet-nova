{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6e30ed7-19bd-495b-9c3b-37b7753f3934",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Manual mobilenet implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69987bda-3e35-44f6-9117-a90513a9513b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from models.mobilenet import MyMobileNet\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pytorch_lightning as pl\n",
    "import torchvision\n",
    "import torchvision.transforms.v2 as transforms\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1a79f141-4c15-4283-9a09-4825a99028f9",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Circom defines the range of positives are [0, p/2] and the range of negatives are [(p/2)+1, (p-1)].\n",
    "CIRCOM_PRIME = 21888242871839275222246405745257275088548364400416034343698204186575808495617\n",
    "MAX_POSITIVE = CIRCOM_PRIME // 2\n",
    "MAX_NEGATIVE = MAX_POSITIVE + 1 # The most positive number\n",
    "CIRCOM_NEGATIVE_1 = 21888242871839275222246405745257275088548364400416034343698204186575808495617 - 1\n",
    "EXPONENT = 15\n",
    "\n",
    "def from_circom(x):\n",
    "    if type(x) != int:\n",
    "        x = int(x)\n",
    "    if x > MAX_POSITIVE: \n",
    "        return x - CIRCOM_PRIME\n",
    "    return x\n",
    "    \n",
    "def to_circom(x):\n",
    "    return x % CIRCOM_PRIME\n",
    "    \n",
    "def to_circom_input(array: np.array): \n",
    "    if type(array) != np.array:\n",
    "        array = np.array(array)\n",
    "    int_array = array.round().astype(int)\n",
    "    int_array = to_circom(int_array)\n",
    "    return int_array.astype(str).tolist()\n",
    "\n",
    "\n",
    "def DepthwiseConv(nRows, nCols, nChannels, nFilters, kernelSize, strides, n, input, weights, bias):\n",
    "    assert(nFilters % nChannels == 0)\n",
    "    outRows = (nRows - kernelSize)//strides + 1\n",
    "    outCols = (nCols - kernelSize)//strides + 1\n",
    "    \n",
    "    # out = np.zeros((outRows, outCols, nFilters))\n",
    "    out = [[[0 for _ in range(nFilters)] for _ in range(outCols)] for _ in range(outRows)]\n",
    "    remainder = [[[0 for _ in range(nFilters)] for _ in range(outCols)] for _ in range(outRows)]\n",
    "    # remainder = np.zeros((outRows, outCols, nFilters))\n",
    "    \n",
    "    for row in range(outRows):\n",
    "        for col in range(outCols):\n",
    "            for channel in range(nChannels):\n",
    "                for x in range(kernelSize):\n",
    "                    for y in range(kernelSize):\n",
    "                        out[row][col][channel] += int(input[row*strides+x, col*strides+y, channel]) * int(weights[x, y, channel])\n",
    "                \n",
    "                out[row][col][channel] += int(bias[channel])\n",
    "                remainder[row][col][channel] = str(int(out[row][col][channel] % n))\n",
    "                out[row][col][channel] = int(out[row][col][channel] // n)\n",
    "                            \n",
    "    return out, remainder\n",
    "\n",
    "def PointwiseConv2d(nRows, nCols, nChannels, nFilters, strides, n, input, weights, bias):\n",
    "    kernelSize = 1\n",
    "    outRows = (nRows - kernelSize)//strides + 1\n",
    "    outCols = (nCols - kernelSize)//strides + 1\n",
    "    out = [[[0 for _ in range(nFilters)] for _ in range(outCols)] for _ in range(outRows)]\n",
    "    str_out = [[[0 for _ in range(nFilters)] for _ in range(outCols)] for _ in range(outRows)]\n",
    "    remainder = [[[None for _ in range(nFilters)] for _ in range(outCols)] for _ in range(outRows)]\n",
    "    for row in range(outRows):\n",
    "        for col in range(outCols):\n",
    "            for filter in range(nFilters):\n",
    "                for k in range(nChannels):\n",
    "                    out[row][col][filter] += int(input[row*strides, col*strides, k]) * int(weights[k, filter])\n",
    "                    \n",
    "                out[row][col][filter] += int(bias[filter])\n",
    "                remainder[row][col][filter] = str(int(out[row][col][filter] % n))\n",
    "                out[row][col][filter] = int(out[row][col][filter] // n)\n",
    "                str_out[row][col][filter] = str(out[row][col][filter] % p)\n",
    "                            \n",
    "    return out, str_out, remainder\n",
    "\n",
    "def SeparableConvImpl(nRows, nCols, nChannels, nDepthFilters, nPointFilters, kernelSize, strides, n, input, depthWeights, pointWeights, depthBias, pointBias):\n",
    "    outRows = (nRows - kernelSize)//strides + 1\n",
    "    outCols = (nCols - kernelSize)//strides + 1\n",
    "\n",
    "    depth_out, depth_remainder = DepthwiseConv(nRows, nCols, nChannels, nDepthFilters, kernelSize, strides, n, input, depthWeights, depthBias)\n",
    "    point_out, point_str_out, point_remainder = PointwiseConv2d(outRows, outCols, nChannels, nPointFilters, strides, n, depth_out, pointWeights, pointBias)\n",
    "    return depth_out, depth_remainder, point_out, point_str_out, point_remainder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "faaa5941-253c-4676-94bd-c4dfbb1e524f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetWrapper(torch.utils.data.Dataset):\n",
    "    def __init__(self, subset, transform=None):\n",
    "        self.subset = subset\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        x, y = self.subset[index]\n",
    "        # print(x)\n",
    "        if self.transform:\n",
    "            x = self.transform(x)\n",
    "        return x, y\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "622df7cd-d2f1-40bd-b801-27df6c3b0cdf",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/semar/.local/lib/python3.11/site-packages/torchvision/transforms/v2/_deprecated.py:43: UserWarning: The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)])`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(\n",
    "    root='./CIFAR10', train=False, download=True, transform=transform)\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(\n",
    "    root='./CIFAR10', train=True, download=True)\n",
    "\n",
    "# split the train set into train/validation\n",
    "train_set_size = int(len(trainset) * 0.8)\n",
    "valid_set_size = len(trainset) - train_set_size\n",
    "\n",
    "seed = torch.Generator().manual_seed(42)\n",
    "trainset, validset = torch.utils.data.random_split(trainset, [train_set_size, valid_set_size], generator=seed)\n",
    "\n",
    "trainset = DatasetWrapper(trainset, transform)\n",
    "validset = DatasetWrapper(validset, transform)\n",
    "\n",
    "# Create train dataloader\n",
    "trainloader = torch.utils.data.DataLoader(\n",
    "    trainset, batch_size=512, shuffle=True, num_workers=24)\n",
    "\n",
    "# Create validation dataloader\n",
    "validloader = torch.utils.data.DataLoader(\n",
    "    validset, batch_size=512, shuffle=False, num_workers=24)\n",
    "\n",
    "# Create test dataloader\n",
    "testloader = torch.utils.data.DataLoader(\n",
    "    testset, batch_size=512, shuffle=False, num_workers=24)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat', 'deer',\n",
    "           'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0ef5c968-2309-4bcb-a680-9450536ef079",
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted horse - idx: 7\n"
     ]
    }
   ],
   "source": [
    "MODEL_WEIGHTS_PATH = './checkpoints/model_small_100epochs.pth'\n",
    "\n",
    "model = MyMobileNet(trainloader, num_classes=10, alpha=0.25, max_epochs=100)\n",
    "checkpoint = torch.load(MODEL_WEIGHTS_PATH)\n",
    "# model.load_state_dict(checkpoint['state_dict'])\n",
    "model.load_state_dict(checkpoint['net'])\n",
    "model.eval()\n",
    "\n",
    "image, label = validset[0]\n",
    "image = image.unsqueeze(0)\n",
    "logits = model(image)\n",
    "pred_idx = logits.argmax()\n",
    "\n",
    "print(f\"Predicted {classes[pred_idx]} - idx: {pred_idx}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "da689ff6-ad5a-421b-b388-bf0b26467475",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def Conv2DInt(nRows, nCols, nChannels, nFilters, kernelSize, strides, n, input, weights, bias):\n",
    "#     outRows = (nRows - kernelSize)//strides + 1\n",
    "#     outCols = (nCols - kernelSize)//strides + 1\n",
    "    \n",
    "#     out = [[[0 for _ in range(nFilters)] for _ in range((nCols - kernelSize)//strides + 1)] for _ in range((nRows - kernelSize)//strides + 1)]\n",
    "#     remainder = [[[None for _ in range(nFilters)] for _ in range((nCols - kernelSize)//strides + 1)] for _ in range((nRows - kernelSize)//strides + 1)]\n",
    "    \n",
    "#     for row in range(outRows):\n",
    "#         for col in range(outCols):\n",
    "#             for filter in range(nFilters):\n",
    "#                 for channel in range(nChannels):\n",
    "#                     for x in range(kernelSize):\n",
    "#                         for y in range(kernelSize):\n",
    "#                             out[row][col][filter] += int(input[row*strides+x][col*strides+y][channel]) * int(weights[x][y][channel][filter])\n",
    "#                 out[row][col][filter] += int(bias[filter])\n",
    "#                 remainder[row][col][filter] = int(out[row][col][filter] % n)\n",
    "#                 out[row][col][filter] = int(int(out[row][col][filter]) // n)\n",
    "                \n",
    "#     return out, remainder\n",
    "\n",
    "def Conv2DInt(nRows, nCols, nChannels, nFilters, kernelSize, strides, n, input, weights, bias):\n",
    "    p = CIRCOM_PRIME\n",
    "    Input = [[[str(int(input[i][j][k]) % p) for k in range(nChannels)] for j in range(nCols)] for i in range(nRows)]\n",
    "    Weights = [[[[str(int(weights[i][j][k][l]) % p) for l in range(nFilters)] for k in range(nChannels)] for j in range(kernelSize)] for i in range(kernelSize)]\n",
    "    Bias = [str(int(bias[i]) % p) for i in range(nFilters)]\n",
    "    out = [[[0 for _ in range(nFilters)] for _ in range((nCols - kernelSize)//strides + 1)] for _ in range((nRows - kernelSize)//strides + 1)]\n",
    "    remainder = [[[None for _ in range(nFilters)] for _ in range((nCols - kernelSize)//strides + 1)] for _ in range((nRows - kernelSize)//strides + 1)]\n",
    "    for i in range((nRows - kernelSize)//strides + 1):\n",
    "        for j in range((nCols - kernelSize)//strides + 1):\n",
    "            for m in range(nFilters):\n",
    "                for k in range(nChannels):\n",
    "                    for x in range(kernelSize):\n",
    "                        for y in range(kernelSize):\n",
    "                            out[i][j][m] += int(input[i*strides+x][j*strides+y][k]) * int(weights[x][y][k][m])\n",
    "                out[i][j][m] += int(bias[m])\n",
    "                remainder[i][j][m] = str(int(out[i][j][m]) % n)\n",
    "                out[i][j][m] = str(int(out[i][j][m]) // n % p)\n",
    "    return Input, Weights, Bias, out, remainder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fbe4fd19-94b7-4b11-9dac-e308c523b7a8",
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "something\n",
      "something\n",
      "something\n",
      "something\n",
      "TEST\n"
     ]
    }
   ],
   "source": [
    "weights = model.conv.weight.detach().numpy().transpose(2, 3, 1, 0)\n",
    "image, label = testset[0]\n",
    "expected = model.conv(image).detach().numpy()\n",
    "bias = torch.zeros(weights.shape[3]).numpy()\n",
    "\n",
    "# padded = pad(image, 1).transpose(1,2,0)\n",
    "padded = F.pad(image, (1,1,1,1), \"constant\", 0).numpy()\n",
    "padded = padded.transpose(1,2,0)\n",
    "\n",
    "quantized_image = (padded * 10**EXPONENT).round()\n",
    "quantized_weights = (weights * 10**EXPONENT).round() # .transpose(0, 3, 1, 0) # [nFilters, nChannels, H, W] -> \n",
    "\n",
    "Input, Weights, Bias, out, remainder = Conv2DInt(34, 34, 3, 8, 3, 1, 10**EXPONENT, quantized_image, quantized_weights, bias)\n",
    "# output, remainder = Conv2DInt(34, 34, 3, 8, 3, 1, 10**EXPONENT, quantized_image, quantized_weights, bias)\n",
    "# test_output = output / 10**(EXPONENT)\n",
    "test_output = [[[from_circom(int(out)) / 10**EXPONENT for out in asdf] for asdf in asdfasdf] for asdfasdf in out]\n",
    "\n",
    "expected = expected.transpose((1, 2, 0))\n",
    "\n",
    "assert(np.allclose(test_output, expected, atol=1e-2))\n",
    "    \n",
    "input_json_path = \"head_input1.json\"\n",
    "with open(input_json_path, \"w\") as input_file:\n",
    "    json.dump({\"in\": Input,\n",
    "               \"conv2d_weights\": Weights,\n",
    "               \"conv2d_bias\": Bias,\n",
    "               \"conv2d_out\": out,\n",
    "               \"conv2d_remainder\": remainder,\n",
    "               }, input_file)\n",
    "\n",
    "os.chdir(\"circuits\")\n",
    "!./head/head_cpp/head ../head_input1.json head.wtns\n",
    "!npx snarkjs groth16 prove head/circuit_final.zkey head.wtns proof.json public_test.json\n",
    "os.chdir(\"../\")\n",
    "\n",
    "print(\"TEST\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3119f5f2-f882-40f5-955e-278c9ec5dac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def batch_norm(X, gamma, beta, moving_mean, moving_var, eps, momentum):\n",
    "#     # Use is_grad_enabled to determine whether we are in training mode\n",
    "#     X_hat = (X - moving_mean) #/ torch.sqrt(moving_var + eps)\n",
    "#     Y = gamma * X_hat + beta  # Scale and shift\n",
    "#     return Y\n",
    "    \n",
    "# def batch_norm(X, gamma, beta, moving_mean, moving_var, eps, momentum):\n",
    "#     a = gamma/(var+eps)**.5\n",
    "#     b = beta-gamma*mean/(var+eps)**.5\n",
    "    \n",
    "#     X_hat = (X - moving_mean) / torch.sqrt(moving_var + eps)\n",
    "#     Y = gamma * X_hat + beta  # Scale and shift\n",
    "\n",
    "#     y = a * X + b\n",
    "#     assert(torch.allclose(y, Y, atol=1e-7))\n",
    "#     return y\n",
    "\n",
    "\n",
    "# def BatchNormalizationInt(nRows, nCols, nChannels, n, X_in, a_in, b_in):\n",
    "#     X = [[[str(X_in[i][j][k] % p) for k in range(nChannels)] for j in range(nCols)] for i in range(nRows)]\n",
    "#     A = [str(a_in[k] % p) for k in range(nChannels)]\n",
    "#     B = [str(b_in[k] % p) for k in range(nChannels)]\n",
    "#     out = [[[None for _ in range(nChannels)] for _ in range(nCols)] for _ in range(nRows)]\n",
    "#     remainder = [[[None for _ in range(nChannels)] for _ in range(nCols)] for _ in range(nRows)]\n",
    "#     for i in range(nRows):\n",
    "#         for j in range(nCols):\n",
    "#             for k in range(nChannels):\n",
    "#                 out[i][j][k] = (X_in[i][j][k]*a_in[k] + b_in[k])\n",
    "#                 remainder[i][j][k] = str(out[i][j][k] % n)\n",
    "#                 out[i][j][k] = str(out[i][j][k] // n % p)\n",
    "                \n",
    "#     return X, A, B, out, remainder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a62ca9e3-8272-4bbd-8293-fdfeb8db5562",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_norm(X, gamma, beta, moving_mean, moving_var, eps, momentum):\n",
    "    a = gamma/(var+eps)**.5\n",
    "    b = beta-gamma*mean/(var+eps)**.5\n",
    "    \n",
    "    X_hat = (X - moving_mean) / torch.sqrt(moving_var + eps)\n",
    "    Y = gamma * X_hat + beta  # Scale and shift\n",
    "\n",
    "    y = a * X + b\n",
    "    assert(torch.allclose(y, Y, atol=1e-7))\n",
    "    return y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "87d0b5bf-0f80-45ea-b6f8-c2b6dcd435f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gamma=Parameter containing:\n",
      "tensor([1.7132, 0.9582, 0.6173, 0.8834, 0.6231, 0.8451, 0.7998, 1.2492],\n",
      "       requires_grad=True)\n",
      "beta=Parameter containing:\n",
      "tensor([ 0.2958,  1.0282,  1.3571,  1.1435,  1.2801,  0.9873,  1.1311, -0.0990],\n",
      "       requires_grad=True)\n",
      "mean=tensor([ 0.2411,  0.3613,  0.0998, -0.1060,  0.1290, -0.0216, -0.2255,  0.0701])\n",
      "var=tensor([28.2820, 20.5319,  5.9350,  8.0902, 11.8494, 17.0723, 22.9259,  6.8973])\n",
      "eps=1e-05\n",
      "out.shape=torch.Size([1, 8, 32, 32])\n",
      "out.shape=torch.Size([1, 8, 32, 32])\n",
      "expected.shape=torch.Size([32, 32, 8])\n",
      "out.shape=torch.Size([32, 32, 8])\n",
      "actual.shape=torch.Size([32, 32, 8])\n"
     ]
    }
   ],
   "source": [
    "gamma = model.bn.weight\n",
    "beta = model.bn.bias\n",
    "mean = model.bn.running_mean\n",
    "var = model.bn.running_var\n",
    "eps = model.bn.eps\n",
    "\n",
    "print(f\"{gamma=}\")\n",
    "print(f\"{beta=}\")\n",
    "print(f\"{mean=}\")\n",
    "print(f\"{var=}\")\n",
    "print(f\"{eps=}\")\n",
    "\n",
    "image, label = testset[0]\n",
    "image = image.unsqueeze(0)\n",
    "out = model.conv(image)\n",
    "print(f\"{out.shape=}\")\n",
    "expected = model.bn(out)\n",
    "# out = out.squeeze()\n",
    "print(f\"{out.shape=}\")\n",
    "\n",
    "expected = torch.permute(expected.squeeze(), (1, 2, 0))\n",
    "\n",
    "print(f\"{expected.shape=}\")\n",
    "\n",
    "out = torch.permute(out.squeeze(), (1, 2, 0))\n",
    "print(f\"{out.shape=}\")\n",
    "\n",
    "# actual = batch_norm(torch.permute(out, (2, 3, 1, 0)), gamma, beta, mean, var, eps, 0)\n",
    "# actual = batch_norm(torch.permute(out, (1, 2, 0)), gamma, beta, mean, var, eps, 0)\n",
    "actual = batch_norm(out, gamma, beta, mean, var, eps, 0)\n",
    "\n",
    "print(f\"{actual.shape=}\")\n",
    "assert(torch.allclose(actual, expected, atol=1e-6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "247d5284-3c4f-4fd3-9781-43b47c592365",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def batch_normalization(nRows, nCols, nChannels, n, X_in, a_in, b_in):\n",
    "#     p = CIRCOM_PRIME\n",
    "#     # X = [[[str(int(X_in[i][j][k] % p) for k in range(nChannels)] for j in range(nCols)] for i in range(nRows)]\n",
    "#     # A = [str(a_in[k] % p) for k in range(nChannels)]\n",
    "#     # B = [str(b_in[k] % p) for k in range(nChannels)]\n",
    "#     X, A, B = 0,0,0\n",
    "#     out = [[[0 for _ in range(nChannels)] for _ in range(nCols)] for _ in range(nRows)]\n",
    "#     remainder = [[[0 for _ in range(nChannels)] for _ in range(nCols)] for _ in range(nRows)]\n",
    "#     for i in range(nRows):\n",
    "#         for j in range(nCols):\n",
    "#             for k in range(nChannels):\n",
    "#                 out[i][j][k] = float(X_in[i][j][k]*a_in[k] + b_in[k])\n",
    "#                 # remainder[i][j][k] = str(out[i][j][k] % n)\n",
    "#                 # out[i][j][k] = out[i][j][k] / n\n",
    "#     return X, A, B, out, remainder\n",
    "    \n",
    "def BatchNormalizationInt(nRows, nCols, nChannels, n, X_in, a_in, b_in):\n",
    "    p = CIRCOM_PRIME\n",
    "    # X = [[[str(int(X_in[i][j][k] % p) for k in range(nChannels)] for j in range(nCols)] for i in range(nRows)]\n",
    "    X = [[[str(int(X_in[i][j][k]) % p) for k in range(nChannels)] for j in range(nCols)] for i in range(nRows)]\n",
    "    A = [str(int(a_in[k]) % p) for k in range(nChannels)]\n",
    "    B = [str(int(b_in[k]) % p) for k in range(nChannels)]\n",
    "    # X, A, B = 0,0,0\n",
    "    out = [[[0 for _ in range(nChannels)] for _ in range(nCols)] for _ in range(nRows)]\n",
    "    remainder = [[[None for _ in range(nChannels)] for _ in range(nCols)] for _ in range(nRows)]\n",
    "    for i in range(nRows):\n",
    "        for j in range(nCols):\n",
    "            for k in range(nChannels):\n",
    "                out[i][j][k] = int(int(X_in[i][j][k]) * int(a_in[k]) + int(b_in[k]))\n",
    "                remainder[i][j][k] = str(int(out[i][j][k]) % n)\n",
    "                out[i][j][k] = int(out[i][j][k] // n)\n",
    "    return X, A, B, out, remainder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "56e21b0a-fbb2-46db-80c1-5d8f57518c95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gamma=Parameter containing:\n",
      "tensor([1.7132, 0.9582, 0.6173, 0.8834, 0.6231, 0.8451, 0.7998, 1.2492],\n",
      "       requires_grad=True)\n",
      "beta=Parameter containing:\n",
      "tensor([ 0.2958,  1.0282,  1.3571,  1.1435,  1.2801,  0.9873,  1.1311, -0.0990],\n",
      "       requires_grad=True)\n",
      "mean=tensor([ 0.2411,  0.3613,  0.0998, -0.1060,  0.1290, -0.0216, -0.2255,  0.0701])\n",
      "var=tensor([28.2820, 20.5319,  5.9350,  8.0902, 11.8494, 17.0723, 22.9259,  6.8973])\n",
      "eps=1e-05\n",
      "out.shape=torch.Size([1, 8, 32, 32])\n",
      "out.shape=torch.Size([1, 8, 32, 32])\n",
      "expected.shape=torch.Size([32, 32, 8])\n",
      "torch.Size([1, 8, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "gamma = model.bn.weight\n",
    "beta = model.bn.bias\n",
    "mean = model.bn.running_mean\n",
    "var = model.bn.running_var\n",
    "eps = model.bn.eps\n",
    "\n",
    "print(f\"{gamma=}\")\n",
    "print(f\"{beta=}\")\n",
    "print(f\"{mean=}\")\n",
    "print(f\"{var=}\")\n",
    "print(f\"{eps=}\")\n",
    "\n",
    "a = (gamma/(var+eps)**.5).detach()\n",
    "b = (beta-gamma*mean/(var+eps)**.5).detach().tolist()\n",
    "\n",
    "image, label = testset[0]\n",
    "image = image.unsqueeze(0)\n",
    "out = model.conv(image)\n",
    "print(f\"{out.shape=}\")\n",
    "expected = model.bn(out)\n",
    "# out = out.squeeze()\n",
    "print(f\"{out.shape=}\")\n",
    "\n",
    "expected = torch.permute(expected.squeeze(), (1, 2, 0))\n",
    "\n",
    "print(f\"{expected.shape=}\")\n",
    "\n",
    "quantized_in = torch.permute(out.squeeze(), (1, 2, 0)) * 10**EXPONENT\n",
    "quantized_a = (a * 10**EXPONENT).tolist()\n",
    "# quantized_b = (b * 10**(2*EXPONENT)).tolist()\n",
    "quantized_b = [bi * 10**(2*EXPONENT) for bi in b]\n",
    "print(out.shape)\n",
    "\n",
    "# actual = batch_norm(torch.permute(out, (1, 2, 0)), gamma, beta, mean, var, eps, 0)\n",
    "X, A, B, actual, remainder = BatchNormalizationInt(32, 32, 8, 10**EXPONENT, quantized_in, quantized_a, quantized_b)\n",
    "test_output = [[[from_circom(int(out)) / 10**EXPONENT for out in asdf] for asdf in asdfasdf] for asdfasdf in actual]\n",
    "# print(f\"{actual.shape=}\")\n",
    "# print(f\"{np.array(actual).shape=}\")\n",
    "assert(torch.allclose(torch.Tensor(test_output), expected, atol=1e-6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "5b04c78d-dcb3-47d2-b0a1-a3fcda1dcadf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;22m[INFO]  \u001b[39;1msnarkJS\u001b[0m: OK!\n",
      "TEST\n"
     ]
    }
   ],
   "source": [
    "X, A, B, actual, remainder = BatchNormalizationInt(32, 32, 8, 10**EXPONENT, quantized_in, quantized_a, quantized_b)\n",
    "test_output = [[[from_circom(int(out)) / 10**EXPONENT for out in asdf] for asdf in asdfasdf] for asdfasdf in actual]\n",
    "\n",
    "assert(torch.allclose(torch.Tensor(test_output), expected, atol=1e-6))\n",
    "\n",
    "input_json_path = \"test_input.json\"\n",
    "with open(input_json_path, \"w\") as input_file:\n",
    "    json.dump({\"in\": X,\n",
    "               \"a\": A,\n",
    "               \"b\": B,\n",
    "               \"out\": actual,\n",
    "               \"remainder\": remainder,\n",
    "               }, input_file)\n",
    "\n",
    "os.chdir(\"circuits\")\n",
    "!./test/test_cpp/test ../test_input.json test.wtns\n",
    "!npx snarkjs groth16 prove test/circuit_final.zkey test.wtns proof.json public_test.json\n",
    "!npx snarkjs groth16 verify test/verification_key.json public_test.json proof.json\n",
    "os.chdir(\"../\")\n",
    "\n",
    "# print(f\"{np.array(actual).shape=}\")\n",
    "print(f\"TEST\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7be953d5-a998-4b97-90e4-94f6825f371a",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Can't call numpy() on Tensor that requires grad. Use tensor.detach().numpy() instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mactual\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m=}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/_tensor.py:1030\u001b[0m, in \u001b[0;36mTensor.__array__\u001b[0;34m(self, dtype)\u001b[0m\n\u001b[1;32m   1028\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(Tensor\u001b[38;5;241m.\u001b[39m__array__, (\u001b[38;5;28mself\u001b[39m,), \u001b[38;5;28mself\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[1;32m   1029\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1030\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1031\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1032\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39mastype(dtype, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Can't call numpy() on Tensor that requires grad. Use tensor.detach().numpy() instead."
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b2247356-737d-4de6-b101-1b1366304024",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "actual = batch_norm(torch.permute(out.squeeze(), (1, 2, 0)), gamma, beta, mean, var, eps, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c3d2fb5e-e3a3-4f64-99a3-a489d4dfa93b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 32, 8]), torch.Size([32, 32, 8]))"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actual.shape, expected.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9f73442b-3a36-43ad-970f-8cac39ec6fce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.allclose(expected, actual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b17640f3-e45c-4292-95ab-a36f9eacb66c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
