{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6e30ed7-19bd-495b-9c3b-37b7753f3934",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Manual mobilenet implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69987bda-3e35-44f6-9117-a90513a9513b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from models.mobilenet import ZkMobileNet\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pytorch_lightning as pl\n",
    "import torchvision\n",
    "import torchvision.transforms.v2 as transforms\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1a79f141-4c15-4283-9a09-4825a99028f9",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Circom defines the range of positives are [0, p/2] and the range of negatives are [(p/2)+1, (p-1)].\n",
    "# CIRCOM_PRIME = 21888242871839275222246405745257275088548364400416034343698204186575808495617\n",
    "p = CIRCOM_PRIME = 28948022309329048855892746252171976963363056481941647379679742748393362948097\n",
    "MAX_POSITIVE = CIRCOM_PRIME // 2\n",
    "MAX_NEGATIVE = MAX_POSITIVE + 1 # The most positive number\n",
    "CIRCOM_NEGATIVE_1 = CIRCOM_PRIME - 1\n",
    "EXPONENT = 15\n",
    "\n",
    "def from_circom(x):\n",
    "    if type(x) != int:\n",
    "        x = int(x)\n",
    "    if x > MAX_POSITIVE: \n",
    "        return x - CIRCOM_PRIME\n",
    "    return x\n",
    "    \n",
    "def to_circom(x):\n",
    "    return x % CIRCOM_PRIME\n",
    "    \n",
    "def to_circom_input(array: np.array): \n",
    "    if type(array) != np.array:\n",
    "        array = np.array(array)\n",
    "    int_array = array.round().astype(int)\n",
    "    int_array = to_circom(int_array)\n",
    "    return int_array.astype(str).tolist()\n",
    "\n",
    "\n",
    "def DepthwiseConv(nRows, nCols, nChannels, nFilters, kernelSize, strides, n, input, weights, bias):\n",
    "    assert(nFilters % nChannels == 0)\n",
    "    outRows = (nRows - kernelSize)//strides + 1\n",
    "    outCols = (nCols - kernelSize)//strides + 1\n",
    "    \n",
    "    # out = np.zeros((outRows, outCols, nFilters))\n",
    "    out = [[[0 for _ in range(nFilters)] for _ in range(outCols)] for _ in range(outRows)]\n",
    "    remainder = [[[0 for _ in range(nFilters)] for _ in range(outCols)] for _ in range(outRows)]\n",
    "    # remainder = np.zeros((outRows, outCols, nFilters))\n",
    "    \n",
    "    for row in range(outRows):\n",
    "        for col in range(outCols):\n",
    "            for channel in range(nChannels):\n",
    "                for x in range(kernelSize):\n",
    "                    for y in range(kernelSize):\n",
    "                        out[row][col][channel] += int(input[row*strides+x, col*strides+y, channel]) * int(weights[x, y, channel])\n",
    "                \n",
    "                out[row][col][channel] += int(bias[channel])\n",
    "                remainder[row][col][channel] = str(int(out[row][col][channel] % n))\n",
    "                out[row][col][channel] = int(out[row][col][channel] // n)\n",
    "                            \n",
    "    return out, remainder\n",
    "\n",
    "def PointwiseConv2d(nRows, nCols, nChannels, nFilters, strides, n, input, weights, bias):\n",
    "    kernelSize = 1\n",
    "    outRows = (nRows - kernelSize)//strides + 1\n",
    "    outCols = (nCols - kernelSize)//strides + 1\n",
    "    out = [[[0 for _ in range(nFilters)] for _ in range(outCols)] for _ in range(outRows)]\n",
    "    str_out = [[[0 for _ in range(nFilters)] for _ in range(outCols)] for _ in range(outRows)]\n",
    "    remainder = [[[None for _ in range(nFilters)] for _ in range(outCols)] for _ in range(outRows)]\n",
    "    for row in range(outRows):\n",
    "        for col in range(outCols):\n",
    "            for filter in range(nFilters):\n",
    "                for k in range(nChannels):\n",
    "                    out[row][col][filter] += int(input[row*strides, col*strides, k]) * int(weights[k, filter])\n",
    "                    \n",
    "                out[row][col][filter] += int(bias[filter])\n",
    "                remainder[row][col][filter] = str(int(out[row][col][filter] % n))\n",
    "                out[row][col][filter] = int(out[row][col][filter] // n)\n",
    "                str_out[row][col][filter] = str(out[row][col][filter] % p)\n",
    "                            \n",
    "    return out, str_out, remainder\n",
    "\n",
    "def SeparableConvImpl(nRows, nCols, nChannels, nDepthFilters, nPointFilters, kernelSize, strides, n, input, depthWeights, pointWeights, depthBias, pointBias):\n",
    "    outRows = (nRows - kernelSize)//strides + 1\n",
    "    outCols = (nCols - kernelSize)//strides + 1\n",
    "\n",
    "    depth_out, depth_remainder = DepthwiseConv(nRows, nCols, nChannels, nDepthFilters, kernelSize, strides, n, input, depthWeights, depthBias)\n",
    "    point_out, point_str_out, point_remainder = PointwiseConv2d(outRows, outCols, nChannels, nPointFilters, strides, n, depth_out, pointWeights, pointBias)\n",
    "    return depth_out, depth_remainder, point_out, point_str_out, point_remainder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "faaa5941-253c-4676-94bd-c4dfbb1e524f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetWrapper(torch.utils.data.Dataset):\n",
    "    def __init__(self, subset, transform=None):\n",
    "        self.subset = subset\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        x, y = self.subset[index]\n",
    "        \n",
    "        if self.transform:\n",
    "            x = self.transform(x)\n",
    "        return x, y\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "622df7cd-d2f1-40bd-b801-27df6c3b0cdf",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/semar/.local/lib/python3.11/site-packages/torchvision/transforms/v2/_deprecated.py:43: UserWarning: The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)])`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(\n",
    "    root='./CIFAR10', train=False, download=True, transform=transform)\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(\n",
    "    root='./CIFAR10', train=True, download=True)\n",
    "\n",
    "# split the train set into train/validation\n",
    "train_set_size = int(len(trainset) * 0.8)\n",
    "valid_set_size = len(trainset) - train_set_size\n",
    "\n",
    "seed = torch.Generator().manual_seed(42)\n",
    "trainset, validset = torch.utils.data.random_split(trainset, [train_set_size, valid_set_size], generator=seed)\n",
    "\n",
    "trainset = DatasetWrapper(trainset, transform)\n",
    "validset = DatasetWrapper(validset, transform)\n",
    "\n",
    "# Create train dataloader\n",
    "trainloader = torch.utils.data.DataLoader(\n",
    "    trainset, batch_size=512, shuffle=True, num_workers=24)\n",
    "\n",
    "# Create validation dataloader\n",
    "validloader = torch.utils.data.DataLoader(\n",
    "    validset, batch_size=512, shuffle=False, num_workers=24)\n",
    "\n",
    "# Create test dataloader\n",
    "testloader = torch.utils.data.DataLoader(\n",
    "    testset, batch_size=512, shuffle=False, num_workers=24)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat', 'deer',\n",
    "           'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0ef5c968-2309-4bcb-a680-9450536ef079",
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted frog - idx: 6\n"
     ]
    }
   ],
   "source": [
    "MODEL_WEIGHTS_PATH = \"./checkpoints/no_padding_100epochs.pth\", \n",
    "\n",
    "model = ZkMobileNet(trainloader, num_classes=10, alpha=0.25, max_epochs=100)\n",
    "\n",
    "saved = torch.load(\"./checkpoints/no_padding_100epochs.pth\")\n",
    "# model.load_state_dict(saved['state_dict'])\n",
    "model.load_state_dict(saved['net'])\n",
    "model.eval()\n",
    "\n",
    "image, label = validset[0]\n",
    "image = image.unsqueeze(0)\n",
    "logits = model(image)\n",
    "pred_idx = logits.argmax()\n",
    "\n",
    "print(f\"Predicted {classes[pred_idx]} - idx: {pred_idx}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "da689ff6-ad5a-421b-b388-bf0b26467475",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Conv2DInt(nRows, nCols, nChannels, nFilters, kernelSize, strides, n, input, weights, bias):\n",
    "    p = CIRCOM_PRIME\n",
    "    Input = [[[str(int(input[i][j][k]) % p) for k in range(nChannels)] for j in range(nCols)] for i in range(nRows)]\n",
    "    Weights = [[[[str(int(weights[i][j][k][l]) % p) for l in range(nFilters)] for k in range(nChannels)] for j in range(kernelSize)] for i in range(kernelSize)]\n",
    "    Bias = [str(int(bias[i]) % p) for i in range(nFilters)]\n",
    "    out = [[[0 for _ in range(nFilters)] for _ in range((nCols - kernelSize)//strides + 1)] for _ in range((nRows - kernelSize)//strides + 1)]\n",
    "    remainder = [[[None for _ in range(nFilters)] for _ in range((nCols - kernelSize)//strides + 1)] for _ in range((nRows - kernelSize)//strides + 1)]\n",
    "    for i in range((nRows - kernelSize)//strides + 1):\n",
    "        for j in range((nCols - kernelSize)//strides + 1):\n",
    "            for m in range(nFilters):\n",
    "                for k in range(nChannels):\n",
    "                    for x in range(kernelSize):\n",
    "                        for y in range(kernelSize):\n",
    "                            out[i][j][m] += int(input[i*strides+x][j*strides+y][k]) * int(weights[x][y][k][m])\n",
    "                out[i][j][m] += int(bias[m])\n",
    "                remainder[i][j][m] = str(int(out[i][j][m]) % n)\n",
    "                out[i][j][m] = str(int(out[i][j][m]) // n % p)\n",
    "    return Input, Weights, Bias, out, remainder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "247d5284-3c4f-4fd3-9781-43b47c592365",
   "metadata": {},
   "outputs": [],
   "source": [
    "def BatchNormalizationInt(nRows, nCols, nChannels, n, X_in, a_in, b_in):\n",
    "    p = CIRCOM_PRIME\n",
    "    X = [[[str(int(X_in[i][j][k]) % p) for k in range(nChannels)] for j in range(nCols)] for i in range(nRows)]\n",
    "    A = [str(int(a_in[k]) % p) for k in range(nChannels)]\n",
    "    B = [str(int(b_in[k]) % p) for k in range(nChannels)]\n",
    "    out = [[[0 for _ in range(nChannels)] for _ in range(nCols)] for _ in range(nRows)]\n",
    "    remainder = [[[None for _ in range(nChannels)] for _ in range(nCols)] for _ in range(nRows)]\n",
    "    for i in range(nRows):\n",
    "        for j in range(nCols):\n",
    "            for k in range(nChannels):\n",
    "                out[i][j][k] = int(int(X_in[i][j][k]) * int(a_in[k]) + int(b_in[k]))\n",
    "                remainder[i][j][k] = str(int(out[i][j][k]) % n)\n",
    "                out[i][j][k] = int(out[i][j][k] // n)\n",
    "    return X, A, B, out, remainder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "56e21b0a-fbb2-46db-80c1-5d8f57518c95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out.shape=torch.Size([1, 8, 32, 32])\n",
      "expected.shape=torch.Size([32, 32, 8])\n",
      "torch.Size([1, 8, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "gamma = model.bn.weight\n",
    "beta = model.bn.bias\n",
    "mean = model.bn.running_mean\n",
    "var = model.bn.running_var\n",
    "eps = model.bn.eps\n",
    "\n",
    "a = (gamma/(var+eps)**.5).detach()\n",
    "b = (beta-gamma*mean/(var+eps)**.5).detach().tolist()\n",
    "\n",
    "image, label = testset[0]\n",
    "image = image.unsqueeze(0)\n",
    "out = model.conv(image)\n",
    "expected = model.bn(out)\n",
    "# out = out.squeeze()\n",
    "print(f\"{out.shape=}\")\n",
    "\n",
    "expected = torch.permute(expected.squeeze(), (1, 2, 0))\n",
    "\n",
    "print(f\"{expected.shape=}\")\n",
    "\n",
    "quantized_in = torch.permute(out.squeeze(), (1, 2, 0)) * 10**EXPONENT\n",
    "quantized_a = (a * 10**EXPONENT).tolist()\n",
    "quantized_b = [bi * 10**(2*EXPONENT) for bi in b]\n",
    "print(out.shape)\n",
    "\n",
    "X, A, B, actual, remainder = BatchNormalizationInt(32, 32, 8, 10**EXPONENT, quantized_in, quantized_a, quantized_b)\n",
    "test_output = [[[from_circom(int(out)) / 10**EXPONENT for out in asdf] for asdf in asdfasdf] for asdfasdf in actual]\n",
    "\n",
    "assert(torch.allclose(torch.Tensor(test_output), expected, atol=1e-6))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea13b7cc-41ef-4127-b1fd-99ffcc4723b8",
   "metadata": {},
   "source": [
    "# Testing head layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fbe4fd19-94b7-4b11-9dac-e308c523b7a8",
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "something\n",
      "before conv2d\n",
      "after conv2d\n",
      "after bn\n",
      "at:  0 1 6\n",
      "in:  1745789835369983\n",
      "after relu\n",
      "end\n",
      "ok\n"
     ]
    }
   ],
   "source": [
    "# CONVOLUTION LAYER\n",
    "weights = model.conv.weight.detach().numpy().transpose(2, 3, 1, 0)\n",
    "image, label = testset[0]\n",
    "\n",
    "expected = model.conv(image).detach().numpy()\n",
    "bias = torch.zeros(weights.shape[3]).numpy()\n",
    "\n",
    "# padded = pad(image, 1).transpose(1,2,0)\n",
    "padded = F.pad(image, (1,1,1,1), \"constant\", 0).numpy()\n",
    "padded = padded.transpose(1,2,0)\n",
    "\n",
    "quantized_image = (padded * 10**EXPONENT).round()\n",
    "quantized_weights = (weights * 10**EXPONENT).round() # .transpose(0, 3, 1, 0) # [nFilters, nChannels, H, W] -> \n",
    "\n",
    "circuit_in, circuit_conv_weights, circuit_conv_bias, circuit_conv_out, circuit_conv_remainder = Conv2DInt(34, 34, 3, 8, 3, 1, 10**EXPONENT, quantized_image, quantized_weights, bias)\n",
    "# output, remainder = Conv2DInt(34, 34, 3, 8, 3, 1, 10**EXPONENT, quantized_image, quantized_weights, bias)\n",
    "# test_output = output / 10**(EXPONENT)\n",
    "test_output = [[[from_circom(int(out)) / 10**EXPONENT for out in asdf] for asdf in asdfasdf] for asdfasdf in circuit_conv_out]\n",
    "\n",
    "expected = expected.transpose((1, 2, 0))\n",
    "\n",
    "assert(np.allclose(test_output, expected, atol=1e-6))\n",
    "\n",
    "# BATCH NORM CONSTANTS\n",
    "gamma = model.bn.weight\n",
    "beta = model.bn.bias\n",
    "mean = model.bn.running_mean\n",
    "var = model.bn.running_var\n",
    "eps = model.bn.eps\n",
    "\n",
    "a = (gamma/(var+eps)**.5).detach()\n",
    "b = (beta-gamma*mean/(var+eps)**.5).detach().tolist()\n",
    "\n",
    "quantized_a = (a * 10**EXPONENT).tolist()\n",
    "quantized_b = [bi * 10**(2*EXPONENT) for bi in b]\n",
    "\n",
    "# BATCH NORM USING PYTORCH OUTPUT\n",
    "image, label = testset[0]\n",
    "image = image.unsqueeze(0)\n",
    "out = model.conv(image)\n",
    "expected = model.bn(out)\n",
    "\n",
    "expected = torch.permute(expected.squeeze(), (1, 2, 0))\n",
    "\n",
    "quantized_in = torch.permute(out.squeeze(), (1, 2, 0)) * 10**EXPONENT\n",
    "\n",
    "X, A, B, actual, remainder = BatchNormalizationInt(32, 32, 8, 10**EXPONENT, quantized_in, quantized_a, quantized_b)\n",
    "test_output = [[[from_circom(int(out)) / 10**EXPONENT for out in vec] for vec in matrix] for matrix in actual]\n",
    "assert(torch.allclose(torch.Tensor(test_output), expected, atol=1e-6))\n",
    "\n",
    "# BATCH NORM USING CIRCUIT CONV OUTPUT\n",
    "quantized_in = [[[from_circom(int(out)) for out in vec] for vec in matrix] for matrix in circuit_conv_out]\n",
    "\n",
    "_, circuit_bn_a, circuit_bn_b, circuit_bn_out, circuit_bn_remainder = BatchNormalizationInt(32, 32, 8, 10**EXPONENT, quantized_in, quantized_a, quantized_b)\n",
    "\n",
    "test_output = [[[int(from_circom(int(out))) / 10**EXPONENT for out in vec] for vec in matrix] for matrix in actual]\n",
    "\n",
    "assert(torch.allclose(torch.Tensor(test_output), expected, atol=1e-6))\n",
    "\n",
    "# RELU USING CIRCUIT OUTPUT\n",
    "relu_in = [[[to_circom(bn_out) if bn_out < MAX_POSITIVE else 0 for bn_out in vec] for vec in matrix] for matrix in circuit_bn_out]\n",
    "relu_out = [[[str(bn_out) if bn_out < MAX_POSITIVE else 0 for bn_out in vec] for vec in matrix] for matrix in relu_in]\n",
    "\n",
    "input_json_path = \"head_input1.json\"\n",
    "with open(input_json_path, \"w\") as input_file:\n",
    "    json.dump({\"in\": circuit_in,\n",
    "               \"conv2d_weights\": circuit_conv_weights,\n",
    "               \"conv2d_bias\": circuit_conv_bias,\n",
    "               \"conv2d_out\": circuit_conv_out,\n",
    "               \"conv2d_remainder\": circuit_conv_remainder,\n",
    "               \n",
    "               \"bn_a\": circuit_bn_a,\n",
    "               \"bn_b\": circuit_bn_b,\n",
    "               \"bn_out\": circuit_bn_out,\n",
    "               \"bn_remainder\": circuit_bn_remainder,\n",
    "               \n",
    "               \"relu_out\": relu_out,\n",
    "               }, input_file)\n",
    "\n",
    "os.chdir(\"circuits\")\n",
    "!./head/head_cpp/head ../head_input1.json head.wtns\n",
    "# !npx snarkjs groth16 prove head/circuit_final.zkey head.wtns proof.json public_test.json\n",
    "os.chdir(\"../\")\n",
    "\n",
    "print(\"ok\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f410f561-1bc2-4fc4-94cb-cf4c1d4ed258",
   "metadata": {},
   "source": [
    "# Testing padding over highly padded input (to try and fold the circuit using nova)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ecdd2b74-e553-4f68-b755-7f50d91a58a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SeparableConv2d(nn.Module):\n",
    "    '''Separable convolution'''\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super(SeparableConv2d, self).__init__()\n",
    "        self.dw_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, in_channels, kernel_size=3, stride=stride, padding=0, groups=in_channels, bias=False),\n",
    "            nn.BatchNorm2d(in_channels),\n",
    "            # nn.ReLU(inplace=False),\n",
    "        )\n",
    "        self.pw_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            # nn.ReLU(inplace=False),\n",
    "        )\n",
    "        \n",
    "input = torch.randn((1, 3, 5, 5))\n",
    "test_model = SeparableConv2d(3, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f933a59-1605-4a64-ac94-f4e7107a79e7",
   "metadata": {},
   "source": [
    "# Padded Convolution test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c6fd691-e729-404e-a858-2d3e45e26846",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b8a3222d-4eee-4060-b336-07da6efa1ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "input = torch.randn((1, 3, 7, 7))\n",
    "# model = SeparableConv2d(3, 6)\n",
    "# model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "48256a72-45ff-42bd-a29a-0aec9d0faa5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def PaddedDepthwiseConv(nRows, nCols, nChannels, nFilters, kernelSize, strides, n, input, weights, bias):\n",
    "    assert(nFilters % nChannels == 0)\n",
    "    outRows = (nRows - kernelSize)//strides + 1\n",
    "    outCols = (nCols - kernelSize)//strides + 1\n",
    "    \n",
    "    # out = np.zeros((outRows, outCols, nFilters))\n",
    "    Input = [[[str(int(input[i][j][k]) % p) for k in range(nChannels)] for j in range(nCols)] for i in range(nRows)]\n",
    "    Weights = [[[str(int(weights[i][j][k].round()) % p) for k in range(weights.shape[2])] for j in range(weights.shape[1])] for i in range(weights.shape[0])]\n",
    "    out = [[[0 for _ in range(nFilters)] for _ in range(nCols)] for _ in range(nRows)]\n",
    "    Bias = bias.round().astype(int).astype(str).tolist()\n",
    "    out_str = [[[str(0) for _ in range(nFilters)] for _ in range(nCols)] for _ in range(nRows)]\n",
    "    remainder = [[[str(0) for _ in range(nFilters)] for _ in range(nCols)] for _ in range(nRows)]\n",
    "    \n",
    "    for row in range(outRows):\n",
    "        for col in range(outCols):\n",
    "            for channel in range(nChannels):\n",
    "                for x in range(kernelSize):\n",
    "                    for y in range(kernelSize):\n",
    "                        out[row+1][col+1][channel] += int(input[row*strides+x][col*strides+y][channel]) * int(weights[x, y, channel])\n",
    "                \n",
    "                out[row+1][col+1][channel] += int(bias[channel])\n",
    "                remainder[row+1][col+1][channel] = str(int(out[row+1][col+1][channel] % n))\n",
    "                out[row+1][col+1][channel] = int(out[row+1][col+1][channel] // n)\n",
    "                out_str[row+1][col+1][channel] = str(out[row+1][col+1][channel] % p)\n",
    "                            \n",
    "    return Input, Weights, Bias, out_str, out, remainder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bcc591b9-c270-4a6d-a1bd-ac926f5b8641",
   "metadata": {},
   "outputs": [],
   "source": [
    "def PointwiseConv2d(nRows, nCols, nChannels, nFilters, strides, n, input, weights, bias):\n",
    "    kernelSize = 1\n",
    "    outRows = (nRows - kernelSize)//strides + 1\n",
    "    outCols = (nCols - kernelSize)//strides + 1\n",
    "    # out = [[[0 for _ in range(nFilters)] for _ in range(nCols)] for _ in range(nRows)]\n",
    "    # remainder = [[[None for _ in range(nFilters)] for _ in range(nCols)] for _ in range(nRows)]\n",
    "    \n",
    "    Input = [[[str(int(input[i][j][k]) % p) for k in range(nChannels)] for j in range(nCols)] for i in range(nRows)]\n",
    "    # Weights = [[[str(int(weights[i][j][k].round()) % p) for k in range(weights.shape[2])] for j in range(weights.shape[1])] for i in range(weights.shape[0])]\n",
    "    Weights = [[str(int(weights[i][j].round()) % p)for j in range(weights.shape[1])] for i in range(weights.shape[0])]\n",
    "    out = [[[0 for _ in range(nFilters)] for _ in range(nCols)] for _ in range(nRows)]\n",
    "    Bias = bias.round().astype(int).astype(str).tolist()\n",
    "    out_str = [[[str(0) for _ in range(nFilters)] for _ in range(nCols)] for _ in range(nRows)]\n",
    "    remainder = [[[str(0) for _ in range(nFilters)] for _ in range(nCols)] for _ in range(nRows)]\n",
    "    \n",
    "    for row in range(outRows):\n",
    "        for col in range(outCols):\n",
    "            for filter in range(nFilters):\n",
    "                for k in range(nChannels):\n",
    "                    out[row][col][filter] += int(input[row*strides][col*strides][k]) * int(weights[k, filter])\n",
    "                    \n",
    "                out[row][col][filter] += int(bias[filter])\n",
    "                remainder[row][col][filter] = str(int(out[row][col][filter] % n))\n",
    "                out[row][col][filter] = int(out[row][col][filter] // n)\n",
    "                out_str[row][col][filter] = str(out[row][col][filter] % p)\n",
    "                            \n",
    "    # return out, remainder\n",
    "    return Input, Weights, Bias, out_str, out, remainder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ad9186ba-a226-4c99-b60c-929a6b4b2f05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape:  torch.Size([1, 3, 7, 7])\n",
      "Expected shape:  torch.Size([1, 3, 5, 5])\n",
      "a shape:  torch.Size([3])\n",
      "b shape:  torch.Size([3])\n",
      "point weights shape:  (6, 3, 1, 1)\n",
      "Input shape:  torch.Size([1, 3, 7, 7])\n",
      "Depth Expected shape:  torch.Size([1, 3, 5, 5])\n",
      "Point Expected shape:  (5, 5, 6)\n",
      "point weights shape:  (3, 6)\n",
      "a shape:  torch.Size([6])\n",
      "b shape:  torch.Size([6])\n",
      "Input shape:  torch.Size([1, 3, 7, 7])\n",
      "Depth Expected shape:  torch.Size([1, 6, 5, 5])\n",
      "Expected shape:  torch.Size([5, 5, 6])\n",
      "test shape:  (5, 5, 6)\n",
      "terminate called after throwing an instance of 'std::runtime_error'\n",
      "  what():  Error loading signal dw_bn_a: Not enough values\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "# Depthwise convolution\n",
    "input = torch.randn((1, 3, 7, 7))\n",
    "\n",
    "depth_weights = test_model.dw_conv[0].weight.squeeze().detach().numpy()\n",
    "depth_bias = torch.zeros(depth_weights.shape[0]).numpy()\n",
    "\n",
    "print(\"Input shape: \", input.shape)\n",
    "expected = test_model.dw_conv[0](input).detach()\n",
    "print(\"Expected shape: \", expected.shape)\n",
    "\n",
    "depth_weights = depth_weights.transpose((1, 2, 0))\n",
    "\n",
    "quantized_image = input.squeeze().numpy().transpose((1,2,0)) * 10**EXPONENT\n",
    "# quantized_image = padded * 10**EXPONENT\n",
    "quantized_depth_weights = depth_weights * 10**EXPONENT\n",
    "\n",
    "circuit_in, circuit_depth_weights, circuit_depth_bias, circuit_depth_out, depth_out, circuit_depth_remainder = PaddedDepthwiseConv(7, 7, 3, 3, 3, 1, 10**EXPONENT, quantized_image.round(), quantized_depth_weights.round(), depth_bias)\n",
    "\n",
    "test_output = np.array([[[int(out) / 10**EXPONENT for out in asdf] for asdf in asdfasdf] for asdfasdf in depth_out])\n",
    "test_output = test_output[1:-1, 1:-1, :]\n",
    "\n",
    "expected = expected.squeeze().numpy().transpose((1,2,0))\n",
    "\n",
    "assert(np.allclose(expected, test_output, atol=0.00001))\n",
    "\n",
    "# Batch normalization step\n",
    "# BATCH NORM CONSTANTS\n",
    "test_model.eval()\n",
    "gamma = test_model.dw_conv[1].weight\n",
    "beta = test_model.dw_conv[1].bias\n",
    "mean = test_model.dw_conv[1].running_mean\n",
    "var = test_model.dw_conv[1].running_var\n",
    "eps = test_model.dw_conv[1].eps\n",
    "\n",
    "a = (gamma/(var+eps)**.5).detach()\n",
    "print('a shape: ', a.shape)\n",
    "b = (beta-gamma*mean/(var+eps)**.5).detach()\n",
    "print('b shape: ', b.shape)\n",
    "\n",
    "b = b.tolist()\n",
    "\n",
    "quantized_a = (a * 10**EXPONENT).tolist()\n",
    "quantized_b = [bi * 10**(2*EXPONENT) for bi in b]\n",
    "\n",
    "out = test_model.dw_conv[0](input)\n",
    "expected = test_model.dw_conv[1](out)\n",
    "\n",
    "expected = torch.permute(expected.squeeze(), (1, 2, 0))\n",
    "\n",
    "quantized_in = torch.permute(out.squeeze(), (1, 2, 0)) * 10**EXPONENT\n",
    "\n",
    "X, A, B, actual, remainder = BatchNormalizationInt(5, 5, 3, 10**EXPONENT, quantized_in, quantized_a, quantized_b)\n",
    "test_output = [[[from_circom(int(out)) / 10**EXPONENT for out in vec] for vec in matrix] for matrix in actual]\n",
    "\n",
    "assert(torch.allclose(torch.Tensor(test_output), expected, atol=1e-6))\n",
    "\n",
    "# quantized_in = [[[from_circom(int(out)) for out in vec] for vec in matrix] for matrix in circuit_conv_out]\n",
    "\n",
    "_, circuit_depth_bn_a, circuit_depth_bn_b, circuit_depth_bn_out, circuit_depth_bn_remainder = BatchNormalizationInt(7, 7, 3, 10**EXPONENT, depth_out, quantized_a, quantized_b)\n",
    "\n",
    "test_output = np.array([[[int(out) / 10**EXPONENT for out in asdf] for asdf in asdfasdf] for asdfasdf in circuit_depth_bn_out])\n",
    "test_output = test_output[1:-1, 1:-1, :]\n",
    "\n",
    "assert(torch.allclose(torch.Tensor(test_output), expected, atol=1e-6))\n",
    "\n",
    "# Pointwise convolution\n",
    "# point_weights = test_model.pw_conv[0].weight.squeeze().detach().numpy()\n",
    "point_weights = test_model.pw_conv[0].weight.detach().numpy()\n",
    "print(\"point weights shape: \", point_weights.shape)\n",
    "point_bias = torch.zeros(point_weights.shape[0]).numpy()\n",
    "\n",
    "print(\"Input shape: \", input.shape)\n",
    "depth_expected = test_model.dw_conv[0](input)\n",
    "bn_expected = test_model.dw_conv[1](depth_expected)\n",
    "point_expected = test_model.pw_conv[0](bn_expected)\n",
    "print(\"Depth Expected shape: \", depth_expected.shape)\n",
    "# print(\"Point Expected shape: \", point_expected.shape)\n",
    "\n",
    "point_expected = point_expected.squeeze().detach().numpy().transpose((1,2,0))\n",
    "print(\"Point Expected shape: \", point_expected.shape)\n",
    "\n",
    "point_weights = point_weights.transpose((2, 3, 1, 0)).squeeze()\n",
    "quantized_point_weights = point_weights * 10**EXPONENT\n",
    "print(\"point weights shape: \", quantized_point_weights.shape)\n",
    "\n",
    "# circuit_in, circuit_depth_weights, circuit_depth_bias, circuit_depth_out, depth_out, circuit_depth_remainder = PaddedDepthwiseConv(7, 7, 3, 3, 3, 1, 10**EXPONENT, quantized_image.round(), quantized_depth_weights.round(), depth_bias)\n",
    "point_input, circuit_point_weights, circuit_point_bias, circuit_point_out, point_out, circuit_point_remainder = PointwiseConv2d(7, 7, 3, 6, 1, 10**EXPONENT, circuit_depth_bn_out, quantized_point_weights.round(), point_bias)\n",
    "\n",
    "test_output = np.array([[[int(out) / 10**EXPONENT for out in asdf] for asdf in asdfasdf] for asdfasdf in point_out])\n",
    "test_output = test_output[1:-1, 1:-1, :]\n",
    "\n",
    "assert(np.allclose(point_expected, test_output, atol=1e-6))\n",
    "\n",
    "                   \n",
    "# Batch normalization step\n",
    "# BATCH NORM CONSTANTS\n",
    "test_model.eval()\n",
    "gamma = test_model.pw_conv[1].weight\n",
    "beta = test_model.pw_conv[1].bias\n",
    "mean = test_model.pw_conv[1].running_mean\n",
    "var = test_model.pw_conv[1].running_var\n",
    "eps = test_model.pw_conv[1].eps\n",
    "\n",
    "a = (gamma/(var+eps)**.5).detach()\n",
    "print('a shape: ', a.shape)\n",
    "b = (beta-gamma*mean/(var+eps)**.5).detach()\n",
    "print('b shape: ', b.shape)\n",
    "\n",
    "b = b.tolist()\n",
    "\n",
    "quantized_a = (a * 10**EXPONENT).tolist()\n",
    "quantized_b = [bi * 10**(2*EXPONENT) for bi in b]\n",
    "\n",
    "# out = test_model.dw_conv[0](input)\n",
    "# expected = test_model.dw_conv[1](out)\n",
    "        \n",
    "print(\"Input shape: \", input.shape)\n",
    "depth_expected = test_model.dw_conv[0](input)\n",
    "bn_expected = test_model.dw_conv[1](depth_expected)\n",
    "point_expected = test_model.pw_conv[0](bn_expected)\n",
    "expected = test_model.pw_conv[1](point_expected)\n",
    "print(\"Depth Expected shape: \", expected.shape)\n",
    "\n",
    "              \n",
    "expected = torch.permute(expected.squeeze(), (1, 2, 0))\n",
    "\n",
    "quantized_in = torch.permute(point_expected.squeeze(), (1, 2, 0)) * 10**EXPONENT\n",
    "\n",
    "X, A, B, actual, remainder = BatchNormalizationInt(5, 5, 6, 10**EXPONENT, quantized_in, quantized_a, quantized_b)\n",
    "test_output = [[[from_circom(int(out)) / 10**EXPONENT for out in vec] for vec in matrix] for matrix in actual]\n",
    "\n",
    "assert(torch.allclose(torch.Tensor(test_output), expected, atol=1e-6))\n",
    "\n",
    "_, circuit_point_bn_a, circuit_point_bn_b, circuit_point_bn_out, circuit_point_bn_remainder = BatchNormalizationInt(7, 7, 6, 10**EXPONENT, point_out, quantized_a, quantized_b)\n",
    "\n",
    "test_output = np.array([[[int(out) / 10**EXPONENT for out in asdf] for asdf in asdfasdf] for asdfasdf in circuit_point_bn_out])\n",
    "test_output = test_output[1:-1, 1:-1, :]\n",
    "\n",
    "print(\"Expected shape: \", expected.shape)\n",
    "print(\"test shape: \", test_output.shape)\n",
    "assert(torch.allclose(torch.Tensor(test_output), expected, atol=1e-6))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb20792-ff40-4c22-8acc-73b7a96f594f",
   "metadata": {},
   "source": [
    "# Padded convolution 2 iterations test with true input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8bcbe0be-2d60-4ebf-8d0f-814d63b76dfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted frog - idx: 6\n"
     ]
    }
   ],
   "source": [
    "# MODEL_WEIGHTS_PATH = './checkpoints/model_small_100epochs.pth'\n",
    "\n",
    "# model = ZkMobileNet(trainloader, num_classes=10, alpha=0.25, max_epochs=100)\n",
    "# checkpoint = torch.load(MODEL_WEIGHTS_PATH)\n",
    "# # model.load_state_dict(checkpoint['state_dict'])\n",
    "# model.load_state_dict(checkpoint['net'])\n",
    "model.eval()\n",
    "\n",
    "image, label = testset[0]\n",
    "image = image.unsqueeze(0)\n",
    "logits = model(image)\n",
    "pred_idx = logits.argmax()\n",
    "\n",
    "print(f\"Predicted {classes[pred_idx]} - idx: {pred_idx}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5b7d7f64-c2c6-420d-aae5-084443e83cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONVOLUTION LAYER\n",
    "weights = model.conv.weight.detach().numpy().transpose(2, 3, 1, 0)\n",
    "image, label = testset[0]\n",
    "\n",
    "expected = model.conv(image).detach().numpy()\n",
    "bias = torch.zeros(weights.shape[3]).numpy()\n",
    "\n",
    "# padded = pad(image, 1).transpose(1,2,0)\n",
    "padded = F.pad(image, (1,1,1,1), \"constant\", 0).numpy()\n",
    "padded = padded.transpose(1,2,0)\n",
    "\n",
    "quantized_image = (padded * 10**EXPONENT).round()\n",
    "quantized_weights = (weights * 10**EXPONENT).round() # .transpose(0, 3, 1, 0) # [nFilters, nChannels, H, W] -> \n",
    "\n",
    "circuit_in, circuit_conv_weights, circuit_conv_bias, circuit_conv_out, circuit_conv_remainder = Conv2DInt(34, 34, 3, 8, 3, 1, 10**EXPONENT, quantized_image, quantized_weights, bias)\n",
    "# output, remainder = Conv2DInt(34, 34, 3, 8, 3, 1, 10**EXPONENT, quantized_image, quantized_weights, bias)\n",
    "# test_output = output / 10**(EXPONENT)\n",
    "test_output = [[[from_circom(int(out)) / 10**EXPONENT for out in asdf] for asdf in asdfasdf] for asdfasdf in circuit_conv_out]\n",
    "\n",
    "expected = expected.transpose((1, 2, 0))\n",
    "\n",
    "assert(np.allclose(test_output, expected, atol=1e-6))\n",
    "\n",
    "# BATCH NORM CONSTANTS\n",
    "gamma = model.bn.weight\n",
    "beta = model.bn.bias\n",
    "mean = model.bn.running_mean\n",
    "var = model.bn.running_var\n",
    "eps = model.bn.eps\n",
    "\n",
    "a = (gamma/(var+eps)**.5).detach()\n",
    "b = (beta-gamma*mean/(var+eps)**.5).detach().tolist()\n",
    "\n",
    "quantized_a = (a * 10**EXPONENT).tolist()\n",
    "quantized_b = [bi * 10**(2*EXPONENT) for bi in b]\n",
    "\n",
    "# BATCH NORM USING PYTORCH OUTPUT\n",
    "image, label = testset[0]\n",
    "image = image.unsqueeze(0)\n",
    "out = model.conv(image)\n",
    "expected = model.bn(out)\n",
    "\n",
    "expected = torch.permute(expected.squeeze(), (1, 2, 0))\n",
    "\n",
    "quantized_in = torch.permute(out.squeeze(), (1, 2, 0)) * 10**EXPONENT\n",
    "\n",
    "X, A, B, actual, remainder = BatchNormalizationInt(32, 32, 8, 10**EXPONENT, quantized_in, quantized_a, quantized_b)\n",
    "test_output = [[[from_circom(int(out)) / 10**EXPONENT for out in vec] for vec in matrix] for matrix in actual]\n",
    "assert(torch.allclose(torch.Tensor(test_output), expected, atol=1e-6))\n",
    "\n",
    "# BATCH NORM USING CIRCUIT CONV OUTPUT\n",
    "quantized_in = [[[from_circom(int(out)) for out in vec] for vec in matrix] for matrix in circuit_conv_out]\n",
    "\n",
    "_, circuit_bn_a, circuit_bn_b, circuit_bn_out, circuit_bn_remainder = BatchNormalizationInt(32, 32, 8, 10**EXPONENT, quantized_in, quantized_a, quantized_b)\n",
    "\n",
    "test_output = [[[int(from_circom(int(out))) / 10**EXPONENT for out in vec] for vec in matrix] for matrix in circuit_bn_out]\n",
    "\n",
    "assert(torch.allclose(torch.Tensor(test_output), expected, atol=1e-6))\n",
    "\n",
    "# RELU USING CIRCUIT OUTPUT\n",
    "relu_in = [[[to_circom(bn_out) if bn_out < MAX_POSITIVE else 0 for bn_out in vec] for vec in matrix] for matrix in circuit_bn_out]\n",
    "relu_out = [[[str(bn_out) if bn_out < MAX_POSITIVE else 0 for bn_out in vec] for vec in matrix] for matrix in relu_in]\n",
    "\n",
    "image, label = testset[0]\n",
    "image = image.unsqueeze(0)\n",
    "conv_output = model.conv(image)\n",
    "bn_output = model.bn(conv_output)\n",
    "relu_expected = model.relu(bn_output).squeeze().detach().numpy().transpose((1,2,0))\n",
    "\n",
    "test_output = [[[out / 10**EXPONENT if out > 0 else 0 for out in vec] for vec in matrix] for matrix in circuit_bn_out]\n",
    "\n",
    "assert(np.allclose(test_output, relu_expected, atol=1e-6))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9812bfb-62fb-4a46-95ab-6253fb8ecd6d",
   "metadata": {},
   "source": [
    "# DW INPUT WITH ACTUAL IMAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "343b03fa-07c4-49b1-a56a-2edd1da98364",
   "metadata": {},
   "outputs": [],
   "source": [
    "quantized_input = torch.Tensor([[[out if out > 0 else 0 for out in vec] for vec in matrix] for matrix in circuit_bn_out])\n",
    "pytorch_input = torch.Tensor([[[int(out) / 10**EXPONENT if out > 0 else 0 for out in vec] for vec in matrix] for matrix in circuit_bn_out])\n",
    "pytorch_input = torch.permute(pytorch_input, (2, 0, 1)).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd6ee5f-3a14-447d-8be4-8eec261581ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1a167274-01b4-4cb1-b265-8f4db622cad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def BatchNormalizationInt(nRows, nCols, nChannels, n, X_in, a_in, b_in):\n",
    "    p = CIRCOM_PRIME\n",
    "    X = [[[str(int(X_in[i][j][k]) % p) for k in range(nChannels)] for j in range(nCols)] for i in range(nRows)]\n",
    "    A = [str(int(a_in[k]) % p) for k in range(nChannels)]\n",
    "    B = [str(int(b_in[k]) % p) for k in range(nChannels)]\n",
    "    out = [[[0 for _ in range(nChannels)] for _ in range(nCols)] for _ in range(nRows)]\n",
    "    out_str = [[[str(0) for _ in range(nChannels)] for _ in range(nCols)] for _ in range(nRows)]\n",
    "    remainder = [[[str(0) for _ in range(nChannels)] for _ in range(nCols)] for _ in range(nRows)]\n",
    "    for i in range(nRows):\n",
    "        for j in range(nCols):\n",
    "            for k in range(nChannels):\n",
    "                out[i][j][k] = int(int(X_in[i][j][k]) * int(a_in[k]) + int(b_in[k]))\n",
    "                remainder[i][j][k] = str(int(out[i][j][k]) % n)\n",
    "                out[i][j][k] = int(out[i][j][k] // n)\n",
    "                out_str[i][j][k] = str(out[i][j][k] % p)\n",
    "    return X, A, B, out_str, out, remainder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "id": "34c9e624-1d0e-4772-b2fd-be0064480030",
   "metadata": {},
   "outputs": [],
   "source": [
    "def circom2pytorch(circuit_output):\n",
    "    formatted = torch.Tensor([[[int(out) / 10**EXPONENT if out > 0 else 0 for out in vec] for vec in matrix] for matrix in circuit_output])\n",
    "    formatted = torch.permute(formatted, (2, 0, 1)).unsqueeze(0)\n",
    "    return formatted\n",
    "    \n",
    "def pytorch2quantized(pytorch_output: torch.Tensor):\n",
    "    return pytorch_output.squeeze().detach().numpy().transpose((1, 2, 0)) * 10**EXPONENT\n",
    "\n",
    "def dequantize(input: List[List[List[int]]], padding: int=1, channel_padding: Optional[int]=None):\n",
    "    test_output = np.array([[[int(value) / 10**EXPONENT for value in vec] for vec in matrix] for matrix in input])\n",
    "    \n",
    "    if channel_padding is None or channel_padding == 0:\n",
    "        return test_output[padding:-padding, padding:-padding, :]\n",
    "    \n",
    "    return test_output[padding:-padding, padding:-padding, :channel_padding]\n",
    "    \n",
    "def check_quantized_input(quantized_input):\n",
    "    \"\"\"quantized_input should be quantized and should be (Height, Depth, Channels)\"\"\"\n",
    "    assert(len(np.array(quantized_input).shape) == 3)\n",
    "\n",
    "def check_pytorch_input(quantized_input):\n",
    "    \"\"\"pytorch_input should be quantized and should be (N=1, Channels, Height, Depth)\"\"\"\n",
    "    assert(len(quantized_input.shape) == 4)\n",
    "    assert(quantized_input.shape[0] == 1)\n",
    "    assert(type(quantized_input) == torch.Tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4f198400-2205-4415-8b9e-3e13094775d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "from typing import List, Optional, Union\n",
    "\n",
    "class CircuitConvInput(BaseModel):\n",
    "    input: Optional[List[List[List[str]]]]\n",
    "    weights: Union[List[List[List[str]]], List[List[str]]]\n",
    "    bias: List[str]\n",
    "    out_str: List[List[List[str]]]\n",
    "    remainder: List[List[List[str]]]\n",
    "    \n",
    "    out: List[List[List[int]]]\n",
    "    \n",
    "    \n",
    "class CircuitBatchNormInput(BaseModel):\n",
    "    input: Optional[List[List[List[str]]]]\n",
    "    a: List[str]\n",
    "    b: List[str]\n",
    "    out_str: List[List[List[str]]]\n",
    "    out: List[List[List[int]]]\n",
    "    remainder: List[List[List[str]]]\n",
    "\n",
    "\n",
    "class ConvBN(BaseModel):\n",
    "    conv: CircuitConvInput\n",
    "    bn: CircuitBatchNormInput\n",
    "\n",
    "class CircuitLayerInput(BaseModel):\n",
    "    depthwise: ConvBN\n",
    "    pointwise: ConvBN\n",
    "\n",
    "    def input(self):\n",
    "        return self.depthwise.conv.input\n",
    "        \n",
    "    def out(self):\n",
    "        return self.pointwise.bn.out\n",
    "    \n",
    "    def to_json(self, json_path: str):\n",
    "        with open(json_path, \"w\") as input_file:\n",
    "            json.dump({\n",
    "                       \"in\": self.depthwise.conv.input,\n",
    "                       \"dw_conv_weights\": self.depthwise.conv.weights,\n",
    "                       \"dw_conv_bias\": self.depthwise.conv.bias,\n",
    "                       \"dw_conv_remainder\": self.depthwise.conv.remainder,\n",
    "                       \"dw_conv_out\": self.depthwise.conv.out_str,\n",
    "                \n",
    "                       \"dw_bn_a\": self.depthwise.bn.a,\n",
    "                       \"dw_bn_b\": self.depthwise.bn.b,\n",
    "                       \"dw_bn_remainder\": self.depthwise.bn.remainder,\n",
    "                       \"dw_bn_out\": self.depthwise.bn.out,\n",
    "                \n",
    "                       \"pw_conv_weights\": self.pointwise.conv.weights,\n",
    "                       \"pw_conv_bias\": self.pointwise.conv.bias,\n",
    "                       \"pw_conv_remainder\": self.pointwise.conv.remainder,\n",
    "                       \"pw_conv_out\": self.pointwise.conv.out_str,\n",
    "                \n",
    "                       \"pw_bn_a\": self.pointwise.bn.a,\n",
    "                       \"pw_bn_b\": self.pointwise.bn.b,\n",
    "                       \"pw_bn_remainder\": self.pointwise.bn.remainder,\n",
    "                       \"pw_bn_out\": self.pointwise.bn.out,\n",
    "                      },\n",
    "                      input_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "43330e06-1631-4860-be40-e027a472b01e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def TypedPaddedDepthwiseConv(nRows, nCols, nChannels, nFilters, kernelSize, strides, n, input, weights, bias):\n",
    "    Input, Weights, Bias, out_str, out, remainder = PaddedDepthwiseConv(nRows, nCols, nChannels, nFilters, kernelSize, strides, n, input, weights, bias)\n",
    "    return CircuitConvInput(\n",
    "        input=Input, \n",
    "        weights=Weights, \n",
    "        bias=Bias, \n",
    "        out_str=out_str, \n",
    "        out=out, \n",
    "        remainder=remainder, \n",
    "    )\n",
    "    \n",
    "def BatchNormalizationPadded(nRows, nCols, nChannels, n, X_in, a_in, b_in, padding):\n",
    "    p = CIRCOM_PRIME\n",
    "    X = [[[str(int(X_in[i][j][k]) % p) for k in range(nChannels)] for j in range(nCols)] for i in range(nRows)]\n",
    "    A = [str(int(a_in[k]) % p) for k in range(nChannels)]\n",
    "    B = [str(int(b_in[k]) % p) for k in range(nChannels)]\n",
    "    out = [[[0 for _ in range(nChannels)] for _ in range(nCols)] for _ in range(nRows)]\n",
    "    out_str = [[[str(0) for _ in range(nChannels)] for _ in range(nCols)] for _ in range(nRows)]\n",
    "    remainder = [[[str(0) for _ in range(nChannels)] for _ in range(nCols)] for _ in range(nRows)]\n",
    "    for i in range(padding, nRows-padding):\n",
    "        for j in range(padding, nCols-padding):\n",
    "            for k in range(nChannels):\n",
    "                out[i][j][k] = int(int(X_in[i][j][k]) * int(a_in[k]) + int(b_in[k]))\n",
    "                remainder[i][j][k] = str(int(out[i][j][k]) % n)\n",
    "                out[i][j][k] = int(out[i][j][k] // n)\n",
    "                out_str[i][j][k] = str(out[i][j][k] % p)\n",
    "    return X, A, B, out_str, out, remainder\n",
    "    \n",
    "def TypedBatchNormalizationInt(nRows, nCols, nChannels, n, X_in, a_in, b_in, padding):\n",
    "    X, A, B, out_str, out, remainder = BatchNormalizationPadded(nRows, nCols, nChannels, n, X_in, a_in, b_in, padding)\n",
    "    return CircuitBatchNormInput(\n",
    "        input=X, \n",
    "        a=A, \n",
    "        b=B, \n",
    "        out_str=out_str, \n",
    "        out=out, \n",
    "        remainder=remainder, \n",
    "    )\n",
    "    \n",
    "def TypedPointwiseConv2d(nRows, nCols, nChannels, nFilters, strides, n, input, weights, bias):\n",
    "    Input, Weights, Bias, out_str, out, remainder = PointwiseConv2d(nRows, nCols, nChannels, nFilters, strides, n, input, weights, bias)\n",
    "    return CircuitConvInput(\n",
    "        input=Input, \n",
    "        weights=Weights, \n",
    "        bias=Bias, \n",
    "        out_str=out_str, \n",
    "        out=out, \n",
    "        remainder=remainder, \n",
    "    )\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b4067b-0823-4f70-a8b0-f68a60836e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "quantized_input = torch.Tensor([[[out if out > 0 else 0 for out in vec] for vec in matrix] for matrix in circuit_bn_out])\n",
    "pytorch_input = torch.Tensor([[[int(out) / 10**EXPONENT if out > 0 else 0 for out in vec] for vec in matrix] for matrix in circuit_bn_out])\n",
    "pytorch_input = torch.permute(pytorch_input, (2, 0, 1)).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "id": "86e67b3c-d5fc-4d30-93ba-cd831d22b105",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input shape:  torch.Size([1, 8, 32, 32])\n",
      "circuit_layer_inputs\n",
      "_circuit_conv_bn\n",
      "_forward_module\n",
      "_get_conv_circuit_input\n",
      "CHANNEL PADDING!!!:  8\n",
      "QUANTIZED_INPUT SHAPE:  torch.Size([32, 32, 16])\n",
      "_depthwise circuit_input\n",
      "OUTPUT SHAPE:  (30, 30, 8)\n",
      "EXPECTED SHAPE:  (30, 30, 8)\n",
      "_forward_module\n",
      "_get_bn_circuit_input\n",
      "_circuit_conv_bn\n",
      "_forward_module\n",
      "_get_conv_circuit_input\n",
      "CHANNEL PADDING!!!:  8\n",
      "QUANTIZED_INPUT SHAPE:  torch.Size([32, 32, 16])\n",
      "_pointwise circuit_input\n",
      "WEIGHTS SHAPE:  torch.Size([8, 16])\n",
      "PADDED WEIGHTS SHAPE:  torch.Size([16, 16])\n",
      "QUANTIZED WEIGHTS SHAPE:  torch.Size([16, 16])\n",
      "OUTPUT SHAPE:  (30, 30, 16)\n",
      "EXPECTED SHAPE:  (30, 30, 16)\n",
      "_forward_module\n",
      "_get_bn_circuit_input\n"
     ]
    }
   ],
   "source": [
    "class CircuitMobilenet():\n",
    "    def __init__(self, model: ZkMobileNet, max_dims: List[int]):\n",
    "        self.model = model\n",
    "        self.model.eval()\n",
    "        self.max_rows = max_dims[0]\n",
    "        self.max_cols = max_dims[1]\n",
    "        self.max_filters = max_dims[2]\n",
    "        self.max_output_filters = max_dims[3]\n",
    "        self.dw_kernel_size = 3\n",
    "        self.stride = 1\n",
    "        self.scalar_factor = 10**EXPONENT\n",
    "\n",
    "        \n",
    "    def _get_bn_circuit_input(self, layer: int, quantized_input: List[List[List[int]]], expected: np.array, batch_norm: nn.modules.batchnorm.BatchNorm2d) -> CircuitBatchNormInput:\n",
    "        print(\"_get_bn_circuit_input\")\n",
    "        check_quantized_input(quantized_input)\n",
    "        \n",
    "        gamma = batch_norm.weight\n",
    "        beta = batch_norm.bias\n",
    "        mean = batch_norm.running_mean\n",
    "        var = batch_norm.running_var\n",
    "        eps = batch_norm.eps\n",
    "        \n",
    "        a = (gamma/(var+eps)**.5).detach().tolist()\n",
    "        b = (beta-gamma*mean/(var+eps)**.5).detach().tolist()\n",
    "        \n",
    "        quantized_a = [ai * 10**(EXPONENT) for ai in a]\n",
    "        quantized_b = [bi * 10**(2*EXPONENT) for bi in b]\n",
    "        \n",
    "        bn_input = TypedBatchNormalizationInt(\n",
    "            self.max_rows, \n",
    "            self.max_cols, \n",
    "            len(quantized_a), \n",
    "            self.scalar_factor,\n",
    "            quantized_input,\n",
    "            quantized_a,\n",
    "            quantized_b,\n",
    "            padding=(layer+1)\n",
    "        )\n",
    "        \n",
    "        test_output = dequantize(bn_input.out)\n",
    "        \n",
    "        assert(np.allclose(test_output, expected, atol=1e-5))\n",
    "        return bn_input\n",
    "\n",
    "    def _forward_module(self, module: nn.Module, input: torch.Tensor):\n",
    "        print(\"_forward_module\")\n",
    "        output = module(input)\n",
    "        expected = output.squeeze().detach().numpy().transpose((1,2,0))\n",
    "        return output, expected\n",
    "        \n",
    "    def _circuit_conv_bn(self, layer: int, pytorch_input: torch.Tensor, quantized_input: List[List[List[int]]], conv: nn.Sequential):\n",
    "        print(\"_circuit_conv_bn\")\n",
    "        conv_output, conv_expected = self._forward_module(conv[0], pytorch_input)\n",
    "        \n",
    "        circuit_conv_input = self._get_conv_circuit_input(layer, quantized_input, conv_expected, conv[0])\n",
    "        \n",
    "        bn_output, bn_expected = self._forward_module(conv[1], conv_output)\n",
    "        \n",
    "        circuit_bn_input = self._get_bn_circuit_input(layer, circuit_conv_input.out, bn_expected, conv[1])\n",
    "        \n",
    "        return ConvBN(conv=circuit_conv_input, bn=circuit_bn_input), bn_output\n",
    "\n",
    "    def circuit_layer_inputs(self, layer: int, pytorch_input: torch.Tensor, quantized_input: List[List[List[int]]]):\n",
    "        print(\"circuit_layer_inputs\")\n",
    "        check_pytorch_input(pytorch_input)\n",
    "        check_quantized_input(quantized_input)\n",
    "\n",
    "        depthwise, dw_output = self._circuit_conv_bn(layer, pytorch_input, quantized_input, self.model.features[layer].dw_conv)\n",
    "        pointwise, pw_output = self._circuit_conv_bn(layer, dw_output, depthwise.bn.out, self.model.features[layer].pw_conv)\n",
    "\n",
    "        layer_input = CircuitLayerInput(depthwise=depthwise, pointwise=pointwise)\n",
    "\n",
    "        layer_input.to_json(\"it_worked.json\")\n",
    "        return layer_input, pw_output\n",
    "\n",
    "    def _get_conv_circuit_input(self, layer: int, quantized_input: List[List[List[int]]], expected: np.array, module: nn.Conv2d) -> CircuitConvInput:\n",
    "        print(\"_get_conv_circuit_input\")\n",
    "        check_quantized_input(quantized_input)\n",
    "\n",
    "        padding = layer+1\n",
    "        channel_padding = self.max_filters - np.array(quantized_input).shape[2]\n",
    "        # channel_padding = self.max_filters - expected.shape[2]\n",
    "        \n",
    "        print(\"CHANNEL PADDING!!!: \", channel_padding)\n",
    "        weights = module.weight.detach()\n",
    "        bias = np.zeros(weights.shape[0] + channel_padding)\n",
    "        \n",
    "        # weights = weights.transpose((2, 3, 1, 0)).squeeze()\n",
    "        weights = torch.permute(weights, (2, 3, 1, 0)).squeeze()\n",
    "        # (3 x 3 x 8) -> (3 x 3 x 16)\n",
    "\n",
    "        \n",
    "        quantized_input = torch.Tensor(quantized_input)\n",
    "        quantized_input = F.pad(quantized_input, (0, channel_padding), \"constant\", 0)\n",
    "        print(\"QUANTIZED_INPUT SHAPE: \", quantized_input.shape)\n",
    "\n",
    "        if module.kernel_size == (3, 3): \n",
    "            print(\"_depthwise circuit_input\")\n",
    "            padded_weights = F.pad(weights, (0, channel_padding), \"constant\", 0)\n",
    "            # print(\"PADDED WEIGHTS SHAPE: \", padded_weights.shape)\n",
    "            quantized_weights = padded_weights * 10**EXPONENT\n",
    "        # quantized_weights = weights * 10**EXPONENT\n",
    "            # conv_input = TypedPaddedDepthwiseConv(32, 32, 8, 8, 3, 1, 10**EXPONENT, quantized_input, quantized_weights.round(), bias)\n",
    "            conv_input = TypedPaddedDepthwiseConv(\n",
    "                self.max_rows, \n",
    "                self.max_cols, \n",
    "                self.max_filters, \n",
    "                self.max_output_filters, \n",
    "                self.dw_kernel_size, \n",
    "                self.stride, \n",
    "                self.scalar_factor,\n",
    "                quantized_input, \n",
    "                quantized_weights.round(), \n",
    "                bias\n",
    "            )\n",
    "            # conv_input = TypedPaddedDepthwiseConv(32, 32, 16, 16, 3, 1, 10**EXPONENT, quantized_input, quantized_weights.round(), bias)\n",
    "            # -------------- Need to use (MAX_H, MAX_W, MAX_N_CHANNELS, MAX_N_FILTERS)\n",
    "            # conv_input = TypedPaddedDepthwiseConv(32, 32, 16, 16, 3, 1, 10**EXPONENT, circuit_layer_input.out(), quantized_weights.round(), bias)\n",
    "        elif module.kernel_size == (1, 1):\n",
    "            print(\"_pointwise circuit_input\")\n",
    "            print(\"WEIGHTS SHAPE: \", weights.shape)\n",
    "            # quantized_weights = weights * 10**EXPONENT\n",
    "            \n",
    "            padded_weights = F.pad(weights, (0, 0, 0, 8), \"constant\", 0)\n",
    "            print(\"PADDED WEIGHTS SHAPE: \", padded_weights.shape)\n",
    "            quantized_weights = padded_weights * 10**EXPONENT\n",
    "            print(\"QUANTIZED WEIGHTS SHAPE: \", quantized_weights.shape)\n",
    "            conv_input = TypedPointwiseConv2d(\n",
    "                self.max_rows, \n",
    "                self.max_cols, \n",
    "                self.max_filters,\n",
    "                self.max_output_filters,\n",
    "                self.stride, \n",
    "                self.scalar_factor,\n",
    "                quantized_input,\n",
    "                quantized_weights.round(),\n",
    "                bias\n",
    "            )\n",
    "            channel_padding = None\n",
    "        \n",
    "        # test_output = dequantize(conv_input.out)\n",
    "        test_output = dequantize(conv_input.out, padding, channel_padding)\n",
    "\n",
    "        print(\"OUTPUT SHAPE: \", test_output.shape)\n",
    "        print(\"EXPECTED SHAPE: \", expected.shape)\n",
    "        assert(np.allclose(expected, test_output, atol=1e-5))\n",
    "        return conv_input\n",
    "        \n",
    "print(\"input shape: \", pytorch_input.shape)\n",
    "\n",
    "circuit = CircuitMobilenet(model, (32, 32, 16, 16))\n",
    "circuit_layer_input, pytorch_output = circuit.circuit_layer_inputs(0, pytorch_input, quantized_input)\n",
    "\n",
    "image, label = testset[0]\n",
    "image = image.unsqueeze(0)\n",
    "conv_output = model.conv(image)\n",
    "bn_output = model.bn(conv_output)\n",
    "relu_expected = model.relu(bn_output)\n",
    "layer0_expected = model.features[0](relu_expected)\n",
    "\n",
    "assert(torch.allclose(pytorch_output, layer0_expected, atol=1e-5))\n",
    "\n",
    "test_output = dequantize(circuit_layer_input.out())\n",
    "expected = layer0_expected.squeeze().detach().numpy().transpose((1,2,0))\n",
    "assert(np.allclose(test_output, expected, atol=1e-5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8f61027-08fe-482a-9e55-d00c366497b8",
   "metadata": {},
   "source": [
    "# AAAAAAAAAAAAAAAAAAAAA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "id": "ba345ecf-7333-4932-9aaf-f79a24f3ff08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): ZkSeparableConv2d(\n",
       "    (dw_conv): Sequential(\n",
       "      (0): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), groups=8, bias=False)\n",
       "      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (pw_conv): Sequential(\n",
       "      (0): Conv2d(8, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (1): ZkSeparableConv2d(\n",
       "    (dw_conv): Sequential(\n",
       "      (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), groups=16, bias=False)\n",
       "      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (pw_conv): Sequential(\n",
       "      (0): Conv2d(16, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 382,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.features[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "id": "8a99c08e-0fdb-4ef9-9af5-4820cb1b5e06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input shape:  torch.Size([1, 8, 32, 32])\n",
      "circuit_layer_inputs\n",
      "_circuit_conv_bn\n",
      "_forward_module\n",
      "_get_conv_circuit_input\n",
      "INITAL QUANTIZED INPUT SHAPE:  (32, 32, 8)\n",
      "!!!CONV WEIGHTS SHAPE:  torch.Size([8, 1, 3, 3])\n",
      "FILTER PADDING CONV:  24\n",
      "CHANNEL PADDING CONV:  24\n",
      "TENSOR QUANTIZED INPUT SHAPE:  (32, 32, 8)\n",
      "FILTER PADDING CONV:  24\n",
      "PADDED QUANTIZED INPUT SHAPE:  (32, 32, 32)\n",
      "_depthwise circuit_input\n",
      "DEPTH QUANTIZED INPUT SHAPE:  (32, 32, 32)\n",
      "DEPTH EXPECTED OUTPUT SHAPE:  (30, 30, 8)\n",
      "DEPTH CHANNEL PADDING:  24\n",
      "DEPTH WEIGHTS SHAPE:  torch.Size([3, 3, 8])\n",
      "DEPTH PADDED WEIGHTS SHAPE:  torch.Size([3, 3, 32])\n",
      "OUTPUT SHAPE:  (32, 32, 32)\n",
      "DEQUANTIZED SHAPE:  (30, 30, 8)\n",
      "EXPECTED    SHAPE:  (30, 30, 8)\n",
      "_forward_module\n",
      "_get_bn_circuit_input -------------------------------------------------------------------------\n",
      "INITAL QUANTIZED INPUT SHAPE:  (32, 32, 32)\n",
      "EXPECTED SHAPE:  (30, 30, 8)\n",
      "SELF.MAX_FILTERS:  32\n",
      "CHANNEL_PADDING BN SHAPE:  24\n",
      "A SHAPE:  (32,)\n",
      "B SHAPE:  (32,)\n",
      "QUANTIZED_A SHAPE:  (32,)\n",
      "QUANTIZED_B SHAPE:  (32,)\n",
      "LEN(QUANTIZED_A) SHAPE:  32\n",
      "BATCHNORM OUTPUT SHAPE:  (32, 32, 32)\n",
      "EXPECTED OUTPUT SHAPE:  (30, 30, 8)\n",
      "DEQUANTIZED OUTPUT SHAPE:  (30, 30, 8)\n",
      "BATCHNORM   OUTPUT SHAPE:  (32, 32, 32)\n",
      "EXPECTED           SHAPE:  (30, 30, 8)\n",
      "------------------------------------------------------------------------------------------------\n",
      "_circuit_conv_bn\n",
      "_forward_module\n",
      "_get_conv_circuit_input\n",
      "INITAL QUANTIZED INPUT SHAPE:  (32, 32, 32)\n",
      "!!!CONV WEIGHTS SHAPE:  torch.Size([16, 8, 1, 1])\n",
      "FILTER PADDING CONV:  16\n",
      "CHANNEL PADDING CONV:  16\n",
      "TENSOR QUANTIZED INPUT SHAPE:  (32, 32, 32)\n",
      "FILTER PADDING CONV:  16\n",
      "_pointwise circuit_input\n",
      "POINT QUANTIZED INPUT SHAPE:  (32, 32, 32)\n",
      "POINT EXPECTED OUTPUT SHAPE:  (30, 30, 16)\n",
      "POINT WEIGHT PADDING:  24\n",
      "POINT WEIGHTS SHAPE:  torch.Size([8, 16])\n",
      "POINT PADDED WEIGHTS SHAPE:  torch.Size([32, 32])\n",
      "OUTPUT SHAPE:  (32, 32, 32)\n",
      "DEQUANTIZED SHAPE:  (30, 30, 16)\n",
      "EXPECTED    SHAPE:  (30, 30, 16)\n",
      "_forward_module\n",
      "_get_bn_circuit_input -------------------------------------------------------------------------\n",
      "INITAL QUANTIZED INPUT SHAPE:  (32, 32, 32)\n",
      "EXPECTED SHAPE:  (30, 30, 16)\n",
      "SELF.MAX_FILTERS:  32\n",
      "CHANNEL_PADDING BN SHAPE:  16\n",
      "A SHAPE:  (32,)\n",
      "B SHAPE:  (32,)\n",
      "QUANTIZED_A SHAPE:  (32,)\n",
      "QUANTIZED_B SHAPE:  (32,)\n",
      "LEN(QUANTIZED_A) SHAPE:  32\n",
      "BATCHNORM OUTPUT SHAPE:  (32, 32, 32)\n",
      "EXPECTED OUTPUT SHAPE:  (30, 30, 16)\n",
      "DEQUANTIZED OUTPUT SHAPE:  (30, 30, 16)\n",
      "BATCHNORM   OUTPUT SHAPE:  (32, 32, 32)\n",
      "EXPECTED           SHAPE:  (30, 30, 16)\n",
      "------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "class CircuitMobilenet():\n",
    "    def __init__(self, model: ZkMobileNet, max_dims: List[int]):\n",
    "        self.model = model\n",
    "        self.model.eval()\n",
    "        self.max_rows = max_dims[0]\n",
    "        self.max_cols = max_dims[1]\n",
    "        self.max_channels = max_dims[2]\n",
    "        self.max_filters = max_dims[3]\n",
    "        self.dw_kernel_size = 3\n",
    "        self.stride = 1\n",
    "        self.scalar_factor = 10**EXPONENT\n",
    "\n",
    "        \n",
    "\n",
    "    def _forward_module(self, module: nn.Module, input: torch.Tensor):\n",
    "        print(\"_forward_module\")\n",
    "        output = module(input)\n",
    "        expected = output.squeeze().detach().numpy().transpose((1,2,0))\n",
    "        return output, expected\n",
    "        \n",
    "    def _circuit_conv_bn(self, layer: int, pytorch_input: torch.Tensor, quantized_input: List[List[List[int]]], conv: nn.Sequential):\n",
    "        print(\"_circuit_conv_bn\")\n",
    "        conv_output, conv_expected = self._forward_module(conv[0], pytorch_input)\n",
    "        \n",
    "        circuit_conv_input = self._get_conv_circuit_input(layer, quantized_input, conv_expected, conv[0])\n",
    "        \n",
    "        bn_output, bn_expected = self._forward_module(conv[1], conv_output)\n",
    "        \n",
    "        circuit_bn_input = self._get_bn_circuit_input(layer, circuit_conv_input.out, bn_expected, conv[1])\n",
    "        \n",
    "        return ConvBN(conv=circuit_conv_input, bn=circuit_bn_input), bn_output\n",
    "\n",
    "    def circuit_layer_inputs(self, layer: int, pytorch_input: torch.Tensor, quantized_input: List[List[List[int]]]):\n",
    "        print(\"circuit_layer_inputs\")\n",
    "        check_pytorch_input(pytorch_input)\n",
    "        check_quantized_input(quantized_input)\n",
    "\n",
    "        depthwise, dw_output = self._circuit_conv_bn(layer, pytorch_input, quantized_input, self.model.features[layer].dw_conv)\n",
    "        pointwise, pw_output = self._circuit_conv_bn(layer, dw_output, depthwise.bn.out, self.model.features[layer].pw_conv)\n",
    "\n",
    "        layer_input = CircuitLayerInput(depthwise=depthwise, pointwise=pointwise)\n",
    "\n",
    "        layer_input.to_json(\"it_worked.json\")\n",
    "        return layer_input, pw_output\n",
    "\n",
    "    def _get_conv_circuit_input(self, layer: int, quantized_input: List[List[List[int]]], expected: np.array, module: nn.Conv2d) -> CircuitConvInput:\n",
    "        print(\"_get_conv_circuit_input\")\n",
    "        print(\"INITAL QUANTIZED INPUT SHAPE: \", np.array(quantized_input).shape)\n",
    "        check_quantized_input(quantized_input)\n",
    "\n",
    "        padding = layer+1\n",
    "        \n",
    "        weights = module.weight.detach()\n",
    "        print(\"!!!CONV WEIGHTS SHAPE: \", weights.shape)\n",
    "        \n",
    "        filter_padding = self.max_filters - weights.shape[0]\n",
    "        # filter_padding = self.max_filters - np.array(quantized_input).shape[2]\n",
    "        # filter_padding = self.max_filters - expected.shape[2]\n",
    "        print(\"FILTER PADDING CONV: \", filter_padding)\n",
    "        \n",
    "        assert(filter_padding >= 0)\n",
    "        bias = np.zeros(weights.shape[0] + filter_padding)\n",
    "        \n",
    "        # weights = weights.transpose((2, 3, 1, 0)).squeeze()\n",
    "        weights = torch.permute(weights, (2, 3, 1, 0)).squeeze()\n",
    "        print(\"CHANNEL PADDING CONV: \", filter_padding)\n",
    "        assert(filter_padding >= 0)\n",
    "        # (3 x 3 x 8) -> (3 x 3 x 16)\n",
    "\n",
    "        \n",
    "        quantized_input = torch.Tensor(quantized_input)\n",
    "        \n",
    "        print(\"TENSOR QUANTIZED INPUT SHAPE: \", np.array(quantized_input).shape)\n",
    "        print(\"FILTER PADDING CONV: \", filter_padding)\n",
    "        \n",
    "        if quantized_input.shape[2] != self.max_filters:\n",
    "            quantized_input = F.pad(quantized_input, (0, filter_padding), \"constant\", 0)\n",
    "            print(\"PADDED QUANTIZED INPUT SHAPE: \", np.array(quantized_input).shape)\n",
    "\n",
    "        if module.kernel_size == (3, 3): \n",
    "            print(\"_depthwise circuit_input\")\n",
    "            # weight_padding = self.max_filters - np.array(weights).shape[2]\n",
    "            \n",
    "            print(\"DEPTH QUANTIZED INPUT SHAPE: \", np.array(quantized_input).shape)\n",
    "            print(\"DEPTH EXPECTED OUTPUT SHAPE: \", expected.shape)\n",
    "            print(\"DEPTH CHANNEL PADDING: \", filter_padding)\n",
    "            print(\"DEPTH WEIGHTS SHAPE: \", weights.shape)\n",
    "            \n",
    "            padded_weights = F.pad(weights, (0, filter_padding), \"constant\", 0)\n",
    "            quantized_weights = padded_weights * 10**EXPONENT\n",
    "            \n",
    "            print(\"DEPTH PADDED WEIGHTS SHAPE: \", padded_weights.shape)\n",
    "            \n",
    "        # quantized_weights = weights * 10**EXPONENT\n",
    "            # conv_input = TypedPaddedDepthwiseConv(32, 32, 8, 8, 3, 1, 10**EXPONENT, quantized_input, quantized_weights.round(), bias)\n",
    "            conv_input = TypedPaddedDepthwiseConv(\n",
    "                self.max_rows, \n",
    "                self.max_cols, \n",
    "                self.max_channels, \n",
    "                self.max_filters, \n",
    "                self.dw_kernel_size, \n",
    "                self.stride, \n",
    "                self.scalar_factor,\n",
    "                quantized_input, \n",
    "                quantized_weights.round(), \n",
    "                bias\n",
    "            )\n",
    "            # conv_input = TypedPaddedDepthwiseConv(32, 32, 16, 16, 3, 1, 10**EXPONENT, quantized_input, quantized_weights.round(), bias)\n",
    "            # -------------- Need to use (MAX_H, MAX_W, MAX_N_CHANNELS, MAX_N_FILTERS)\n",
    "            # conv_input = TypedPaddedDepthwiseConv(32, 32, 16, 16, 3, 1, 10**EXPONENT, circuit_layer_input.out(), quantized_weights.round(), bias)\n",
    "        elif module.kernel_size == (1, 1):\n",
    "            # POINT INITIAL WEIGHT: N x C x H x W\n",
    "            # POINT INITIAL WEIGHT: N x C x H x W\n",
    "            print(\"_pointwise circuit_input\")\n",
    "            # quantized_weights = weights * 10**EXPONENT\n",
    "            \n",
    "            channel_padding = self.max_channels - np.array(weights).shape[0]\n",
    "            assert(channel_padding >= 0)\n",
    "            print(\"POINT QUANTIZED INPUT SHAPE: \", np.array(quantized_input).shape)\n",
    "            print(\"POINT EXPECTED OUTPUT SHAPE: \", expected.shape)\n",
    "            print(\"POINT WEIGHT PADDING: \", channel_padding)\n",
    "            print(\"POINT WEIGHTS SHAPE: \", weights.shape)\n",
    "            # padded_weights = F.pad(weights, (0, channel_padding, 0, 0), \"constant\", 0)\n",
    "            padded_weights = F.pad(weights, (0, filter_padding, 0, channel_padding), \"constant\", 0)\n",
    "            print(\"POINT PADDED WEIGHTS SHAPE: \", padded_weights.shape)\n",
    "            quantized_weights = padded_weights * 10**EXPONENT\n",
    "            conv_input = TypedPointwiseConv2d(\n",
    "                self.max_rows, \n",
    "                self.max_cols, \n",
    "                self.max_channels,\n",
    "                self.max_filters,\n",
    "                self.stride, \n",
    "                self.scalar_factor,\n",
    "                quantized_input,\n",
    "                quantized_weights.round(),\n",
    "                bias\n",
    "            )\n",
    "            # channel_padding = None\n",
    "        \n",
    "        # test_output = dequantize(conv_input.out)\n",
    "        test_output = dequantize(conv_input.out, padding, self.max_filters - filter_padding)\n",
    "\n",
    "        print(\"OUTPUT SHAPE: \", np.array(conv_input.out).shape)\n",
    "        print(\"DEQUANTIZED SHAPE: \", np.array(test_output).shape)\n",
    "        print(\"EXPECTED    SHAPE: \", expected.shape)\n",
    "        if not np.allclose(test_output, expected, atol=1e-4):\n",
    "            print(\"EXPECTED::: \", expected[0][0])\n",
    "            print(\"ACTUALLL::: \", test_output[0][0])\n",
    "        assert(np.allclose(expected, test_output, atol=1e-4))\n",
    "        return conv_input\n",
    "        \n",
    "    def _get_bn_circuit_input(self, layer: int, quantized_input: List[List[List[int]]], expected: np.array, batch_norm: nn.modules.batchnorm.BatchNorm2d) -> CircuitBatchNormInput:\n",
    "        print(\"_get_bn_circuit_input -------------------------------------------------------------------------\")\n",
    "        print(\"INITAL QUANTIZED INPUT SHAPE: \", np.array(quantized_input).shape)\n",
    "        check_quantized_input(quantized_input)\n",
    "        \n",
    "        padding = layer+1\n",
    "        # channel_padding = self.max_filters - np.array(quantized_input).shape[2]\n",
    "        assert(len(expected.shape) == 3)\n",
    "        channel_padding = self.max_filters - expected.shape[2]\n",
    "        print(\"EXPECTED SHAPE: \", expected.shape)\n",
    "        print(\"SELF.MAX_FILTERS: \", self.max_filters)\n",
    "        print(\"CHANNEL_PADDING BN SHAPE: \", channel_padding)\n",
    "        \n",
    "        gamma = batch_norm.weight\n",
    "        beta = batch_norm.bias\n",
    "        mean = batch_norm.running_mean\n",
    "        var = batch_norm.running_var\n",
    "        eps = batch_norm.eps\n",
    "        \n",
    "        a = (gamma/(var+eps)**.5).detach()\n",
    "        b = (beta-gamma*mean/(var+eps)**.5).detach()\n",
    "        \n",
    "        channel_padding = self.max_filters - len(a)\n",
    "        a = F.pad(a, (0, channel_padding), \"constant\", 0).tolist()\n",
    "        b = F.pad(b, (0, channel_padding), \"constant\", 0).tolist()\n",
    "        print(\"A SHAPE: \", np.array(a).shape)\n",
    "        print(\"B SHAPE: \", np.array(b).shape)\n",
    "        \n",
    "        quantized_a = [ai * 10**(EXPONENT) for ai in a]\n",
    "        quantized_b = [bi * 10**(2*EXPONENT) for bi in b]\n",
    "        \n",
    "        print(\"QUANTIZED_A SHAPE: \", np.array(quantized_a).shape)\n",
    "        print(\"QUANTIZED_B SHAPE: \", np.array(quantized_b).shape)\n",
    "        print(\"LEN(QUANTIZED_A) SHAPE: \", len(quantized_a))\n",
    "        \n",
    "        bn_input = TypedBatchNormalizationInt(\n",
    "            self.max_rows, \n",
    "            self.max_cols, \n",
    "            len(quantized_a), \n",
    "            self.scalar_factor,\n",
    "            quantized_input,\n",
    "            quantized_a,\n",
    "            quantized_b,\n",
    "            padding=(layer+1)\n",
    "        )\n",
    "        print(\"BATCHNORM OUTPUT SHAPE: \", np.array(bn_input.out).shape)\n",
    "        print(\"EXPECTED OUTPUT SHAPE: \", expected.shape)\n",
    "\n",
    "        # if channel_padding == 0:\n",
    "        #     test_output = dequantize(bn_input.out, padding, None)\n",
    "        # else:\n",
    "        #     test_output = dequantize(bn_input.out, padding, channel_padding)\n",
    "        # test_output = dequantize(bn_input.out, padding, channel_padding)\n",
    "        test_output = dequantize(bn_input.out, padding, self.max_filters - channel_padding)\n",
    "        print(\"DEQUANTIZED OUTPUT SHAPE: \", np.array(test_output).shape)\n",
    "        print(\"BATCHNORM   OUTPUT SHAPE: \", np.array(bn_input.out).shape)\n",
    "        print(\"EXPECTED           SHAPE: \", expected.shape)\n",
    "\n",
    "        if not np.allclose(test_output, expected, atol=1e-4):\n",
    "            print(\"EXPECTED::: \", expected[0][0])\n",
    "            print(\"ACTUALLL::: \", test_output[0][0])\n",
    "        assert(np.allclose(test_output, expected, atol=1e-4))\n",
    "        print(\"------------------------------------------------------------------------------------------------\")\n",
    "        return bn_input\n",
    "        \n",
    "print(\"input shape: \", pytorch_input.shape)\n",
    "\n",
    "# circuit = CircuitMobilenet(model, (32, 32, 16, 16))\n",
    "MAX_ROWS, MAX_COLS, MAX_CHANNELS, MAX_FILTERS = (32, 32, 32, 32)\n",
    "circuit = CircuitMobilenet(model, (MAX_ROWS, MAX_COLS, MAX_CHANNELS, MAX_FILTERS))\n",
    "circuit_layer_input, pytorch_output = circuit.circuit_layer_inputs(0, pytorch_input, quantized_input)\n",
    "\n",
    "image, label = testset[0]\n",
    "image = image.unsqueeze(0)\n",
    "conv_output = model.conv(image)\n",
    "bn_output = model.bn(conv_output)\n",
    "relu_expected = model.relu(bn_output)\n",
    "layer0_expected = model.features[0](relu_expected)\n",
    "\n",
    "assert(torch.allclose(pytorch_output, layer0_expected, atol=1e-5))\n",
    "\n",
    "padding = (np.array(circuit_layer_input.out()).shape[0] - expected.shape[0]) // 2\n",
    "channel_padding = np.array(circuit_layer_input.out()).shape[2] - expected.shape[2]\n",
    "test_output = dequantize(circuit_layer_input.out(), padding, MAX_FILTERS - channel_padding)\n",
    "expected = layer0_expected.squeeze().detach().numpy().transpose((1,2,0))\n",
    "assert(np.allclose(test_output, expected, atol=1e-5))\n",
    "# circuit_layer_input, pytorch_output = circuit.circuit_layer_inputs(1, pytorch_output, circuit_layer_input.out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "id": "2559b27d-bdda-4bb8-8235-0cf2bd0529be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "circuit_layer_inputs\n",
      "_circuit_conv_bn\n",
      "_forward_module\n",
      "_get_conv_circuit_input\n",
      "INITAL QUANTIZED INPUT SHAPE:  (32, 32, 32)\n",
      "!!!CONV WEIGHTS SHAPE:  torch.Size([16, 1, 3, 3])\n",
      "FILTER PADDING CONV:  16\n",
      "CHANNEL PADDING CONV:  16\n",
      "TENSOR QUANTIZED INPUT SHAPE:  (32, 32, 32)\n",
      "FILTER PADDING CONV:  16\n",
      "_depthwise circuit_input\n",
      "DEPTH QUANTIZED INPUT SHAPE:  (32, 32, 32)\n",
      "DEPTH EXPECTED OUTPUT SHAPE:  (28, 28, 16)\n",
      "DEPTH CHANNEL PADDING:  16\n",
      "DEPTH WEIGHTS SHAPE:  torch.Size([3, 3, 16])\n",
      "DEPTH PADDED WEIGHTS SHAPE:  torch.Size([3, 3, 32])\n",
      "OUTPUT SHAPE:  (32, 32, 32)\n",
      "DEQUANTIZED SHAPE:  (28, 28, 16)\n",
      "EXPECTED    SHAPE:  (28, 28, 16)\n",
      "_forward_module\n",
      "_get_bn_circuit_input -------------------------------------------------------------------------\n",
      "INITAL QUANTIZED INPUT SHAPE:  (32, 32, 32)\n",
      "EXPECTED SHAPE:  (28, 28, 16)\n",
      "SELF.MAX_FILTERS:  32\n",
      "CHANNEL_PADDING BN SHAPE:  16\n",
      "A SHAPE:  (32,)\n",
      "B SHAPE:  (32,)\n",
      "QUANTIZED_A SHAPE:  (32,)\n",
      "QUANTIZED_B SHAPE:  (32,)\n",
      "LEN(QUANTIZED_A) SHAPE:  32\n",
      "BATCHNORM OUTPUT SHAPE:  (32, 32, 32)\n",
      "EXPECTED OUTPUT SHAPE:  (28, 28, 16)\n",
      "DEQUANTIZED OUTPUT SHAPE:  (28, 28, 16)\n",
      "BATCHNORM   OUTPUT SHAPE:  (32, 32, 32)\n",
      "EXPECTED           SHAPE:  (28, 28, 16)\n",
      "------------------------------------------------------------------------------------------------\n",
      "_circuit_conv_bn\n",
      "_forward_module\n",
      "_get_conv_circuit_input\n",
      "INITAL QUANTIZED INPUT SHAPE:  (32, 32, 32)\n",
      "!!!CONV WEIGHTS SHAPE:  torch.Size([32, 16, 1, 1])\n",
      "FILTER PADDING CONV:  0\n",
      "CHANNEL PADDING CONV:  0\n",
      "TENSOR QUANTIZED INPUT SHAPE:  (32, 32, 32)\n",
      "FILTER PADDING CONV:  0\n",
      "_pointwise circuit_input\n",
      "POINT QUANTIZED INPUT SHAPE:  (32, 32, 32)\n",
      "POINT EXPECTED OUTPUT SHAPE:  (28, 28, 32)\n",
      "POINT WEIGHT PADDING:  16\n",
      "POINT WEIGHTS SHAPE:  torch.Size([16, 32])\n",
      "POINT PADDED WEIGHTS SHAPE:  torch.Size([32, 32])\n",
      "OUTPUT SHAPE:  (32, 32, 32)\n",
      "DEQUANTIZED SHAPE:  (28, 28, 32)\n",
      "EXPECTED    SHAPE:  (28, 28, 32)\n",
      "_forward_module\n",
      "_get_bn_circuit_input -------------------------------------------------------------------------\n",
      "INITAL QUANTIZED INPUT SHAPE:  (32, 32, 32)\n",
      "EXPECTED SHAPE:  (28, 28, 32)\n",
      "SELF.MAX_FILTERS:  32\n",
      "CHANNEL_PADDING BN SHAPE:  0\n",
      "A SHAPE:  (32,)\n",
      "B SHAPE:  (32,)\n",
      "QUANTIZED_A SHAPE:  (32,)\n",
      "QUANTIZED_B SHAPE:  (32,)\n",
      "LEN(QUANTIZED_A) SHAPE:  32\n",
      "BATCHNORM OUTPUT SHAPE:  (32, 32, 32)\n",
      "EXPECTED OUTPUT SHAPE:  (28, 28, 32)\n",
      "DEQUANTIZED OUTPUT SHAPE:  (28, 28, 32)\n",
      "BATCHNORM   OUTPUT SHAPE:  (32, 32, 32)\n",
      "EXPECTED           SHAPE:  (28, 28, 32)\n",
      "------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "circuit_layer_input, pytorch_output = circuit.circuit_layer_inputs(1, pytorch_output, circuit_layer_input.out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "id": "02566179-70c5-4517-80c7-47ac2d5f7028",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pytorch_output shape:  torch.Size([1, 16, 30, 30])\n",
      "OK\n",
      "OK\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "image, label = testset[0]\n",
    "image = image.unsqueeze(0)\n",
    "conv_output = model.conv(image)\n",
    "bn_output = model.bn(conv_output)\n",
    "relu_expected = model.relu(bn_output)\n",
    "layer0_expected = model.features[0](relu_expected)\n",
    "\n",
    "assert(torch.allclose(pytorch_output, layer0_expected, atol=1e-5))\n",
    "\n",
    "padding = (np.array(circuit_layer_input.out()).shape[0] - expected.shape[0]) // 2\n",
    "channel_padding = np.array(circuit_layer_input.out()).shape[2] - expected.shape[2]\n",
    "test_output = dequantize(circuit_layer_input.out(), padding, MAX_FILTERS - channel_padding)\n",
    "expected = layer0_expected.squeeze().detach().numpy().transpose((1,2,0))\n",
    "assert(np.allclose(test_output, expected, atol=1e-5))\n",
    "# circuit_layer_input, pytorch_output = circuit.circuit_layer_inputs(1, pytorch_output, circuit_layer_input.out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "id": "35bafd12-44a9-4491-8d07-383924bb69bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXPECTED SHAPE:  (30, 30, 16)\n",
      "ACTUAL   SHAPE:  (32, 32, 32)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1, 16)"
      ]
     },
     "execution_count": 417,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padding = (np.array(circuit_layer_input.out()).shape[0] - expected.shape[0]) // 2\n",
    "channel_padding = np.array(circuit_layer_input.out()).shape[2] - expected.shape[2]\n",
    "print(\"EXPECTED SHAPE: \", expected.shape)\n",
    "print(\"ACTUAL   SHAPE: \", np.array(circuit_layer_input.out()).shape)\n",
    "padding, channel_padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "id": "14b1cd83-0f48-475d-9f25-f37047b67571",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'ZkSeparableConv2d' object has no attribute 'weight'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[392], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m batch_norm \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mfeatures[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m      2\u001b[0m check_quantized_input(quantized_input)\n\u001b[0;32m----> 4\u001b[0m gamma \u001b[38;5;241m=\u001b[39m \u001b[43mbatch_norm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\n\u001b[1;32m      5\u001b[0m beta \u001b[38;5;241m=\u001b[39m batch_norm\u001b[38;5;241m.\u001b[39mbias\n\u001b[1;32m      6\u001b[0m mean \u001b[38;5;241m=\u001b[39m batch_norm\u001b[38;5;241m.\u001b[39mrunning_mean\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/nn/modules/module.py:1695\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1693\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[1;32m   1694\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1695\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'ZkSeparableConv2d' object has no attribute 'weight'"
     ]
    }
   ],
   "source": [
    "batch_norm = model.features[0]\n",
    "check_quantized_input(quantized_input)\n",
    "\n",
    "gamma = batch_norm.weight\n",
    "beta = batch_norm.bias\n",
    "mean = batch_norm.running_mean\n",
    "var = batch_norm.running_var\n",
    "eps = batch_norm.eps\n",
    "\n",
    "a = (gamma/(var+eps)**.5).detach()\n",
    "b = (beta-gamma*mean/(var+eps)**.5).detach()\n",
    "\n",
    "channel_padding = self.max_filters - expected.shape[2]\n",
    "print(\"CHANNEL PADDING BATCH NORM: \", channel_padding)\n",
    "\n",
    "a = F.pad(a, (0, channel_padding), \"constant\", 0).tolist()\n",
    "b = F.pad(b, (0, channel_padding), \"constant\", 0).tolist()\n",
    "\n",
    "print(\"A shape: \", np.array(a).shape)\n",
    "print(\"B shape: \", np.array(b).shape)\n",
    "\n",
    "quantized_a = [ai * 10**(EXPONENT) for ai in a]\n",
    "quantized_b = [bi * 10**(2*EXPONENT) for bi in b]\n",
    "print(\"quantized_a shape: \", np.array(quantized_a).shape)\n",
    "print(\"quantized_a shape: \", np.array(quantized_b).shape)\n",
    "print(\"quantized_b shape: \", channel_padding)\n",
    "\n",
    "\n",
    "bn_input = TypedBatchNormalizationInt(\n",
    "    self.max_rows, \n",
    "    self.max_cols, \n",
    "    # len(quantized_a), \n",
    "    self.max_filters,\n",
    "    self.scalar_factor,\n",
    "    quantized_input,\n",
    "    quantized_a,\n",
    "    quantized_b,\n",
    "    padding=(layer+1)\n",
    ")\n",
    "\n",
    "# test_output = dequantize(bn_input.out)\n",
    "padding = layer+1\n",
    "test_output = dequantize(conv_input.out, padding, channel_padding)\n",
    "\n",
    "assert(np.allclose(test_output, expected, atol=1e-5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "124e27b0-9b33-4565-8cc1-8f0375b42d89",
   "metadata": {},
   "source": [
    "# REWRITING FOR SECOND LAYER!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "b0ae88c2-4eaa-4b55-bb50-986a462b55b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image shape:  torch.Size([1, 3, 32, 32])\n",
      "Conv  shape:  torch.Size([1, 8, 32, 32])\n",
      "BN    shape:  torch.Size([1, 8, 32, 32])\n",
      "ReLU  shape:  torch.Size([1, 8, 32, 32])\n",
      "after features[0].depthwise shape:  torch.Size([1, 16, 30, 30])\n",
      "----------> layer0 depthwise  shape:  (32, 32, 16)\n",
      "after features[1].pointwise shape:  torch.Size([1, 16, 30, 30])\n",
      "----------> layer0 pointwise  shape:  (32, 32, 16)\n",
      "expected shape:  torch.Size([1, 16, 30, 30])\n",
      "----------> layer1 --->> quantized_weights  shape:  (3, 3, 16)\n",
      "----------> layer1 depthwise  shape:  (32, 32, 16)\n",
      "conv_input.out shape:  (32, 32, 16)\n",
      "test_output shape:  (28, 28, 16)\n",
      "expected shape:  (1, 16, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "layer1_expected = model.features[1](layer0_expected)\n",
    "print(\"Image shape: \", image.shape)\n",
    "print(\"Conv  shape: \", conv_output.shape)\n",
    "print(\"BN    shape: \", bn_output.shape)\n",
    "print(\"ReLU  shape: \", relu_expected.shape)\n",
    "\n",
    "layer0_dw_expected = model.features[0].dw_conv(relu_expected)\n",
    "print(\"after features[0].depthwise shape: \", layer0_expected.shape)\n",
    "print(\"----------> layer0 depthwise  shape: \", np.array(circuit_layer_input.depthwise.conv.out).shape)\n",
    "layer0_expected = model.features[0].pw_conv(layer0_dw_expected)\n",
    "print(\"after features[1].pointwise shape: \", layer0_expected.shape)\n",
    "print(\"----------> layer0 pointwise  shape: \", np.array(circuit_layer_input.pointwise.conv.out).shape)\n",
    "expected = model.features[1].dw_conv[0](layer0_expected).detach()\n",
    "print(\"expected shape: \", pytorch_output.shape)\n",
    "\n",
    "# print(\"depthwise circuit_input\")\n",
    "weights = model.features[1].dw_conv[0].weight.detach().numpy()\n",
    "bias = torch.zeros(weights.shape[0]).numpy()\n",
    "\n",
    "weights = weights.transpose((2, 3, 1, 0)).squeeze()\n",
    "quantized_weights = weights * 10**EXPONENT\n",
    "print(\"----------> layer1 --->> quantized_weights  shape: \", np.array(quantized_weights).shape)\n",
    "\n",
    "conv_input = TypedPaddedDepthwiseConv(32, 32, 16, 16, 3, 1, 10**EXPONENT, circuit_layer_input.out(), quantized_weights.round(), bias)\n",
    "    \n",
    "print(\"----------> layer1 depthwise  shape: \", np.array(conv_input.out).shape)\n",
    "test_output = dequantize(conv_input.out, 2)\n",
    "print(\"conv_input.out shape: \", np.array(conv_input.out).shape)\n",
    "print(\"test_output shape: \", np.array(test_output).shape)\n",
    "print(\"expected shape: \", np.array(expected).shape)\n",
    "expected = expected.squeeze().detach().numpy().transpose((1, 2, 0))\n",
    "\n",
    "assert(np.allclose(expected, test_output, atol=1e-5))\n",
    "# circuit_depth_input = circuit.circuit_layer_inputs(1, layer0_expected, circuit_layer_input.out())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee865f9-a0f3-4ac7-8bb0-05762981d817",
   "metadata": {},
   "source": [
    "# Testing padded convolution on layer 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "39873921-1114-44df-9b5f-2b15ef1b5900",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image shape:  torch.Size([1, 3, 32, 32])\n",
      "Conv  shape:  torch.Size([1, 8, 32, 32])\n",
      "BN    shape:  torch.Size([1, 8, 32, 32])\n",
      "ReLU  shape:  torch.Size([1, 8, 32, 32])\n",
      "layer0_dw_expected shape:  torch.Size([1, 8, 30, 30])\n",
      "transposed layer0_dw_expected shape:  (30, 30, 8)\n",
      "WEIGHTS SHAPE:  torch.Size([8, 1, 3, 3])\n",
      "TRANSPOSED WEIGHTS SHAPE:  torch.Size([3, 3, 8])\n",
      "PADDED WEIGHTS SHAPE:  torch.Size([3, 3, 16])\n",
      "quantized_input INPUT SHAPE:  (32, 32, 16)\n",
      "test_output shape:  (30, 30, 8)\n",
      "expected shape:  (30, 30, 8)\n"
     ]
    }
   ],
   "source": [
    "# # Need 32 x 32 x 256 input/output shapes\n",
    "# Have 32 x 32 x 8 input shape,\n",
    "# Need 32 x 32 x 16 input/output shapes\n",
    "\n",
    "image, label = testset[0]\n",
    "image = image.unsqueeze(0)\n",
    "conv_output = model.conv(image)\n",
    "bn_output = model.bn(conv_output)\n",
    "relu_expected = model.relu(bn_output)\n",
    "layer0_dw_expected = model.features[0].dw_conv[0](relu_expected)\n",
    "expected = layer0_dw_expected.squeeze().detach().numpy().transpose((1, 2, 0))\n",
    "\n",
    "weights = model.features[0].dw_conv[0].weight\n",
    "bias = torch.zeros(16).numpy()\n",
    "\n",
    "weights = torch.permute(weights, (2, 3, 1, 0)).squeeze()\n",
    "print(\"TRANSPOSED WEIGHTS SHAPE: \", weights.shape)\n",
    "\n",
    "# (3 x 3 x 8) -> (3 x 3 x 16)\n",
    "padded_weights = F.pad(weights, (0, 8), \"constant\", 0)\n",
    "print(\"PADDED WEIGHTS SHAPE: \", padded_weights.shape)\n",
    "quantized_weights = padded_weights * 10**EXPONENT\n",
    "\n",
    "# dw_input = circuit_layer_input.out()\n",
    "\n",
    "quantized_input = torch.Tensor([[[out if out > 0 else 0 for out in vec] for vec in matrix] for matrix in circuit_bn_out])\n",
    "quantized_input = F.pad(quantized_input, (0, 8), \"constant\", 0)\n",
    "print(\"quantized_input INPUT SHAPE: \", np.array(quantized_input).shape)\n",
    "\n",
    "conv_input = TypedPaddedDepthwiseConv(32, 32, 16, 16, 3, 1, 10**EXPONENT, quantized_input, quantized_weights.round(), bias)\n",
    "    \n",
    "test_output = dequantize(conv_input.out, 1, 8)\n",
    "\n",
    "print(\"test_output shape: \", np.array(test_output).shape)\n",
    "print(\"expected shape: \", np.array(expected).shape)\n",
    "assert(np.allclose(expected, test_output, atol=1e-5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "19cacaaf-0ac0-48fc-a803-177456e66e7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "START\n",
      "PRIME - 1 28948022309329048855892746252171976963363056481941647379679742748393362948096\n",
      "------------------------------------------------\n",
      "------------------------------------------------\n",
      "dw_conv done\n",
      "depth batch norm done\n",
      "pw_conv done\n",
      "point batch norm done\n",
      "END\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "os.chdir(\"circuits\")\n",
    "!./padded/padded_cpp/padded ../it_worked.json head.wtns\n",
    "# # !npx snarkjs groth16 prove ./origDepthwiseConv2d/circuit_final.zkey head.wtns proof.json public_test.json\n",
    "os.chdir(\"../\")\n",
    "\n",
    "print(\"OK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "id": "5f55fcb9-fa1c-4733-9935-b32635199b00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "circuit_layer_inputs\n",
      "_circuit_conv_bn\n",
      "_forward_module\n",
      "_get_conv_circuit_input\n",
      "CONV WEIGHTS SHAPE:  torch.Size([16, 1, 3, 3])\n",
      "_depthwise circuit_input\n",
      "DEPTH QUANTIZED INPUT SHAPE:  (32, 32, 16)\n",
      "DEPTH EXPECTED OUTPUT SHAPE:  (28, 28, 16)\n",
      "DEPTH CHANNEL PADDING:  0\n",
      "DEPTH WEIGHTS SHAPE:  torch.Size([3, 3, 16])\n",
      "DEPTH PADDED WEIGHTS SHAPE:  torch.Size([3, 3, 16])\n",
      "OUTPUT SHAPE:  (32, 32, 16)\n",
      "DEQUANTIZED SHAPE:  (28, 28, 16)\n",
      "EXPECTED    SHAPE:  (28, 28, 16)\n",
      "_forward_module\n",
      "_get_bn_circuit_input -------------------------------------------------------------------------\n",
      "QUANTIZED INPUT SHAPE:  (28, 28, 16)\n",
      "SELF.MAX_FILTERS:  16\n",
      "CHANNEL_PADDING BN SHAPE:  0\n",
      "A SHAPE:  (16,)\n",
      "B SHAPE:  (16,)\n",
      "QUANTIZED_A SHAPE:  (16,)\n",
      "QUANTIZED_B SHAPE:  (16,)\n",
      "LEN(QUANTIZED_A) SHAPE:  16\n",
      "BATCHNORM OUTPUT SHAPE:  (32, 32, 16)\n",
      "EXPECTED OUTPUT SHAPE:  (28, 28, 16)\n",
      "DEQUANTIZED OUTPUT SHAPE:  (28, 28, 16)\n",
      "------------------------------------------------------------------------------------------------\n",
      "_circuit_conv_bn\n",
      "_forward_module\n",
      "_get_conv_circuit_input\n",
      "CONV WEIGHTS SHAPE:  torch.Size([32, 16, 1, 1])\n",
      "_pointwise circuit_input\n",
      "POINT QUANTIZED INPUT SHAPE:  (32, 32, 0)\n",
      "POINT EXPECTED OUTPUT SHAPE:  (28, 28, 32)\n",
      "POINT CHANNEL PADDING:  0\n",
      "POINT WEIGHTS SHAPE:  torch.Size([16, 32])\n",
      "POINT PADDED WEIGHTS SHAPE:  torch.Size([16, 32])\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 0 is out of bounds for dimension 0 with size 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[367], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# circuit_layer_input, pytorch_output = circuit.circuit_layer_inputs(0, pytorch_input, quantized_input)\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m circuit_layer_input, pytorch_output \u001b[38;5;241m=\u001b[39m \u001b[43mcircuit\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcircuit_layer_inputs\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpytorch_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcircuit_layer_input\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mout\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# circuit_depth_input = circuit.circuit_layer_inputs(1, pytorch_input, quantized_input)\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[366], line 96\u001b[0m, in \u001b[0;36mCircuitMobilenet.circuit_layer_inputs\u001b[0;34m(self, layer, pytorch_input, quantized_input)\u001b[0m\n\u001b[1;32m     93\u001b[0m check_quantized_input(quantized_input)\n\u001b[1;32m     95\u001b[0m depthwise, dw_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_circuit_conv_bn(layer, pytorch_input, quantized_input, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mfeatures[layer]\u001b[38;5;241m.\u001b[39mdw_conv)\n\u001b[0;32m---> 96\u001b[0m pointwise, pw_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_circuit_conv_bn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlayer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdw_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdepthwise\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlayer\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpw_conv\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     98\u001b[0m layer_input \u001b[38;5;241m=\u001b[39m CircuitLayerInput(depthwise\u001b[38;5;241m=\u001b[39mdepthwise, pointwise\u001b[38;5;241m=\u001b[39mpointwise)\n\u001b[1;32m    100\u001b[0m layer_input\u001b[38;5;241m.\u001b[39mto_json(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mit_worked.json\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[366], line 82\u001b[0m, in \u001b[0;36mCircuitMobilenet._circuit_conv_bn\u001b[0;34m(self, layer, pytorch_input, quantized_input, conv)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_circuit_conv_bn\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     80\u001b[0m conv_output, conv_expected \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_module(conv[\u001b[38;5;241m0\u001b[39m], pytorch_input)\n\u001b[0;32m---> 82\u001b[0m circuit_conv_input \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_conv_circuit_input\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlayer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquantized_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconv_expected\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconv\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     84\u001b[0m bn_output, bn_expected \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_module(conv[\u001b[38;5;241m1\u001b[39m], conv_output)\n\u001b[1;32m     86\u001b[0m circuit_bn_input \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_bn_circuit_input(layer, circuit_conv_input\u001b[38;5;241m.\u001b[39mout, bn_expected, conv[\u001b[38;5;241m1\u001b[39m])\n",
      "Cell \u001b[0;32mIn[366], line 168\u001b[0m, in \u001b[0;36mCircuitMobilenet._get_conv_circuit_input\u001b[0;34m(self, layer, quantized_input, expected, module)\u001b[0m\n\u001b[1;32m    166\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPOINT PADDED WEIGHTS SHAPE: \u001b[39m\u001b[38;5;124m\"\u001b[39m, padded_weights\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m    167\u001b[0m     quantized_weights \u001b[38;5;241m=\u001b[39m padded_weights \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m10\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mEXPONENT\n\u001b[0;32m--> 168\u001b[0m     conv_input \u001b[38;5;241m=\u001b[39m \u001b[43mTypedPointwiseConv2d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    169\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_rows\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_cols\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_filters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_output_filters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscalar_factor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquantized_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquantized_weights\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mround\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbias\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    179\u001b[0m     channel_padding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    181\u001b[0m \u001b[38;5;66;03m# test_output = dequantize(conv_input.out)\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[273], line 41\u001b[0m, in \u001b[0;36mTypedPointwiseConv2d\u001b[0;34m(nRows, nCols, nChannels, nFilters, strides, n, input, weights, bias)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mTypedPointwiseConv2d\u001b[39m(nRows, nCols, nChannels, nFilters, strides, n, \u001b[38;5;28minput\u001b[39m, weights, bias):\n\u001b[0;32m---> 41\u001b[0m     Input, Weights, Bias, out_str, out, remainder \u001b[38;5;241m=\u001b[39m \u001b[43mPointwiseConv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnRows\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnCols\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnChannels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnFilters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrides\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m CircuitConvInput(\n\u001b[1;32m     43\u001b[0m         \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m=\u001b[39mInput, \n\u001b[1;32m     44\u001b[0m         weights\u001b[38;5;241m=\u001b[39mWeights, \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     48\u001b[0m         remainder\u001b[38;5;241m=\u001b[39mremainder, \n\u001b[1;32m     49\u001b[0m     )\n",
      "Cell \u001b[0;32mIn[10], line 8\u001b[0m, in \u001b[0;36mPointwiseConv2d\u001b[0;34m(nRows, nCols, nChannels, nFilters, strides, n, input, weights, bias)\u001b[0m\n\u001b[1;32m      4\u001b[0m outCols \u001b[38;5;241m=\u001b[39m (nCols \u001b[38;5;241m-\u001b[39m kernelSize)\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39mstrides \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# out = [[[0 for _ in range(nFilters)] for _ in range(nCols)] for _ in range(nRows)]\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# remainder = [[[None for _ in range(nFilters)] for _ in range(nCols)] for _ in range(nRows)]\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m Input \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mj\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m%\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mnChannels\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mnCols\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mnRows\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Weights = [[[str(int(weights[i][j][k].round()) % p) for k in range(weights.shape[2])] for j in range(weights.shape[1])] for i in range(weights.shape[0])]\u001b[39;00m\n\u001b[1;32m     10\u001b[0m Weights \u001b[38;5;241m=\u001b[39m [[\u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mint\u001b[39m(weights[i][j]\u001b[38;5;241m.\u001b[39mround()) \u001b[38;5;241m%\u001b[39m p)\u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(weights\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m])] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(weights\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m])]\n",
      "Cell \u001b[0;32mIn[10], line 8\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      4\u001b[0m outCols \u001b[38;5;241m=\u001b[39m (nCols \u001b[38;5;241m-\u001b[39m kernelSize)\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39mstrides \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# out = [[[0 for _ in range(nFilters)] for _ in range(nCols)] for _ in range(nRows)]\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# remainder = [[[None for _ in range(nFilters)] for _ in range(nCols)] for _ in range(nRows)]\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m Input \u001b[38;5;241m=\u001b[39m [\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mj\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m%\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mnChannels\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mnCols\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(nRows)]\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Weights = [[[str(int(weights[i][j][k].round()) % p) for k in range(weights.shape[2])] for j in range(weights.shape[1])] for i in range(weights.shape[0])]\u001b[39;00m\n\u001b[1;32m     10\u001b[0m Weights \u001b[38;5;241m=\u001b[39m [[\u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mint\u001b[39m(weights[i][j]\u001b[38;5;241m.\u001b[39mround()) \u001b[38;5;241m%\u001b[39m p)\u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(weights\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m])] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(weights\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m])]\n",
      "Cell \u001b[0;32mIn[10], line 8\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      4\u001b[0m outCols \u001b[38;5;241m=\u001b[39m (nCols \u001b[38;5;241m-\u001b[39m kernelSize)\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39mstrides \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# out = [[[0 for _ in range(nFilters)] for _ in range(nCols)] for _ in range(nRows)]\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# remainder = [[[None for _ in range(nFilters)] for _ in range(nCols)] for _ in range(nRows)]\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m Input \u001b[38;5;241m=\u001b[39m [[\u001b[43m[\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mj\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m%\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mnChannels\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(nCols)] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(nRows)]\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Weights = [[[str(int(weights[i][j][k].round()) % p) for k in range(weights.shape[2])] for j in range(weights.shape[1])] for i in range(weights.shape[0])]\u001b[39;00m\n\u001b[1;32m     10\u001b[0m Weights \u001b[38;5;241m=\u001b[39m [[\u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mint\u001b[39m(weights[i][j]\u001b[38;5;241m.\u001b[39mround()) \u001b[38;5;241m%\u001b[39m p)\u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(weights\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m])] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(weights\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m])]\n",
      "Cell \u001b[0;32mIn[10], line 8\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      4\u001b[0m outCols \u001b[38;5;241m=\u001b[39m (nCols \u001b[38;5;241m-\u001b[39m kernelSize)\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39mstrides \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# out = [[[0 for _ in range(nFilters)] for _ in range(nCols)] for _ in range(nRows)]\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# remainder = [[[None for _ in range(nFilters)] for _ in range(nCols)] for _ in range(nRows)]\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m Input \u001b[38;5;241m=\u001b[39m [[[\u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mint\u001b[39m(\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mj\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m) \u001b[38;5;241m%\u001b[39m p) \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(nChannels)] \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(nCols)] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(nRows)]\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Weights = [[[str(int(weights[i][j][k].round()) % p) for k in range(weights.shape[2])] for j in range(weights.shape[1])] for i in range(weights.shape[0])]\u001b[39;00m\n\u001b[1;32m     10\u001b[0m Weights \u001b[38;5;241m=\u001b[39m [[\u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mint\u001b[39m(weights[i][j]\u001b[38;5;241m.\u001b[39mround()) \u001b[38;5;241m%\u001b[39m p)\u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(weights\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m])] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(weights\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m])]\n",
      "\u001b[0;31mIndexError\u001b[0m: index 0 is out of bounds for dimension 0 with size 0"
     ]
    }
   ],
   "source": [
    "# circuit_layer_input, pytorch_output = circuit.circuit_layer_inputs(0, pytorch_input, quantized_input)\n",
    "circuit_layer_input, pytorch_output = circuit.circuit_layer_inputs(1, pytorch_output, circuit_layer_input.out())\n",
    "# circuit_depth_input = circuit.circuit_layer_inputs(1, pytorch_input, quantized_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "07e90b1e-be65-4144-8624-97dd10f906e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ZkSeparableConv2d(\n",
       "  (dw_conv): Sequential(\n",
       "    (0): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), groups=8, bias=False)\n",
       "    (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (pw_conv): Sequential(\n",
       "    (0): Conv2d(8, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.features[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a10e35b1-25ed-4aa1-b4e3-eaedfd9d7ae4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "ff636407-3627-4fa9-8b7b-7eeae2a07c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial    SHAPE: 32x32x3\n",
    "# conv       SHAPE: 32x32x8\n",
    "      # (0): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), groups=8, bias=False)\n",
    "      # (0): Conv2d(8, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "# layer0_dw  SHAPE: 30x30x8\n",
    "# layer0_pw  SHAPE: 30x30x16\n",
    "\n",
    "      # (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), groups=16, bias=False)\n",
    "      # (0): Conv2d(16, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "# layer1_dw  SHAPE: 28x28x16\n",
    "# layer1_pw  SHAPE: 28x28x32\n",
    "\n",
    "      # (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), groups=32, bias=False)\n",
    "      # (0): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "# layer2_dw  SHAPE: 26x26x32\n",
    "# layer2_pw  SHAPE: 26x26x32\n",
    "\n",
    "      # (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), groups=32, bias=False)\n",
    "      # (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "# layer3_dw  SHAPE: 24x24x32\n",
    "# layer3_pw  SHAPE: 24x24x64\n",
    "\n",
    "      # (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), groups=64, bias=False)\n",
    "      # (0): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "# layer4_dw  SHAPE: 22x22x64\n",
    "# layer4_pw  SHAPE: 22x22x64\n",
    "\n",
    "      # (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), groups=64, bias=False)\n",
    "      # (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "# layer5_dw  SHAPE: 20x20x64\n",
    "# layer5_pw  SHAPE: 20x20x128\n",
    "\n",
    "      # (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), groups=128, bias=False)\n",
    "      # (0): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "# layer6_dw  SHAPE: 18x18x128\n",
    "# layer6_pw  SHAPE: 18x18x128\n",
    "\n",
    "      # (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), groups=128, bias=False)\n",
    "      # (0): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "# layer7_dw  SHAPE: 16x16x128\n",
    "# layer7_pw  SHAPE: 16x16x128\n",
    "\n",
    "      # (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), groups=128, bias=False)\n",
    "      # (0): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "# layer8_dw  SHAPE: 14x14x128\n",
    "# layer8_pw  SHAPE: 14x14x128\n",
    "\n",
    "      # (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), groups=128, bias=False)\n",
    "      # (0): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "# layer9_dw  SHAPE: 12x12x128\n",
    "# layer9_pw  SHAPE: 12x12x128\n",
    "\n",
    "      # (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), groups=128, bias=False)\n",
    "      # (0): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "# layer10_dw  SHAPE: 10x10x128\n",
    "# layer10_pw  SHAPE: 10x10x128\n",
    "\n",
    "      # (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), groups=128, bias=False)\n",
    "      # (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "# layer11_dw  SHAPE: 8x8x128\n",
    "# layer11_pw  SHAPE: 8x8x256\n",
    "\n",
    "      # (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), groups=256, bias=False)\n",
    "      # (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "# layer12_dw  SHAPE: 6x6x256\n",
    "# layer12_pw  SHAPE: 6x6x256"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
