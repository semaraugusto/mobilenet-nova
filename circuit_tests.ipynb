{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6e30ed7-19bd-495b-9c3b-37b7753f3934",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Manual mobilenet implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69987bda-3e35-44f6-9117-a90513a9513b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from models.mobilenet import ZkMobileNet\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pytorch_lightning as pl\n",
    "import torchvision\n",
    "import torchvision.transforms.v2 as transforms\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1a79f141-4c15-4283-9a09-4825a99028f9",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Circom defines the range of positives are [0, p/2] and the range of negatives are [(p/2)+1, (p-1)].\n",
    "# CIRCOM_PRIME = 21888242871839275222246405745257275088548364400416034343698204186575808495617\n",
    "CIRCOM_PRIME = 28948022309329048855892746252171976963363056481941647379679742748393362948097\n",
    "MAX_POSITIVE = CIRCOM_PRIME // 2\n",
    "MAX_NEGATIVE = MAX_POSITIVE + 1 # The most positive number\n",
    "CIRCOM_NEGATIVE_1 = CIRCOM_PRIME - 1\n",
    "EXPONENT = 15\n",
    "\n",
    "def from_circom(x):\n",
    "    if type(x) != int:\n",
    "        x = int(x)\n",
    "    if x > MAX_POSITIVE: \n",
    "        return x - CIRCOM_PRIME\n",
    "    return x\n",
    "    \n",
    "def to_circom(x):\n",
    "    return x % CIRCOM_PRIME\n",
    "    \n",
    "def to_circom_input(array: np.array): \n",
    "    if type(array) != np.array:\n",
    "        array = np.array(array)\n",
    "    int_array = array.round().astype(int)\n",
    "    int_array = to_circom(int_array)\n",
    "    return int_array.astype(str).tolist()\n",
    "\n",
    "\n",
    "def DepthwiseConv(nRows, nCols, nChannels, nFilters, kernelSize, strides, n, input, weights, bias):\n",
    "    assert(nFilters % nChannels == 0)\n",
    "    outRows = (nRows - kernelSize)//strides + 1\n",
    "    outCols = (nCols - kernelSize)//strides + 1\n",
    "    \n",
    "    # out = np.zeros((outRows, outCols, nFilters))\n",
    "    out = [[[0 for _ in range(nFilters)] for _ in range(outCols)] for _ in range(outRows)]\n",
    "    remainder = [[[0 for _ in range(nFilters)] for _ in range(outCols)] for _ in range(outRows)]\n",
    "    # remainder = np.zeros((outRows, outCols, nFilters))\n",
    "    \n",
    "    for row in range(outRows):\n",
    "        for col in range(outCols):\n",
    "            for channel in range(nChannels):\n",
    "                for x in range(kernelSize):\n",
    "                    for y in range(kernelSize):\n",
    "                        out[row][col][channel] += int(input[row*strides+x, col*strides+y, channel]) * int(weights[x, y, channel])\n",
    "                \n",
    "                out[row][col][channel] += int(bias[channel])\n",
    "                remainder[row][col][channel] = str(int(out[row][col][channel] % n))\n",
    "                out[row][col][channel] = int(out[row][col][channel] // n)\n",
    "                            \n",
    "    return out, remainder\n",
    "\n",
    "def PointwiseConv2d(nRows, nCols, nChannels, nFilters, strides, n, input, weights, bias):\n",
    "    kernelSize = 1\n",
    "    outRows = (nRows - kernelSize)//strides + 1\n",
    "    outCols = (nCols - kernelSize)//strides + 1\n",
    "    out = [[[0 for _ in range(nFilters)] for _ in range(outCols)] for _ in range(outRows)]\n",
    "    str_out = [[[0 for _ in range(nFilters)] for _ in range(outCols)] for _ in range(outRows)]\n",
    "    remainder = [[[None for _ in range(nFilters)] for _ in range(outCols)] for _ in range(outRows)]\n",
    "    for row in range(outRows):\n",
    "        for col in range(outCols):\n",
    "            for filter in range(nFilters):\n",
    "                for k in range(nChannels):\n",
    "                    out[row][col][filter] += int(input[row*strides, col*strides, k]) * int(weights[k, filter])\n",
    "                    \n",
    "                out[row][col][filter] += int(bias[filter])\n",
    "                remainder[row][col][filter] = str(int(out[row][col][filter] % n))\n",
    "                out[row][col][filter] = int(out[row][col][filter] // n)\n",
    "                str_out[row][col][filter] = str(out[row][col][filter] % p)\n",
    "                            \n",
    "    return out, str_out, remainder\n",
    "\n",
    "def SeparableConvImpl(nRows, nCols, nChannels, nDepthFilters, nPointFilters, kernelSize, strides, n, input, depthWeights, pointWeights, depthBias, pointBias):\n",
    "    outRows = (nRows - kernelSize)//strides + 1\n",
    "    outCols = (nCols - kernelSize)//strides + 1\n",
    "\n",
    "    depth_out, depth_remainder = DepthwiseConv(nRows, nCols, nChannels, nDepthFilters, kernelSize, strides, n, input, depthWeights, depthBias)\n",
    "    point_out, point_str_out, point_remainder = PointwiseConv2d(outRows, outCols, nChannels, nPointFilters, strides, n, depth_out, pointWeights, pointBias)\n",
    "    return depth_out, depth_remainder, point_out, point_str_out, point_remainder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "faaa5941-253c-4676-94bd-c4dfbb1e524f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetWrapper(torch.utils.data.Dataset):\n",
    "    def __init__(self, subset, transform=None):\n",
    "        self.subset = subset\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        x, y = self.subset[index]\n",
    "        \n",
    "        if self.transform:\n",
    "            x = self.transform(x)\n",
    "        return x, y\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "622df7cd-d2f1-40bd-b801-27df6c3b0cdf",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/semar/.local/lib/python3.11/site-packages/torchvision/transforms/v2/_deprecated.py:43: UserWarning: The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)])`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(\n",
    "    root='./CIFAR10', train=False, download=True, transform=transform)\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(\n",
    "    root='./CIFAR10', train=True, download=True)\n",
    "\n",
    "# split the train set into train/validation\n",
    "train_set_size = int(len(trainset) * 0.8)\n",
    "valid_set_size = len(trainset) - train_set_size\n",
    "\n",
    "seed = torch.Generator().manual_seed(42)\n",
    "trainset, validset = torch.utils.data.random_split(trainset, [train_set_size, valid_set_size], generator=seed)\n",
    "\n",
    "trainset = DatasetWrapper(trainset, transform)\n",
    "validset = DatasetWrapper(validset, transform)\n",
    "\n",
    "# Create train dataloader\n",
    "trainloader = torch.utils.data.DataLoader(\n",
    "    trainset, batch_size=512, shuffle=True, num_workers=24)\n",
    "\n",
    "# Create validation dataloader\n",
    "validloader = torch.utils.data.DataLoader(\n",
    "    validset, batch_size=512, shuffle=False, num_workers=24)\n",
    "\n",
    "# Create test dataloader\n",
    "testloader = torch.utils.data.DataLoader(\n",
    "    testset, batch_size=512, shuffle=False, num_workers=24)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat', 'deer',\n",
    "           'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0ef5c968-2309-4bcb-a680-9450536ef079",
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted horse - idx: 7\n"
     ]
    }
   ],
   "source": [
    "MODEL_WEIGHTS_PATH = \"./checkpoints/no_padding_100epochs.pth\", \n",
    "\n",
    "model = ZkMobileNet(trainloader, num_classes=10, alpha=0.25, max_epochs=100)\n",
    "\n",
    "saved = torch.load(\"./checkpoints/no_padding_100epochs.pth\")\n",
    "# model.load_state_dict(saved['state_dict'])\n",
    "model.load_state_dict(saved['net'])\n",
    "model.eval()\n",
    "\n",
    "image, label = validset[0]\n",
    "image = image.unsqueeze(0)\n",
    "logits = model(image)\n",
    "pred_idx = logits.argmax()\n",
    "\n",
    "print(f\"Predicted {classes[pred_idx]} - idx: {pred_idx}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "da689ff6-ad5a-421b-b388-bf0b26467475",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Conv2DInt(nRows, nCols, nChannels, nFilters, kernelSize, strides, n, input, weights, bias):\n",
    "    p = CIRCOM_PRIME\n",
    "    Input = [[[str(int(input[i][j][k]) % p) for k in range(nChannels)] for j in range(nCols)] for i in range(nRows)]\n",
    "    Weights = [[[[str(int(weights[i][j][k][l]) % p) for l in range(nFilters)] for k in range(nChannels)] for j in range(kernelSize)] for i in range(kernelSize)]\n",
    "    Bias = [str(int(bias[i]) % p) for i in range(nFilters)]\n",
    "    out = [[[0 for _ in range(nFilters)] for _ in range((nCols - kernelSize)//strides + 1)] for _ in range((nRows - kernelSize)//strides + 1)]\n",
    "    remainder = [[[None for _ in range(nFilters)] for _ in range((nCols - kernelSize)//strides + 1)] for _ in range((nRows - kernelSize)//strides + 1)]\n",
    "    for i in range((nRows - kernelSize)//strides + 1):\n",
    "        for j in range((nCols - kernelSize)//strides + 1):\n",
    "            for m in range(nFilters):\n",
    "                for k in range(nChannels):\n",
    "                    for x in range(kernelSize):\n",
    "                        for y in range(kernelSize):\n",
    "                            out[i][j][m] += int(input[i*strides+x][j*strides+y][k]) * int(weights[x][y][k][m])\n",
    "                out[i][j][m] += int(bias[m])\n",
    "                remainder[i][j][m] = str(int(out[i][j][m]) % n)\n",
    "                out[i][j][m] = str(int(out[i][j][m]) // n % p)\n",
    "    return Input, Weights, Bias, out, remainder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "247d5284-3c4f-4fd3-9781-43b47c592365",
   "metadata": {},
   "outputs": [],
   "source": [
    "def BatchNormalizationInt(nRows, nCols, nChannels, n, X_in, a_in, b_in):\n",
    "    p = CIRCOM_PRIME\n",
    "    X = [[[str(int(X_in[i][j][k]) % p) for k in range(nChannels)] for j in range(nCols)] for i in range(nRows)]\n",
    "    A = [str(int(a_in[k]) % p) for k in range(nChannels)]\n",
    "    B = [str(int(b_in[k]) % p) for k in range(nChannels)]\n",
    "    out = [[[0 for _ in range(nChannels)] for _ in range(nCols)] for _ in range(nRows)]\n",
    "    remainder = [[[None for _ in range(nChannels)] for _ in range(nCols)] for _ in range(nRows)]\n",
    "    for i in range(nRows):\n",
    "        for j in range(nCols):\n",
    "            for k in range(nChannels):\n",
    "                out[i][j][k] = int(int(X_in[i][j][k]) * int(a_in[k]) + int(b_in[k]))\n",
    "                remainder[i][j][k] = str(int(out[i][j][k]) % n)\n",
    "                out[i][j][k] = int(out[i][j][k] // n)\n",
    "    return X, A, B, out, remainder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "56e21b0a-fbb2-46db-80c1-5d8f57518c95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out.shape=torch.Size([1, 8, 32, 32])\n",
      "expected.shape=torch.Size([32, 32, 8])\n",
      "torch.Size([1, 8, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "gamma = model.bn.weight\n",
    "beta = model.bn.bias\n",
    "mean = model.bn.running_mean\n",
    "var = model.bn.running_var\n",
    "eps = model.bn.eps\n",
    "\n",
    "a = (gamma/(var+eps)**.5).detach()\n",
    "b = (beta-gamma*mean/(var+eps)**.5).detach().tolist()\n",
    "\n",
    "image, label = testset[0]\n",
    "image = image.unsqueeze(0)\n",
    "out = model.conv(image)\n",
    "expected = model.bn(out)\n",
    "# out = out.squeeze()\n",
    "print(f\"{out.shape=}\")\n",
    "\n",
    "expected = torch.permute(expected.squeeze(), (1, 2, 0))\n",
    "\n",
    "print(f\"{expected.shape=}\")\n",
    "\n",
    "quantized_in = torch.permute(out.squeeze(), (1, 2, 0)) * 10**EXPONENT\n",
    "quantized_a = (a * 10**EXPONENT).tolist()\n",
    "quantized_b = [bi * 10**(2*EXPONENT) for bi in b]\n",
    "print(out.shape)\n",
    "\n",
    "X, A, B, actual, remainder = BatchNormalizationInt(32, 32, 8, 10**EXPONENT, quantized_in, quantized_a, quantized_b)\n",
    "test_output = [[[from_circom(int(out)) / 10**EXPONENT for out in asdf] for asdf in asdfasdf] for asdfasdf in actual]\n",
    "\n",
    "assert(torch.allclose(torch.Tensor(test_output), expected, atol=1e-6))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea13b7cc-41ef-4127-b1fd-99ffcc4723b8",
   "metadata": {},
   "source": [
    "# Testing head layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fbe4fd19-94b7-4b11-9dac-e308c523b7a8",
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "something\n",
      "before conv2d\n",
      "after conv2d\n",
      "after bn\n",
      "at:  0 1 6\n",
      "in:  1745789835369983\n",
      "after relu\n",
      "end\n",
      "ok\n"
     ]
    }
   ],
   "source": [
    "# CONVOLUTION LAYER\n",
    "weights = model.conv.weight.detach().numpy().transpose(2, 3, 1, 0)\n",
    "image, label = testset[0]\n",
    "\n",
    "expected = model.conv(image).detach().numpy()\n",
    "bias = torch.zeros(weights.shape[3]).numpy()\n",
    "\n",
    "# padded = pad(image, 1).transpose(1,2,0)\n",
    "padded = F.pad(image, (1,1,1,1), \"constant\", 0).numpy()\n",
    "padded = padded.transpose(1,2,0)\n",
    "\n",
    "quantized_image = (padded * 10**EXPONENT).round()\n",
    "quantized_weights = (weights * 10**EXPONENT).round() # .transpose(0, 3, 1, 0) # [nFilters, nChannels, H, W] -> \n",
    "\n",
    "circuit_in, circuit_conv_weights, circuit_conv_bias, circuit_conv_out, circuit_conv_remainder = Conv2DInt(34, 34, 3, 8, 3, 1, 10**EXPONENT, quantized_image, quantized_weights, bias)\n",
    "# output, remainder = Conv2DInt(34, 34, 3, 8, 3, 1, 10**EXPONENT, quantized_image, quantized_weights, bias)\n",
    "# test_output = output / 10**(EXPONENT)\n",
    "test_output = [[[from_circom(int(out)) / 10**EXPONENT for out in asdf] for asdf in asdfasdf] for asdfasdf in circuit_conv_out]\n",
    "\n",
    "expected = expected.transpose((1, 2, 0))\n",
    "\n",
    "assert(np.allclose(test_output, expected, atol=1e-6))\n",
    "\n",
    "# BATCH NORM CONSTANTS\n",
    "gamma = model.bn.weight\n",
    "beta = model.bn.bias\n",
    "mean = model.bn.running_mean\n",
    "var = model.bn.running_var\n",
    "eps = model.bn.eps\n",
    "\n",
    "a = (gamma/(var+eps)**.5).detach()\n",
    "b = (beta-gamma*mean/(var+eps)**.5).detach().tolist()\n",
    "\n",
    "quantized_a = (a * 10**EXPONENT).tolist()\n",
    "quantized_b = [bi * 10**(2*EXPONENT) for bi in b]\n",
    "\n",
    "# BATCH NORM USING PYTORCH OUTPUT\n",
    "image, label = testset[0]\n",
    "image = image.unsqueeze(0)\n",
    "out = model.conv(image)\n",
    "expected = model.bn(out)\n",
    "\n",
    "expected = torch.permute(expected.squeeze(), (1, 2, 0))\n",
    "\n",
    "quantized_in = torch.permute(out.squeeze(), (1, 2, 0)) * 10**EXPONENT\n",
    "\n",
    "X, A, B, actual, remainder = BatchNormalizationInt(32, 32, 8, 10**EXPONENT, quantized_in, quantized_a, quantized_b)\n",
    "test_output = [[[from_circom(int(out)) / 10**EXPONENT for out in vec] for vec in matrix] for matrix in actual]\n",
    "assert(torch.allclose(torch.Tensor(test_output), expected, atol=1e-6))\n",
    "\n",
    "# BATCH NORM USING CIRCUIT CONV OUTPUT\n",
    "quantized_in = [[[from_circom(int(out)) for out in vec] for vec in matrix] for matrix in circuit_conv_out]\n",
    "\n",
    "_, circuit_bn_a, circuit_bn_b, circuit_bn_out, circuit_bn_remainder = BatchNormalizationInt(32, 32, 8, 10**EXPONENT, quantized_in, quantized_a, quantized_b)\n",
    "\n",
    "test_output = [[[int(from_circom(int(out))) / 10**EXPONENT for out in vec] for vec in matrix] for matrix in actual]\n",
    "\n",
    "assert(torch.allclose(torch.Tensor(test_output), expected, atol=1e-6))\n",
    "\n",
    "# RELU USING CIRCUIT OUTPUT\n",
    "relu_in = [[[to_circom(bn_out) if bn_out < MAX_POSITIVE else 0 for bn_out in vec] for vec in matrix] for matrix in circuit_bn_out]\n",
    "relu_out = [[[str(bn_out) if bn_out < MAX_POSITIVE else 0 for bn_out in vec] for vec in matrix] for matrix in relu_in]\n",
    "\n",
    "input_json_path = \"head_input1.json\"\n",
    "with open(input_json_path, \"w\") as input_file:\n",
    "    json.dump({\"in\": circuit_in,\n",
    "               \"conv2d_weights\": circuit_conv_weights,\n",
    "               \"conv2d_bias\": circuit_conv_bias,\n",
    "               \"conv2d_out\": circuit_conv_out,\n",
    "               \"conv2d_remainder\": circuit_conv_remainder,\n",
    "               \n",
    "               \"bn_a\": circuit_bn_a,\n",
    "               \"bn_b\": circuit_bn_b,\n",
    "               \"bn_out\": circuit_bn_out,\n",
    "               \"bn_remainder\": circuit_bn_remainder,\n",
    "               \n",
    "               \"relu_out\": relu_out,\n",
    "               }, input_file)\n",
    "\n",
    "os.chdir(\"circuits\")\n",
    "!./head/head_cpp/head ../head_input1.json head.wtns\n",
    "# !npx snarkjs groth16 prove head/circuit_final.zkey head.wtns proof.json public_test.json\n",
    "os.chdir(\"../\")\n",
    "\n",
    "print(\"ok\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f410f561-1bc2-4fc4-94cb-cf4c1d4ed258",
   "metadata": {},
   "source": [
    "# Testing padding over highly padded input (to try and fold the circuit using nova)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ecdd2b74-e553-4f68-b755-7f50d91a58a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SeparableConv2d(nn.Module):\n",
    "    '''Separable convolution'''\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super(SeparableConv2d, self).__init__()\n",
    "        self.dw_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, in_channels, kernel_size=3, stride=stride, padding=0, groups=in_channels, bias=False),\n",
    "            nn.BatchNorm2d(in_channels),\n",
    "            # nn.ReLU(inplace=False),\n",
    "        )\n",
    "        self.pw_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            # nn.ReLU(inplace=False),\n",
    "        )\n",
    "        \n",
    "input = torch.randn((1, 3, 5, 5))\n",
    "test_model = SeparableConv2d(3, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2288f8d3-b0d8-4954-9d15-b7ca41f5b065",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 3, 3, 3)\n",
      "test_output shape:  (3, 3, 3)\n",
      "expected shape:  (3, 3, 3)\n"
     ]
    }
   ],
   "source": [
    "weights = test_model.dw_conv[0].weight.squeeze().detach().numpy()\n",
    "bias = torch.zeros(weights.shape[0]).numpy()\n",
    "\n",
    "expected = test_model.dw_conv[0](input).detach().numpy()\n",
    "print(expected.shape)\n",
    "\n",
    "padded = F.pad(input, (1,1,1,1), \"constant\", 0)\n",
    "padded = padded.squeeze().numpy().transpose((1, 2, 0))\n",
    "weights = weights.transpose((1, 2, 0))\n",
    "\n",
    "quantized_image = padded * 10**EXPONENT\n",
    "quantized_weights = weights * 10**EXPONENT\n",
    "\n",
    "actual, rem = DepthwiseConv(7, 7, 3, 3, 3, 1, 10**EXPONENT, quantized_image.round(), quantized_weights.round(), bias)\n",
    "\n",
    "test_output = np.array([[[int(out) / 10**EXPONENT for out in asdf] for asdf in asdfasdf] for asdfasdf in actual])\n",
    "test_output = test_output[1:4, 1:4, :]\n",
    "expected = expected.squeeze().transpose((1, 2, 0))\n",
    "# expected = expected * 10**EXPONENT\n",
    "\n",
    "print(\"test_output shape: \", test_output.shape)\n",
    "print(\"expected shape: \", expected.shape)\n",
    "assert(np.allclose(expected, test_output, atol=0.00001))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "abef6b48-7498-40f8-ace1-84603933c80c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input = torch.randn((1, 3, 5, 5))\n",
    "# model = SeparableConv2d(3, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "811eec43-c91a-4750-99a9-21b0789928cc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 3, 3, 3)\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "def DepthwiseConv(nRows, nCols, nChannels, nFilters, kernelSize, strides, n, input, weights, bias):\n",
    "    assert(nFilters % nChannels == 0)\n",
    "    outRows = (nRows - kernelSize)//strides + 1\n",
    "    outCols = (nCols - kernelSize)//strides + 1\n",
    "    \n",
    "    # out = np.zeros((outRows, outCols, nFilters))\n",
    "    out = [[[0 for _ in range(nFilters)] for _ in range(outCols)] for _ in range(outRows)]\n",
    "    remainder = [[[0 for _ in range(nFilters)] for _ in range(outCols)] for _ in range(outRows)]\n",
    "    # remainder = np.zeros((outRows, outCols, nFilters))\n",
    "    \n",
    "    for row in range(outRows):\n",
    "        for col in range(outCols):\n",
    "            for channel in range(nChannels):\n",
    "                for x in range(kernelSize):\n",
    "                    for y in range(kernelSize):\n",
    "                        out[row][col][channel] += int(input[row*strides+x, col*strides+y, channel]) * int(weights[x, y, channel])\n",
    "                \n",
    "                out[row][col][channel] += int(bias[channel])\n",
    "                remainder[row][col][channel] = str(int(out[row][col][channel] % n))\n",
    "                out[row][col][channel] = int(out[row][col][channel] // n)\n",
    "                            \n",
    "    return out, remainder\n",
    "    \n",
    "weights = test_model.dw_conv[0].weight.squeeze().detach().numpy()\n",
    "bias = torch.zeros(weights.shape[0]).numpy()\n",
    "\n",
    "expected = test_model.dw_conv[0](input).detach().numpy()\n",
    "print(expected.shape)\n",
    "\n",
    "padded = F.pad(input, (1,1,1,1), \"constant\", 0)\n",
    "padded = padded.squeeze().numpy().transpose((1, 2, 0))\n",
    "weights = weights.transpose((1, 2, 0))\n",
    "\n",
    "quantized_image = padded * 10**EXPONENT\n",
    "quantized_weights = weights * 10**EXPONENT\n",
    "\n",
    "actual, rem = DepthwiseConv(7, 7, 3, 3, 3, 1, 10**EXPONENT, quantized_image.round(), quantized_weights.round(), bias)\n",
    "\n",
    "# test_output = np.array([[[int(out) / 10**EXPONENT for out in asdf] for asdf in asdfasdf] for asdfasdf in actual])\n",
    "# test_output = test_output[1:6, 1:6, :]\n",
    "test_output = np.array([[[int(out) / 10**EXPONENT for out in asdf] for asdf in asdfasdf] for asdfasdf in actual])\n",
    "test_output = test_output[1:4, 1:4, :]\n",
    "\n",
    "expected = expected.squeeze().transpose((1, 2, 0))\n",
    "# expected = expected * 10**EXPONENT\n",
    "assert(np.allclose(expected, test_output, atol=0.00001))\n",
    "\n",
    "circuit_in = quantized_image.round().astype(int).astype(str).tolist()\n",
    "# circuit_weights = quantized_weights.round().astype(int).astype(str).tolist()\n",
    "circuit_bias = bias.round().astype(int).astype(str).tolist()\n",
    "\n",
    "p = CIRCOM_PRIME\n",
    "Input = [[[str(int(quantized_image[i][j][k].round()) % p) for k in range(quantized_image.shape[2])] for j in range(quantized_image.shape[1])] for i in range(quantized_image.shape[0])]\n",
    "Weights = [[[str(int(quantized_weights[i][j][k].round()) % p) for k in range(quantized_weights.shape[2])] for j in range(quantized_weights.shape[1])] for i in range(quantized_weights.shape[0])]\n",
    "# Out = [[[str(int(actual[i][j][k]) % p) for k in range(3)] for j in range(7)] for i in range(7)]\n",
    "\n",
    "test_output = np.array([[[int(out) for out in asdf] for asdf in asdfasdf] for asdfasdf in actual])\n",
    "# test_output = test_output[1:6, 1:6, :]\n",
    "Out = [[[str(int(test_output[i][j][k]) % p) for k in range(3)] for j in range(5)] for i in range(5)]\n",
    "Rem = [[[str(rem[i][j][k]) for k in range(3)] for j in range(5)] for i in range(5)]\n",
    "\n",
    "input_json_path = \"backbone_input.json\"\n",
    "with open(input_json_path, \"w\") as input_file:\n",
    "    json.dump({\n",
    "               \"in\": Input,\n",
    "               \"weights\": Weights,\n",
    "               \"bias\": circuit_bias,\n",
    "               \"remainder\": Rem,\n",
    "               \"out\": Out,\n",
    "              },\n",
    "              input_file)\n",
    "\n",
    "\n",
    "# os.chdir(\"circuits\")\n",
    "# !./origDepthwiseConv2d/origDepthwiseConv2d_cpp/origDepthwiseConv2d ../backbone_input.json head.wtns\n",
    "# !npx snarkjs groth16 prove ./origDepthwiseConv2d/circuit_final.zkey head.wtns proof.json public_test.json\n",
    "# os.chdir(\"../\")\n",
    "\n",
    "print(\"OK\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f933a59-1605-4a64-ac94-f4e7107a79e7",
   "metadata": {},
   "source": [
    "# Padded Convolution test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c6fd691-e729-404e-a858-2d3e45e26846",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b8a3222d-4eee-4060-b336-07da6efa1ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "input = torch.randn((1, 3, 7, 7))\n",
    "# model = SeparableConv2d(3, 6)\n",
    "# model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1dd45bb1-31a1-42e1-80a8-a7ac0452b45b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def PointwiseConv2d(nRows, nCols, nChannels, nFilters, strides, n, input, weights, bias):\n",
    "    kernelSize = 1\n",
    "    outRows = (nRows - kernelSize)//strides + 1\n",
    "    outCols = (nCols - kernelSize)//strides + 1\n",
    "    out = [[[0 for _ in range(nFilters)] for _ in range(nCols)] for _ in range(nRows)]\n",
    "    remainder = [[[None for _ in range(nFilters)] for _ in range(nCols)] for _ in range(nRows)]\n",
    "    for row in range(outRows):\n",
    "        for col in range(outCols):\n",
    "            for filter in range(nFilters):\n",
    "                for k in range(nChannels):\n",
    "                    out[row][col][filter] += int(input[row*strides][col*strides][k]) * int(weights[k, filter])\n",
    "                    \n",
    "                out[row][col][filter] += int(bias[filter])\n",
    "                remainder[row][col][filter] = str(int(out[row][col][filter] % n))\n",
    "                out[row][col][filter] = int(out[row][col][filter] // n)\n",
    "                            \n",
    "    return out, remainder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b70bb0a6-f586-4719-a200-501ad9bb708d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape:  torch.Size([1, 3, 7, 7])\n",
      "Expected shape:  torch.Size([1, 3, 5, 5])\n",
      "Expected shape:  (5, 5, 3)\n",
      "test_output shape:  (5, 5, 3)\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 54\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_output shape: \u001b[39m\u001b[38;5;124m\"\u001b[39m, test_output\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m     53\u001b[0m \u001b[38;5;66;03m# expected = expected * 10**EXPONENT\u001b[39;00m\n\u001b[0;32m---> 54\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m(np\u001b[38;5;241m.\u001b[39mallclose(expected, test_output, atol\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.00001\u001b[39m))\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--------------------------------\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput shape: \u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mshape)\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# def PaddedDepthwiseConv(nRows, nCols, nChannels, nFilters, kernelSize, strides, n, input, weights, bias):\n",
    "#     assert(nFilters % nChannels == 0)\n",
    "#     outRows = (nRows - kernelSize)//strides + 1\n",
    "#     outCols = (nCols - kernelSize)//strides + 1\n",
    "    \n",
    "#     # out = np.zeros((outRows, outCols, nFilters))\n",
    "#     Input = [[[str(int(input[i][j][k]) % p) for k in range(nChannels)] for j in range(nCols)] for i in range(nRows)]\n",
    "#     Weights = [[[str(int(weights[i][j][k].round()) % p) for k in range(weights.shape[2])] for j in range(weights.shape[1])] for i in range(weights.shape[0])]\n",
    "#     out = [[[0 for _ in range(nFilters)] for _ in range(nCols)] for _ in range(nRows)]\n",
    "#     Bias = bias.round().astype(int).astype(str).tolist()\n",
    "#     out_str = [[[str(0) for _ in range(nFilters)] for _ in range(nCols)] for _ in range(nRows)]\n",
    "#     remainder = [[[str(0) for _ in range(nFilters)] for _ in range(nCols)] for _ in range(nRows)]\n",
    "    \n",
    "#     for row in range(outRows):\n",
    "#         for col in range(outCols):\n",
    "#             for channel in range(nChannels):\n",
    "#                 for x in range(kernelSize):\n",
    "#                     for y in range(kernelSize):\n",
    "#                         out[row+1][col+1][channel] += int(input[row*strides+x][col*strides+y][channel]) * int(weights[x, y, channel])\n",
    "                \n",
    "#                 out[row+1][col+1][channel] += int(bias[channel])\n",
    "#                 remainder[row+1][col+1][channel] = str(int(out[row+1][col+1][channel] % n))\n",
    "#                 out[row+1][col+1][channel] = int(out[row+1][col+1][channel] // n)\n",
    "#                 out_str[row+1][col+1][channel] = str(out[row+1][col+1][channel] % p)\n",
    "                            \n",
    "#     return out, remainder\n",
    "    \n",
    "weights = test_model.dw_conv[0].weight.squeeze().detach().numpy()\n",
    "bias = torch.zeros(weights.shape[0]).numpy()\n",
    "\n",
    "print(\"Input shape: \", input.shape)\n",
    "expected = test_model.dw_conv(input).detach()\n",
    "print(\"Expected shape: \", expected.shape)\n",
    "\n",
    "weights = weights.transpose((1, 2, 0))\n",
    "\n",
    "quantized_image = input.squeeze().numpy().transpose((1,2,0)) * 10**EXPONENT\n",
    "# quantized_image = padded * 10**EXPONENT\n",
    "quantized_weights = weights * 10**EXPONENT\n",
    "\n",
    "# actual, rem = PaddedDepthwiseConv(7, 7, 3, 3, 3, 1, 10**EXPONENT, quantized_image.round(), quantized_weights.round(), bias)\n",
    "circuit_in, circuit_depth_weights, circuit_depth_bias, circuit_depth_out, depth_out, circuit_depth_remainder = PaddedDepthwiseConv(7, 7, 3, 3, 3, 1, 10**EXPONENT, quantized_image.round(), quantized_weights.round(), bias)\n",
    "\n",
    "test_output = np.array([[[int(value) / 10**EXPONENT for value in vec] for vec in matrix] for matrix in depth_out])\n",
    "test_output = test_output[1:6, 1:6, :]\n",
    "\n",
    "# expected = expected.squeeze().transpose((1, 2, 0))\n",
    "# expected = F.pad(expected.squeeze(), (1,1,1,1)).numpy().transpose((1,2,0))\n",
    "expected = expected.squeeze().numpy().transpose((1,2,0))\n",
    "print(\"Expected shape: \", expected.shape)\n",
    "print(\"test_output shape: \", test_output.shape)\n",
    "\n",
    "# expected = expected * 10**EXPONENT\n",
    "assert(np.allclose(expected, test_output, atol=0.00001))\n",
    "print(\"--------------------------------\")\n",
    "\n",
    "print(\"Input shape: \", input.shape)\n",
    "expected = test_model.dw_conv(input).detach()\n",
    "print(\"expected shape: \", expected.shape)\n",
    "expected = test_model.dw_conv(expected).detach()\n",
    "expected = expected.squeeze().numpy().transpose((1,2,0))\n",
    "print(\"expected shape: \", expected.shape)\n",
    "dw_input, rem = PaddedDepthwiseConv(7, 7, 3, 3, 3, 1, 10**EXPONENT, quantized_image.round(), quantized_weights.round(), bias)\n",
    "print(\"1st shape: \", np.array(dw_input).shape)\n",
    "actual, rem = PaddedDepthwiseConv(7, 7, 3, 3, 3, 1, 10**EXPONENT, dw_input, quantized_weights.round(), bias)\n",
    "print(\"2nd shape: \", np.array(actual).shape)\n",
    "test_output = np.array([[[int(out) / 10**EXPONENT for out in asdf] for asdf in asdfasdf] for asdfasdf in actual])\n",
    "test_output = test_output[2:5, 2:5, :]\n",
    "\n",
    "\n",
    "# expected = F.pad(expected.squeeze(), (2,2,2,2)).numpy().transpose((1,2,0))\n",
    "print(\"actual shape: \", np.array(actual).shape)\n",
    "print(\"expected shape: \", np.array(expected).shape)\n",
    "assert(np.allclose(expected, test_output, atol=0.00001))\n",
    "\n",
    "p = CIRCOM_PRIME\n",
    "# quantized_image = np.array(dw_input)\n",
    "# Input = [[[str(int(dw_input[i][j][k]) % p) for k in range(quantized_image.shape[2])] for j in range(quantized_image.shape[1])] for i in range(quantized_image.shape[0])]\n",
    "# Weights = [[[str(int(quantized_weights[i][j][k].round()) % p) for k in range(quantized_weights.shape[2])] for j in range(quantized_weights.shape[1])] for i in range(quantized_weights.shape[0])]\n",
    "# # Out = [[[str(int(actual[i][j][k]) % p) for k in range(3)] for j in range(7)] for i in range(7)]\n",
    "\n",
    "# test_output = np.array([[[int(out) for out in asdf] for asdf in asdfasdf] for asdfasdf in actual])\n",
    "# print(\"test_output shape: \", np.array(test_output).shape)\n",
    "# # # test_output = test_output[1:6, 1:6, :]\n",
    "# Out = [[[str(int(test_output[i][j][k]) % p) for k in range(test_output.shape[2])] for j in range(test_output.shape[1])] for i in range(test_output.shape[0])]\n",
    "# Rem = [[[str(rem[i][j][k]) for k in range(test_output.shape[2])] for j in range(test_output.shape[1])] for i in range(test_output.shape[0])]\n",
    "# print(\"OUT SHAPE: \", np.array(Out).shape)\n",
    "# print(\"REM SHAPE: \", np.array(Rem).shape)\n",
    "\n",
    "# input_json_path = \"padded_input.json\"\n",
    "# with open(input_json_path, \"w\") as input_file:\n",
    "#     json.dump({\n",
    "#                \"in\": Input,\n",
    "#                \"dw_conv_weights\": Weights,\n",
    "#                \"dw_conv_bias\": circuit_bias,\n",
    "#                \"dw_conv_remainder\": Rem,\n",
    "#                \"dw_conv_out\": Out,\n",
    "#               },\n",
    "#               input_file)\n",
    "\n",
    "\n",
    "# os.chdir(\"circuits\")\n",
    "# !./padded/padded_cpp/padded ../padded_input.json head.wtns\n",
    "# # # !npx snarkjs groth16 prove ./origDepthwiseConv2d/circuit_final.zkey head.wtns proof.json public_test.json\n",
    "# os.chdir(\"../\")\n",
    "\n",
    "# # print(\"OK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "48256a72-45ff-42bd-a29a-0aec9d0faa5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def PaddedDepthwiseConv(nRows, nCols, nChannels, nFilters, kernelSize, strides, n, input, weights, bias):\n",
    "    assert(nFilters % nChannels == 0)\n",
    "    outRows = (nRows - kernelSize)//strides + 1\n",
    "    outCols = (nCols - kernelSize)//strides + 1\n",
    "    \n",
    "    # out = np.zeros((outRows, outCols, nFilters))\n",
    "    Input = [[[str(int(input[i][j][k]) % p) for k in range(nChannels)] for j in range(nCols)] for i in range(nRows)]\n",
    "    Weights = [[[str(int(weights[i][j][k].round()) % p) for k in range(weights.shape[2])] for j in range(weights.shape[1])] for i in range(weights.shape[0])]\n",
    "    out = [[[0 for _ in range(nFilters)] for _ in range(nCols)] for _ in range(nRows)]\n",
    "    Bias = bias.round().astype(int).astype(str).tolist()\n",
    "    out_str = [[[str(0) for _ in range(nFilters)] for _ in range(nCols)] for _ in range(nRows)]\n",
    "    remainder = [[[str(0) for _ in range(nFilters)] for _ in range(nCols)] for _ in range(nRows)]\n",
    "    \n",
    "    for row in range(outRows):\n",
    "        for col in range(outCols):\n",
    "            for channel in range(nChannels):\n",
    "                for x in range(kernelSize):\n",
    "                    for y in range(kernelSize):\n",
    "                        out[row+1][col+1][channel] += int(input[row*strides+x][col*strides+y][channel]) * int(weights[x, y, channel])\n",
    "                \n",
    "                out[row+1][col+1][channel] += int(bias[channel])\n",
    "                remainder[row+1][col+1][channel] = str(int(out[row+1][col+1][channel] % n))\n",
    "                out[row+1][col+1][channel] = int(out[row+1][col+1][channel] // n)\n",
    "                out_str[row+1][col+1][channel] = str(out[row+1][col+1][channel] % p)\n",
    "                            \n",
    "    return Input, Weights, Bias, out_str, out, remainder\n",
    "    \n",
    "# def MobilenetDepthwiseConv(nRows, nCols, nChannels, nFilters, n, input, weights, bias):\n",
    "#     kernelSize = 3\n",
    "#     strides = 1\n",
    "    \n",
    "#     assert(nFilters % nChannels == 0)\n",
    "#     outRows = (nRows - kernelSize)//strides + 1\n",
    "#     outCols = (nCols - kernelSize)//strides + 1\n",
    "    \n",
    "#     # out = np.zeros((outRows, outCols, nFilters))\n",
    "#     Input = [[[str(int(input[i][j][k]) % p) for k in range(nChannels)] for j in range(nCols)] for i in range(nRows)]\n",
    "#     Weights = [[[str(int(weights[i][j][k].round()) % p) for k in range(weights.shape[2])] for j in range(weights.shape[1])] for i in range(weights.shape[0])]\n",
    "#     out = [[[0 for _ in range(nFilters)] for _ in range(nCols)] for _ in range(nRows)]\n",
    "#     Bias = bias.round().astype(int).astype(str).tolist()\n",
    "#     out_str = [[[str(0) for _ in range(nFilters)] for _ in range(nCols)] for _ in range(nRows)]\n",
    "#     remainder = [[[str(0) for _ in range(nFilters)] for _ in range(nCols)] for _ in range(nRows)]\n",
    "    \n",
    "#     for row in range(outRows):\n",
    "#         for col in range(outCols):\n",
    "#             for channel in range(nChannels):\n",
    "#                 for x in range(kernelSize):\n",
    "#                     for y in range(kernelSize):\n",
    "#                         out[row+1][col+1][channel] += int(input[row*strides+x][col*strides+y][channel]) * int(weights[x, y, channel])\n",
    "                \n",
    "#                 out[row+1][col+1][channel] += int(bias[channel])\n",
    "#                 remainder[row+1][col+1][channel] = str(int(out[row+1][col+1][channel] % n))\n",
    "#                 out[row+1][col+1][channel] = int(out[row+1][col+1][channel] // n)\n",
    "#                 out_str[row+1][col+1][channel] = str(out[row+1][col+1][channel] % p)\n",
    "                            \n",
    "#     return Input, Weights, Bias, out_str, out, remainder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "bcc591b9-c270-4a6d-a1bd-ac926f5b8641",
   "metadata": {},
   "outputs": [],
   "source": [
    "def PointwiseConv2d(nRows, nCols, nChannels, nFilters, strides, n, input, weights, bias):\n",
    "    kernelSize = 1\n",
    "    outRows = (nRows - kernelSize)//strides + 1\n",
    "    outCols = (nCols - kernelSize)//strides + 1\n",
    "    # out = [[[0 for _ in range(nFilters)] for _ in range(nCols)] for _ in range(nRows)]\n",
    "    # remainder = [[[None for _ in range(nFilters)] for _ in range(nCols)] for _ in range(nRows)]\n",
    "    \n",
    "    Input = [[[str(int(input[i][j][k]) % p) for k in range(nChannels)] for j in range(nCols)] for i in range(nRows)]\n",
    "    # Weights = [[[str(int(weights[i][j][k].round()) % p) for k in range(weights.shape[2])] for j in range(weights.shape[1])] for i in range(weights.shape[0])]\n",
    "    Weights = [[str(int(weights[i][j].round()) % p)for j in range(weights.shape[1])] for i in range(weights.shape[0])]\n",
    "    out = [[[0 for _ in range(nFilters)] for _ in range(nCols)] for _ in range(nRows)]\n",
    "    Bias = bias.round().astype(int).astype(str).tolist()\n",
    "    out_str = [[[str(0) for _ in range(nFilters)] for _ in range(nCols)] for _ in range(nRows)]\n",
    "    remainder = [[[str(0) for _ in range(nFilters)] for _ in range(nCols)] for _ in range(nRows)]\n",
    "    \n",
    "    for row in range(outRows):\n",
    "        for col in range(outCols):\n",
    "            for filter in range(nFilters):\n",
    "                for k in range(nChannels):\n",
    "                    out[row][col][filter] += int(input[row*strides][col*strides][k]) * int(weights[k, filter])\n",
    "                    \n",
    "                out[row][col][filter] += int(bias[filter])\n",
    "                remainder[row][col][filter] = str(int(out[row][col][filter] % n))\n",
    "                out[row][col][filter] = int(out[row][col][filter] // n)\n",
    "                out_str[row][col][filter] = str(out[row][col][filter] % p)\n",
    "                            \n",
    "    # return out, remainder\n",
    "    return Input, Weights, Bias, out_str, out, remainder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8bcc73b0-e18d-403c-a05a-664406df0b99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape:  torch.Size([1, 3, 7, 7])\n",
      "Expected shape:  torch.Size([1, 3, 5, 5])\n",
      "point weights shape:  (6, 3, 1, 1)\n",
      "Input shape:  torch.Size([1, 3, 7, 7])\n",
      "Depth Expected shape:  torch.Size([1, 3, 5, 5])\n",
      "Point Expected shape:  (5, 5, 6)\n",
      "point weights shape:  (3, 6)\n",
      "Not all inputs have been set. Only 1083 out of 1983\n",
      "padded: main.cpp:268: int main(int, char**): Assertion `false' failed.\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "# depth_weights = test_model.dw_conv[0].weight.squeeze().detach().numpy()\n",
    "# depth_bias = torch.zeros(depth_weights.shape[0]).numpy()\n",
    "\n",
    "# print(\"Input shape: \", input.shape)\n",
    "# expected = test_model.dw_conv[0](input).detach()\n",
    "# print(\"Expected shape: \", expected.shape)\n",
    "\n",
    "# depth_weights = depth_weights.transpose((1, 2, 0))\n",
    "\n",
    "# quantized_image = input.squeeze().numpy().transpose((1,2,0)) * 10**EXPONENT\n",
    "# # quantized_image = padded * 10**EXPONENT\n",
    "# quantized_depth_weights = depth_weights * 10**EXPONENT\n",
    "\n",
    "# circuit_in, circuit_depth_weights, circuit_depth_bias, circuit_depth_out, depth_out, circuit_depth_remainder = PaddedDepthwiseConv(7, 7, 3, 3, 3, 1, 10**EXPONENT, quantized_image.round(), quantized_depth_weights.round(), depth_bias)\n",
    "\n",
    "# test_output = np.array([[[int(out) / 10**EXPONENT for out in asdf] for asdf in asdfasdf] for asdfasdf in depth_out])\n",
    "# test_output = test_output[1:6, 1:6, :]\n",
    "\n",
    "# expected = expected.squeeze().numpy().transpose((1,2,0))\n",
    "\n",
    "# assert(np.allclose(expected, test_output, atol=0.00001))\n",
    "\n",
    "# # point_weights = model.pw_conv[0].weight.squeeze().detach().numpy()\n",
    "# point_weights = test_model.pw_conv[0].weight.detach().numpy()\n",
    "# print(\"point weights shape: \", point_weights.shape)\n",
    "# point_bias = torch.zeros(point_weights.shape[0]).numpy()\n",
    "\n",
    "# print(\"Input shape: \", input.shape)\n",
    "# depth_expected = test_model.dw_conv[0](input)\n",
    "# point_expected = test_model.pw_conv[0](depth_expected)\n",
    "# print(\"Depth Expected shape: \", depth_expected.shape)\n",
    "# # print(\"Point Expected shape: \", point_expected.shape)\n",
    "\n",
    "# point_expected = point_expected.squeeze().detach().numpy().transpose((1,2,0))\n",
    "# print(\"Point Expected shape: \", point_expected.shape)\n",
    "\n",
    "# point_weights = point_weights.transpose((2, 3, 1, 0)).squeeze()\n",
    "# quantized_point_weights = point_weights * 10**EXPONENT\n",
    "# print(\"point weights shape: \", quantized_point_weights.shape)\n",
    "\n",
    "# # circuit_in, circuit_depth_weights, circuit_depth_bias, circuit_depth_out, depth_out, circuit_depth_remainder = PaddedDepthwiseConv(7, 7, 3, 3, 3, 1, 10**EXPONENT, quantized_image.round(), quantized_depth_weights.round(), depth_bias)\n",
    "# point_input, circuit_point_weights, circuit_point_bias, circuit_point_out, point_out, circuit_point_remainder = PointwiseConv2d(7, 7, 3, 6, 1, 10**EXPONENT, depth_out, quantized_point_weights.round(), point_bias)\n",
    "\n",
    "# test_output = np.array([[[int(out) / 10**EXPONENT for out in asdf] for asdf in asdfasdf] for asdfasdf in point_out])\n",
    "# test_output = test_output[1:6, 1:6, :]\n",
    "\n",
    "# assert(np.allclose(point_expected, test_output, atol=0.00001))\n",
    "\n",
    "# input_json_path = \"padded_input.json\"\n",
    "# with open(input_json_path, \"w\") as input_file:\n",
    "#     json.dump({\n",
    "#                \"in\": circuit_in,\n",
    "#                \"dw_conv_weights\": circuit_depth_weights,\n",
    "#                \"dw_conv_bias\": circuit_depth_bias,\n",
    "#                \"dw_conv_remainder\": circuit_depth_remainder,\n",
    "#                \"dw_conv_out\": circuit_depth_out,\n",
    "        \n",
    "#                \"pw_conv_weights\": circuit_point_weights,\n",
    "#                \"pw_conv_bias\": circuit_point_bias,\n",
    "#                \"pw_conv_remainder\": circuit_point_remainder,\n",
    "#                \"pw_conv_out\": circuit_point_out,\n",
    "#               },\n",
    "#               input_file)\n",
    "\n",
    "\n",
    "# os.chdir(\"circuits\")\n",
    "# !./padded/padded_cpp/padded ../padded_input.json head.wtns\n",
    "# # # !npx snarkjs groth16 prove ./origDepthwiseConv2d/circuit_final.zkey head.wtns proof.json public_test.json\n",
    "# os.chdir(\"../\")\n",
    "\n",
    "# print(\"OK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 603,
   "id": "cba354a2-5a87-4ac7-ae7a-cf1ebe90c1e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3, 3, 3), (6, 3))"
      ]
     },
     "execution_count": 603,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.dw_conv[0].weight.squeeze().detach().numpy().shape, model.pw_conv[0].weight.squeeze().detach().numpy().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2f30bb90-2bfa-47dc-a3c9-5fa52c481005",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape:  torch.Size([1, 3, 7, 7])\n",
      "Expected shape:  torch.Size([1, 3, 5, 5])\n",
      "a shape:  torch.Size([3])\n",
      "b shape:  torch.Size([3])\n"
     ]
    }
   ],
   "source": [
    "# Depthwise convolution\n",
    "depth_weights = test_model.dw_conv[0].weight.squeeze().detach().numpy()\n",
    "depth_bias = torch.zeros(depth_weights.shape[0]).numpy()\n",
    "\n",
    "print(\"Input shape: \", input.shape)\n",
    "expected = test_model.dw_conv[0](input).detach()\n",
    "print(\"Expected shape: \", expected.shape)\n",
    "\n",
    "depth_weights = depth_weights.transpose((1, 2, 0))\n",
    "\n",
    "quantized_image = input.squeeze().numpy().transpose((1,2,0)) * 10**EXPONENT\n",
    "# quantized_image = padded * 10**EXPONENT\n",
    "quantized_depth_weights = depth_weights * 10**EXPONENT\n",
    "\n",
    "circuit_in, circuit_depth_weights, circuit_depth_bias, circuit_depth_out, depth_out, circuit_depth_remainder = PaddedDepthwiseConv(7, 7, 3, 3, 3, 1, 10**EXPONENT, quantized_image.round(), quantized_depth_weights.round(), depth_bias)\n",
    "\n",
    "test_output = np.array([[[int(out) / 10**EXPONENT for out in asdf] for asdf in asdfasdf] for asdfasdf in depth_out])\n",
    "test_output = test_output[1:6, 1:6, :]\n",
    "\n",
    "expected = expected.squeeze().numpy().transpose((1,2,0))\n",
    "\n",
    "assert(np.allclose(expected, test_output, atol=0.00001))\n",
    "\n",
    "# Batch normalization step\n",
    "# BATCH NORM CONSTANTS\n",
    "test_model.eval()\n",
    "gamma = test_model.dw_conv[1].weight\n",
    "beta = test_model.dw_conv[1].bias\n",
    "mean = test_model.dw_conv[1].running_mean\n",
    "var = test_model.dw_conv[1].running_var\n",
    "eps = test_model.dw_conv[1].eps\n",
    "\n",
    "a = (gamma/(var+eps)**.5).detach()\n",
    "print('a shape: ', a.shape)\n",
    "b = (beta-gamma*mean/(var+eps)**.5).detach()\n",
    "print('b shape: ', b.shape)\n",
    "\n",
    "b = b.tolist()\n",
    "\n",
    "quantized_a = (a * 10**EXPONENT).tolist()\n",
    "quantized_b = [bi * 10**(2*EXPONENT) for bi in b]\n",
    "\n",
    "out = test_model.dw_conv[0](input)\n",
    "expected = test_model.dw_conv[1](out)\n",
    "\n",
    "expected = torch.permute(expected.squeeze(), (1, 2, 0))\n",
    "\n",
    "quantized_in = torch.permute(out.squeeze(), (1, 2, 0)) * 10**EXPONENT\n",
    "\n",
    "X, A, B, actual, remainder = BatchNormalizationInt(5, 5, 3, 10**EXPONENT, quantized_in, quantized_a, quantized_b)\n",
    "test_output = [[[from_circom(int(out)) / 10**EXPONENT for out in vec] for vec in matrix] for matrix in actual]\n",
    "\n",
    "assert(torch.allclose(torch.Tensor(test_output), expected, atol=1e-6))\n",
    "\n",
    "# quantized_in = [[[from_circom(int(out)) for out in vec] for vec in matrix] for matrix in circuit_conv_out]\n",
    "\n",
    "_, circuit_depth_bn_a, circuit_depth_bn_b, circuit_depth_bn_out, circuit_depth_bn_remainder = BatchNormalizationInt(7, 7, 3, 10**EXPONENT, depth_out, quantized_a, quantized_b)\n",
    "\n",
    "test_output = np.array([[[int(out) / 10**EXPONENT for out in asdf] for asdf in asdfasdf] for asdfasdf in circuit_depth_bn_out])\n",
    "test_output = test_output[1:6, 1:6, :]\n",
    "\n",
    "assert(torch.allclose(torch.Tensor(test_output), expected, atol=1e-6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "ad9186ba-a226-4c99-b60c-929a6b4b2f05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape:  torch.Size([1, 3, 7, 7])\n",
      "Expected shape:  torch.Size([1, 3, 5, 5])\n",
      "a shape:  torch.Size([3])\n",
      "b shape:  torch.Size([3])\n",
      "point weights shape:  (6, 3, 1, 1)\n",
      "Input shape:  torch.Size([1, 3, 7, 7])\n",
      "Depth Expected shape:  torch.Size([1, 3, 5, 5])\n",
      "Point Expected shape:  (5, 5, 6)\n",
      "point weights shape:  (3, 6)\n",
      "a shape:  torch.Size([6])\n",
      "b shape:  torch.Size([6])\n",
      "Input shape:  torch.Size([1, 3, 7, 7])\n",
      "Depth Expected shape:  torch.Size([1, 6, 5, 5])\n",
      "Expected shape:  torch.Size([5, 5, 6])\n",
      "test shape:  (5, 5, 6)\n",
      "START\n",
      "PRIME - 1 28948022309329048855892746252171976963363056481941647379679742748393362948096\n",
      "------------------------------------------------\n",
      "------------------------------------------------\n",
      "dw_conv done\n",
      "depth batch norm done\n",
      "pw_conv done\n",
      "point batch norm done\n",
      "END\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "# Depthwise convolution\n",
    "input = torch.randn((1, 3, 7, 7))\n",
    "\n",
    "depth_weights = test_model.dw_conv[0].weight.squeeze().detach().numpy()\n",
    "depth_bias = torch.zeros(depth_weights.shape[0]).numpy()\n",
    "\n",
    "print(\"Input shape: \", input.shape)\n",
    "expected = test_model.dw_conv[0](input).detach()\n",
    "print(\"Expected shape: \", expected.shape)\n",
    "\n",
    "depth_weights = depth_weights.transpose((1, 2, 0))\n",
    "\n",
    "quantized_image = input.squeeze().numpy().transpose((1,2,0)) * 10**EXPONENT\n",
    "# quantized_image = padded * 10**EXPONENT\n",
    "quantized_depth_weights = depth_weights * 10**EXPONENT\n",
    "\n",
    "circuit_in, circuit_depth_weights, circuit_depth_bias, circuit_depth_out, depth_out, circuit_depth_remainder = PaddedDepthwiseConv(7, 7, 3, 3, 3, 1, 10**EXPONENT, quantized_image.round(), quantized_depth_weights.round(), depth_bias)\n",
    "\n",
    "test_output = np.array([[[int(out) / 10**EXPONENT for out in asdf] for asdf in asdfasdf] for asdfasdf in depth_out])\n",
    "test_output = test_output[1:-1, 1:-1, :]\n",
    "\n",
    "expected = expected.squeeze().numpy().transpose((1,2,0))\n",
    "\n",
    "assert(np.allclose(expected, test_output, atol=0.00001))\n",
    "\n",
    "# Batch normalization step\n",
    "# BATCH NORM CONSTANTS\n",
    "test_model.eval()\n",
    "gamma = test_model.dw_conv[1].weight\n",
    "beta = test_model.dw_conv[1].bias\n",
    "mean = test_model.dw_conv[1].running_mean\n",
    "var = test_model.dw_conv[1].running_var\n",
    "eps = test_model.dw_conv[1].eps\n",
    "\n",
    "a = (gamma/(var+eps)**.5).detach()\n",
    "print('a shape: ', a.shape)\n",
    "b = (beta-gamma*mean/(var+eps)**.5).detach()\n",
    "print('b shape: ', b.shape)\n",
    "\n",
    "b = b.tolist()\n",
    "\n",
    "quantized_a = (a * 10**EXPONENT).tolist()\n",
    "quantized_b = [bi * 10**(2*EXPONENT) for bi in b]\n",
    "\n",
    "out = test_model.dw_conv[0](input)\n",
    "expected = test_model.dw_conv[1](out)\n",
    "\n",
    "expected = torch.permute(expected.squeeze(), (1, 2, 0))\n",
    "\n",
    "quantized_in = torch.permute(out.squeeze(), (1, 2, 0)) * 10**EXPONENT\n",
    "\n",
    "X, A, B, actual, remainder = BatchNormalizationInt(5, 5, 3, 10**EXPONENT, quantized_in, quantized_a, quantized_b)\n",
    "test_output = [[[from_circom(int(out)) / 10**EXPONENT for out in vec] for vec in matrix] for matrix in actual]\n",
    "\n",
    "assert(torch.allclose(torch.Tensor(test_output), expected, atol=1e-6))\n",
    "\n",
    "# quantized_in = [[[from_circom(int(out)) for out in vec] for vec in matrix] for matrix in circuit_conv_out]\n",
    "\n",
    "_, circuit_depth_bn_a, circuit_depth_bn_b, circuit_depth_bn_out, circuit_depth_bn_remainder = BatchNormalizationInt(7, 7, 3, 10**EXPONENT, depth_out, quantized_a, quantized_b)\n",
    "\n",
    "test_output = np.array([[[int(out) / 10**EXPONENT for out in asdf] for asdf in asdfasdf] for asdfasdf in circuit_depth_bn_out])\n",
    "test_output = test_output[1:-1, 1:-1, :]\n",
    "\n",
    "assert(torch.allclose(torch.Tensor(test_output), expected, atol=1e-6))\n",
    "\n",
    "# Pointwise convolution\n",
    "# point_weights = test_model.pw_conv[0].weight.squeeze().detach().numpy()\n",
    "point_weights = test_model.pw_conv[0].weight.detach().numpy()\n",
    "print(\"point weights shape: \", point_weights.shape)\n",
    "point_bias = torch.zeros(point_weights.shape[0]).numpy()\n",
    "\n",
    "print(\"Input shape: \", input.shape)\n",
    "depth_expected = test_model.dw_conv[0](input)\n",
    "bn_expected = test_model.dw_conv[1](depth_expected)\n",
    "point_expected = test_model.pw_conv[0](bn_expected)\n",
    "print(\"Depth Expected shape: \", depth_expected.shape)\n",
    "# print(\"Point Expected shape: \", point_expected.shape)\n",
    "\n",
    "point_expected = point_expected.squeeze().detach().numpy().transpose((1,2,0))\n",
    "print(\"Point Expected shape: \", point_expected.shape)\n",
    "\n",
    "point_weights = point_weights.transpose((2, 3, 1, 0)).squeeze()\n",
    "quantized_point_weights = point_weights * 10**EXPONENT\n",
    "print(\"point weights shape: \", quantized_point_weights.shape)\n",
    "\n",
    "# circuit_in, circuit_depth_weights, circuit_depth_bias, circuit_depth_out, depth_out, circuit_depth_remainder = PaddedDepthwiseConv(7, 7, 3, 3, 3, 1, 10**EXPONENT, quantized_image.round(), quantized_depth_weights.round(), depth_bias)\n",
    "point_input, circuit_point_weights, circuit_point_bias, circuit_point_out, point_out, circuit_point_remainder = PointwiseConv2d(7, 7, 3, 6, 1, 10**EXPONENT, circuit_depth_bn_out, quantized_point_weights.round(), point_bias)\n",
    "\n",
    "test_output = np.array([[[int(out) / 10**EXPONENT for out in asdf] for asdf in asdfasdf] for asdfasdf in point_out])\n",
    "test_output = test_output[1:-1, 1:-1, :]\n",
    "\n",
    "assert(np.allclose(point_expected, test_output, atol=1e-6))\n",
    "\n",
    "                   \n",
    "# Batch normalization step\n",
    "# BATCH NORM CONSTANTS\n",
    "test_model.eval()\n",
    "gamma = test_model.pw_conv[1].weight\n",
    "beta = test_model.pw_conv[1].bias\n",
    "mean = test_model.pw_conv[1].running_mean\n",
    "var = test_model.pw_conv[1].running_var\n",
    "eps = test_model.pw_conv[1].eps\n",
    "\n",
    "a = (gamma/(var+eps)**.5).detach()\n",
    "print('a shape: ', a.shape)\n",
    "b = (beta-gamma*mean/(var+eps)**.5).detach()\n",
    "print('b shape: ', b.shape)\n",
    "\n",
    "b = b.tolist()\n",
    "\n",
    "quantized_a = (a * 10**EXPONENT).tolist()\n",
    "quantized_b = [bi * 10**(2*EXPONENT) for bi in b]\n",
    "\n",
    "# out = test_model.dw_conv[0](input)\n",
    "# expected = test_model.dw_conv[1](out)\n",
    "        \n",
    "print(\"Input shape: \", input.shape)\n",
    "depth_expected = test_model.dw_conv[0](input)\n",
    "bn_expected = test_model.dw_conv[1](depth_expected)\n",
    "point_expected = test_model.pw_conv[0](bn_expected)\n",
    "expected = test_model.pw_conv[1](point_expected)\n",
    "print(\"Depth Expected shape: \", expected.shape)\n",
    "\n",
    "              \n",
    "expected = torch.permute(expected.squeeze(), (1, 2, 0))\n",
    "\n",
    "quantized_in = torch.permute(point_expected.squeeze(), (1, 2, 0)) * 10**EXPONENT\n",
    "\n",
    "X, A, B, actual, remainder = BatchNormalizationInt(5, 5, 6, 10**EXPONENT, quantized_in, quantized_a, quantized_b)\n",
    "test_output = [[[from_circom(int(out)) / 10**EXPONENT for out in vec] for vec in matrix] for matrix in actual]\n",
    "\n",
    "assert(torch.allclose(torch.Tensor(test_output), expected, atol=1e-6))\n",
    "\n",
    "_, circuit_point_bn_a, circuit_point_bn_b, circuit_point_bn_out, circuit_point_bn_remainder = BatchNormalizationInt(7, 7, 6, 10**EXPONENT, point_out, quantized_a, quantized_b)\n",
    "\n",
    "test_output = np.array([[[int(out) / 10**EXPONENT for out in asdf] for asdf in asdfasdf] for asdfasdf in circuit_point_bn_out])\n",
    "test_output = test_output[1:-1, 1:-1, :]\n",
    "\n",
    "print(\"Expected shape: \", expected.shape)\n",
    "print(\"test shape: \", test_output.shape)\n",
    "assert(torch.allclose(torch.Tensor(test_output), expected, atol=1e-6))\n",
    "\n",
    "input_json_path = \"padded_input.json\"\n",
    "with open(input_json_path, \"w\") as input_file:\n",
    "    json.dump({\n",
    "               \"in\": circuit_in,\n",
    "               \"dw_conv_weights\": circuit_depth_weights,\n",
    "               \"dw_conv_bias\": circuit_depth_bias,\n",
    "               \"dw_conv_remainder\": circuit_depth_remainder,\n",
    "               \"dw_conv_out\": circuit_depth_out,\n",
    "        \n",
    "               \"dw_bn_a\": circuit_depth_bn_a,\n",
    "               \"dw_bn_b\": circuit_depth_bn_b,\n",
    "               \"dw_bn_remainder\": circuit_depth_bn_remainder,\n",
    "               \"dw_bn_out\": circuit_depth_bn_out,\n",
    "        \n",
    "               \"pw_conv_weights\": circuit_point_weights,\n",
    "               \"pw_conv_bias\": circuit_point_bias,\n",
    "               \"pw_conv_remainder\": circuit_point_remainder,\n",
    "               \"pw_conv_out\": circuit_point_out,\n",
    "        \n",
    "               \"pw_bn_a\": circuit_point_bn_a,\n",
    "               \"pw_bn_b\": circuit_point_bn_b,\n",
    "               \"pw_bn_remainder\": circuit_point_bn_remainder,\n",
    "               \"pw_bn_out\": circuit_point_bn_out,\n",
    "              },\n",
    "              input_file)\n",
    "\n",
    "\n",
    "os.chdir(\"circuits\")\n",
    "!./padded/padded_cpp/padded ../padded_input.json head.wtns\n",
    "# # !npx snarkjs groth16 prove ./origDepthwiseConv2d/circuit_final.zkey head.wtns proof.json public_test.json\n",
    "os.chdir(\"../\")\n",
    "\n",
    "print(\"OK\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb20792-ff40-4c22-8acc-73b7a96f594f",
   "metadata": {},
   "source": [
    "# Padded convolution 2 iterations test with true input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8bcbe0be-2d60-4ebf-8d0f-814d63b76dfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted horse - idx: 7\n"
     ]
    }
   ],
   "source": [
    "# MODEL_WEIGHTS_PATH = './checkpoints/model_small_100epochs.pth'\n",
    "\n",
    "# model = ZkMobileNet(trainloader, num_classes=10, alpha=0.25, max_epochs=100)\n",
    "# checkpoint = torch.load(MODEL_WEIGHTS_PATH)\n",
    "# # model.load_state_dict(checkpoint['state_dict'])\n",
    "# model.load_state_dict(checkpoint['net'])\n",
    "model.eval()\n",
    "\n",
    "image, label = validset[0]\n",
    "image = image.unsqueeze(0)\n",
    "logits = model(image)\n",
    "pred_idx = logits.argmax()\n",
    "\n",
    "print(f\"Predicted {classes[pred_idx]} - idx: {pred_idx}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fae23fc0-c934-4517-95f4-ae6908775129",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "5b7d7f64-c2c6-420d-aae5-084443e83cc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 32, 8)\n",
      "(32, 32, 8)\n"
     ]
    }
   ],
   "source": [
    "# CONVOLUTION LAYER\n",
    "weights = model.conv.weight.detach().numpy().transpose(2, 3, 1, 0)\n",
    "image, label = testset[0]\n",
    "\n",
    "expected = model.conv(image).detach().numpy()\n",
    "bias = torch.zeros(weights.shape[3]).numpy()\n",
    "\n",
    "# padded = pad(image, 1).transpose(1,2,0)\n",
    "padded = F.pad(image, (1,1,1,1), \"constant\", 0).numpy()\n",
    "padded = padded.transpose(1,2,0)\n",
    "\n",
    "quantized_image = (padded * 10**EXPONENT).round()\n",
    "quantized_weights = (weights * 10**EXPONENT).round() # .transpose(0, 3, 1, 0) # [nFilters, nChannels, H, W] -> \n",
    "\n",
    "circuit_in, circuit_conv_weights, circuit_conv_bias, circuit_conv_out, circuit_conv_remainder = Conv2DInt(34, 34, 3, 8, 3, 1, 10**EXPONENT, quantized_image, quantized_weights, bias)\n",
    "# output, remainder = Conv2DInt(34, 34, 3, 8, 3, 1, 10**EXPONENT, quantized_image, quantized_weights, bias)\n",
    "# test_output = output / 10**(EXPONENT)\n",
    "test_output = [[[from_circom(int(out)) / 10**EXPONENT for out in asdf] for asdf in asdfasdf] for asdfasdf in circuit_conv_out]\n",
    "\n",
    "expected = expected.transpose((1, 2, 0))\n",
    "\n",
    "assert(np.allclose(test_output, expected, atol=1e-6))\n",
    "\n",
    "# BATCH NORM CONSTANTS\n",
    "gamma = model.bn.weight\n",
    "beta = model.bn.bias\n",
    "mean = model.bn.running_mean\n",
    "var = model.bn.running_var\n",
    "eps = model.bn.eps\n",
    "\n",
    "a = (gamma/(var+eps)**.5).detach()\n",
    "b = (beta-gamma*mean/(var+eps)**.5).detach().tolist()\n",
    "\n",
    "quantized_a = (a * 10**EXPONENT).tolist()\n",
    "quantized_b = [bi * 10**(2*EXPONENT) for bi in b]\n",
    "\n",
    "# BATCH NORM USING PYTORCH OUTPUT\n",
    "image, label = testset[0]\n",
    "image = image.unsqueeze(0)\n",
    "out = model.conv(image)\n",
    "expected = model.bn(out)\n",
    "\n",
    "expected = torch.permute(expected.squeeze(), (1, 2, 0))\n",
    "\n",
    "quantized_in = torch.permute(out.squeeze(), (1, 2, 0)) * 10**EXPONENT\n",
    "\n",
    "X, A, B, actual, remainder = BatchNormalizationInt(32, 32, 8, 10**EXPONENT, quantized_in, quantized_a, quantized_b)\n",
    "test_output = [[[from_circom(int(out)) / 10**EXPONENT for out in vec] for vec in matrix] for matrix in actual]\n",
    "assert(torch.allclose(torch.Tensor(test_output), expected, atol=1e-6))\n",
    "\n",
    "# BATCH NORM USING CIRCUIT CONV OUTPUT\n",
    "quantized_in = [[[from_circom(int(out)) for out in vec] for vec in matrix] for matrix in circuit_conv_out]\n",
    "\n",
    "_, circuit_bn_a, circuit_bn_b, circuit_bn_out, circuit_bn_remainder = BatchNormalizationInt(32, 32, 8, 10**EXPONENT, quantized_in, quantized_a, quantized_b)\n",
    "\n",
    "test_output = [[[int(from_circom(int(out))) / 10**EXPONENT for out in vec] for vec in matrix] for matrix in circuit_bn_out]\n",
    "\n",
    "assert(torch.allclose(torch.Tensor(test_output), expected, atol=1e-6))\n",
    "\n",
    "# RELU USING CIRCUIT OUTPUT\n",
    "relu_in = [[[to_circom(bn_out) if bn_out < MAX_POSITIVE else 0 for bn_out in vec] for vec in matrix] for matrix in circuit_bn_out]\n",
    "relu_out = [[[str(bn_out) if bn_out < MAX_POSITIVE else 0 for bn_out in vec] for vec in matrix] for matrix in relu_in]\n",
    "\n",
    "image, label = testset[0]\n",
    "image = image.unsqueeze(0)\n",
    "conv_output = model.conv(image)\n",
    "bn_output = model.bn(conv_output)\n",
    "relu_expected = model.relu(bn_output).squeeze().detach().numpy().transpose((1,2,0))\n",
    "print(relu_expected.shape)\n",
    "print(dw_circuit_input.shape)\n",
    "\n",
    "test_output = [[[out / 10**EXPONENT if out > 0 else 0 for out in vec] for vec in matrix] for matrix in circuit_bn_out]\n",
    "\n",
    "assert(np.allclose(test_output, relu_expected, atol=1e-6))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9812bfb-62fb-4a46-95ab-6253fb8ecd6d",
   "metadata": {},
   "source": [
    "# DW INPUT WITH ACTUAL IMAGE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ea82838-2a19-4bf2-81bc-2d8be7dbe39f",
   "metadata": {},
   "source": [
    "# Depthwise convolution step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "id": "616f6ce0-2566-489d-aa91-35f827c51b19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape:  torch.Size([32, 32, 8])\n",
      "Pytorch Input shape:  torch.Size([1, 8, 32, 32])\n",
      "Expected shape:  torch.Size([30, 30, 16])\n"
     ]
    }
   ],
   "source": [
    "# Depthwise convolution\n",
    "input = torch.Tensor([[[out if out > 0 else 0 for out in vec] for vec in matrix] for matrix in circuit_bn_out])\n",
    "pytorch_input = torch.Tensor([[[int(out) / 10**EXPONENT if out > 0 else 0 for out in vec] for vec in matrix] for matrix in circuit_bn_out])\n",
    "pytorch_input = torch.permute(pytorch_input, (2, 0, 1)).unsqueeze(0)\n",
    "print(\"Input shape: \", input.shape)\n",
    "\n",
    "depth_weights = model.features[0].dw_conv[0].weight.squeeze().detach().numpy()\n",
    "depth_bias = torch.zeros(depth_weights.shape[0]).numpy()\n",
    "\n",
    "print(\"Pytorch Input shape: \", pytorch_input.shape)\n",
    "dw_conv_pytorch_output = model.features[0].dw_conv[0](pytorch_input).detach()\n",
    "print(\"Expected shape: \", expected.shape)\n",
    "\n",
    "depth_weights = depth_weights.transpose((1, 2, 0))\n",
    "quantized_depth_weights = depth_weights * 10**EXPONENT\n",
    "\n",
    "circuit_in, circuit_depth_weights, circuit_depth_bias, circuit_depth_out, depth_out, circuit_depth_remainder = PaddedDepthwiseConv(32, 32, 8, 8, 3, 1, 10**EXPONENT, input, quantized_depth_weights.round(), depth_bias)\n",
    "\n",
    "test_output = np.array([[[int(out) / 10**EXPONENT for out in asdf] for asdf in asdfasdf] for asdfasdf in depth_out])\n",
    "test_output = test_output[1:31, 1:31, :]\n",
    "\n",
    "expected = dw_conv_pytorch_output.squeeze().numpy().transpose((1,2,0))\n",
    "\n",
    "assert(np.allclose(expected, test_output, atol=0.00001))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dae9bea-bbbd-4e38-8a17-5274fd4fa532",
   "metadata": {},
   "source": [
    "# Depthwise batch normalization step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "id": "ccdeb08d-3465-4e63-bdcf-5d0c11257278",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a shape:  torch.Size([8])\n",
      "b shape:  torch.Size([8])\n"
     ]
    }
   ],
   "source": [
    "# Batch normalization step\n",
    "# BATCH NORM CONSTANTS\n",
    "model.eval()\n",
    "gamma = model.features[0].dw_conv[1].weight\n",
    "beta = model.features[0].dw_conv[1].bias\n",
    "mean = model.features[0].dw_conv[1].running_mean\n",
    "var = model.features[0].dw_conv[1].running_var\n",
    "eps = model.features[0].dw_conv[1].eps\n",
    "\n",
    "a = (gamma/(var+eps)**.5).detach()\n",
    "print('a shape: ', a.shape)\n",
    "b = (beta-gamma*mean/(var+eps)**.5).detach()\n",
    "print('b shape: ', b.shape)\n",
    "\n",
    "b = b.tolist()\n",
    "\n",
    "quantized_a = (a * 10**EXPONENT).tolist()\n",
    "quantized_b = [bi * 10**(2*EXPONENT) for bi in b]\n",
    "\n",
    "# out = model.features[0].dw_conv[0](input)\n",
    "dw_conv_pytorch_output = model.features[0].dw_conv[0](pytorch_input).detach()\n",
    "dw_bn_pytorch_output = model.features[0].dw_conv[1](dw_conv_pytorch_output)\n",
    "\n",
    "expected = torch.permute(dw_bn_pytorch_output.squeeze(), (1, 2, 0))\n",
    "\n",
    "_, circuit_depth_bn_a, circuit_depth_bn_b, circuit_depth_bn_out, circuit_depth_bn_remainder = BatchNormalizationInt(32, 32, 8, 10**EXPONENT, depth_out, quantized_a, quantized_b)\n",
    "\n",
    "test_output = np.array([[[from_circom(int(out)) / 10**EXPONENT for out in vec] for vec in matrix] for matrix in circuit_depth_bn_out])\n",
    "test_output = test_output[1:-1, 1:-1, :]\n",
    "\n",
    "assert(torch.allclose(torch.Tensor(test_output), expected, atol=1e-5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80cfb60c-3f97-4c4a-b3d8-0398d738c5c3",
   "metadata": {},
   "source": [
    "# Pointwise convolution step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "id": "fbfb48a6-a335-4a4f-9789-e88dfa57ffc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point weights shape:  (16, 8, 1, 1)\n",
      "Input shape:  torch.Size([32, 32, 8])\n",
      "point Expected shape:  torch.Size([30, 30, 16])\n",
      "point weights shape:  (8, 16)\n"
     ]
    }
   ],
   "source": [
    "# point_weights = test_model.pw_conv[0].weight.squeeze().detach().numpy()\n",
    "point_weights = model.features[0].pw_conv[0].weight.detach().numpy()\n",
    "print(\"point weights shape: \", point_weights.shape)\n",
    "point_bias = torch.zeros(point_weights.shape[0]).numpy()\n",
    "\n",
    "print(\"Input shape: \", input.shape)\n",
    "# depth_expected = model.dw_conv[0](input)\n",
    "# bn_expected = test_model.dw_conv[1](depth_expected)\n",
    "# point_expected = test_model.pw_conv[0](bn_expected)\n",
    "\n",
    "dw_conv_pytorch_output = model.features[0].dw_conv[0](pytorch_input).detach()\n",
    "dw_bn_pytorch_output = model.features[0].dw_conv[1](dw_conv_pytorch_output)\n",
    "pw_conv_pytorch_output = model.features[0].pw_conv[0](dw_bn_pytorch_output)\n",
    "\n",
    "expected = torch.permute(pw_conv_pytorch_output.squeeze(), (1, 2, 0))\n",
    "\n",
    "print(\"point Expected shape: \", expected.shape)\n",
    "# print(\"Point Expected shape: \", point_expected.shape)\n",
    "\n",
    "# point_expected = point_expected.squeeze().detach().numpy().transpose((1,2,0))\n",
    "# print(\"Point Expected shape: \", point_expected.shape)\n",
    "\n",
    "point_weights = point_weights.transpose((2, 3, 1, 0)).squeeze()\n",
    "quantized_point_weights = point_weights * 10**EXPONENT\n",
    "print(\"point weights shape: \", quantized_point_weights.shape)\n",
    "\n",
    "# circuit_in, circuit_depth_weights, circuit_depth_bias, circuit_depth_out, depth_out, circuit_depth_remainder = PaddedDepthwiseConv(7, 7, 3, 3, 3, 1, 10**EXPONENT, quantized_image.round(), quantized_depth_weights.round(), depth_bias)\n",
    "point_input, circuit_point_weights, circuit_point_bias, circuit_point_out, point_out, circuit_point_remainder = PointwiseConv2d(32, 32, 8, 16, 1, 10**EXPONENT, circuit_depth_bn_out, quantized_point_weights.round(), point_bias)\n",
    "\n",
    "test_output = np.array([[[int(out) / 10**EXPONENT for out in asdf] for asdf in asdfasdf] for asdfasdf in point_out])\n",
    "test_output = test_output[1:-1, 1:-1, :]\n",
    "\n",
    "assert(np.allclose(expected.detach().numpy(), test_output, atol=1e-4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d953c531-31fc-491b-bf55-a7afdf2669af",
   "metadata": {},
   "source": [
    "# Pointwise batch normalization step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "id": "fc04765c-fe2c-428e-bc0c-0657d245440d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a shape:  torch.Size([16])\n",
      "b shape:  torch.Size([16])\n",
      "TEST OUTPUT SHAPE:  (32, 32, 16)\n",
      "EXPECTED SHAPE:  torch.Size([30, 30, 16])\n"
     ]
    }
   ],
   "source": [
    "# Batch normalization step\n",
    "# BATCH NORM CONSTANTS\n",
    "model.eval()\n",
    "gamma = model.features[0].pw_conv[1].weight\n",
    "beta = model.features[0].pw_conv[1].bias\n",
    "mean = model.features[0].pw_conv[1].running_mean\n",
    "var = model.features[0].pw_conv[1].running_var\n",
    "eps = model.features[0].pw_conv[1].eps\n",
    "\n",
    "a = (gamma/(var+eps)**.5).detach()\n",
    "print('a shape: ', a.shape)\n",
    "b = (beta-gamma*mean/(var+eps)**.5).detach()\n",
    "print('b shape: ', b.shape)\n",
    "\n",
    "b = b.tolist()\n",
    "\n",
    "quantized_a = (a * 10**EXPONENT).tolist()\n",
    "quantized_b = [bi * 10**(2*EXPONENT) for bi in b]\n",
    "\n",
    "# out = model.features[0].dw_conv[0](input)\n",
    "dw_conv_pytorch_output = model.features[0].dw_conv[0](pytorch_input).detach()\n",
    "dw_bn_pytorch_output = model.features[0].dw_conv[1](dw_conv_pytorch_output)\n",
    "\n",
    "pw_conv_pytorch_output = model.features[0].pw_conv[0](dw_bn_pytorch_output)\n",
    "pw_bn_pytorch_output = model.features[0].pw_conv[1](pw_conv_pytorch_output)\n",
    "\n",
    "expected = torch.permute(pw_bn_pytorch_output.squeeze(), (1, 2, 0))\n",
    "\n",
    "_, circuit_point_bn_a, circuit_point_bn_b, circuit_point_bn_out, circuit_point_bn_remainder = BatchNormalizationInt(32, 32, 16, 10**EXPONENT, point_out, quantized_a, quantized_b)\n",
    "\n",
    "test_output = np.array([[[from_circom(int(out)) / 10**EXPONENT for out in vec] for vec in matrix] for matrix in circuit_point_bn_out])\n",
    "print(\"TEST OUTPUT SHAPE: \", test_output.shape)\n",
    "print(\"EXPECTED SHAPE: \", expected.shape)\n",
    "test_output = test_output[1:-1, 1:-1, :]\n",
    "\n",
    "assert(torch.allclose(torch.Tensor(test_output), expected, atol=1e-5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "id": "ff1fa4f3-f267-42c9-8af4-65e156f2b9f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 336,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(circuit_depth_bn_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "id": "50f24812-7964-4e87-acb8-ee808fe3dae3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "START\n",
      "PRIME - 1 28948022309329048855892746252171976963363056481941647379679742748393362948096\n",
      "------------------------------------------------\n",
      "------------------------------------------------\n",
      "dw_conv done\n",
      "depth batch norm done\n",
      "pw_conv done\n",
      "point batch norm done\n",
      "END\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "input_json_path = \"test.json\"\n",
    "with open(input_json_path, \"w\") as input_file:\n",
    "    json.dump({\n",
    "               \"in\": circuit_in,\n",
    "               \"dw_conv_weights\": circuit_depth_weights,\n",
    "               \"dw_conv_bias\": circuit_depth_bias,\n",
    "               \"dw_conv_remainder\": circuit_depth_remainder,\n",
    "               \"dw_conv_out\": circuit_depth_out,\n",
    "        \n",
    "               \"dw_bn_a\": circuit_depth_bn_a,\n",
    "               \"dw_bn_b\": circuit_depth_bn_b,\n",
    "               \"dw_bn_remainder\": circuit_depth_bn_remainder,\n",
    "               \"dw_bn_out\": circuit_depth_bn_out,\n",
    "        \n",
    "               \"pw_conv_weights\": circuit_point_weights,\n",
    "               \"pw_conv_bias\": circuit_point_bias,\n",
    "               \"pw_conv_remainder\": circuit_point_remainder,\n",
    "               \"pw_conv_out\": circuit_point_out,\n",
    "        \n",
    "               \"pw_bn_a\": circuit_point_bn_a,\n",
    "               \"pw_bn_b\": circuit_point_bn_b,\n",
    "               \"pw_bn_remainder\": circuit_point_bn_remainder,\n",
    "               \"pw_bn_out\": circuit_point_bn_out,\n",
    "              },\n",
    "              input_file)\n",
    "\n",
    "\n",
    "os.chdir(\"circuits\")\n",
    "!./padded/padded_cpp/padded ../test.json head.wtns\n",
    "# # !npx snarkjs groth16 prove ./origDepthwiseConv2d/circuit_final.zkey head.wtns proof.json public_test.json\n",
    "os.chdir(\"../\")\n",
    "\n",
    "print(\"OK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "8c89b6ee-88e8-436b-8d69-90d73b675392",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "something\n",
      "before conv2d\n",
      "after conv2d\n",
      "after bn\n",
      "at:  0 1 6\n",
      "in:  1745789835369983\n",
      "after relu\n",
      "end\n",
      "ok\n"
     ]
    }
   ],
   "source": [
    "input_json_path = \"head_input1.json\"\n",
    "with open(input_json_path, \"w\") as input_file:\n",
    "    json.dump({\"in\": circuit_in,\n",
    "               \"conv2d_weights\": circuit_conv_weights,\n",
    "               \"conv2d_bias\": circuit_conv_bias,\n",
    "               \"conv2d_out\": circuit_conv_out,\n",
    "               \"conv2d_remainder\": circuit_conv_remainder,\n",
    "               \n",
    "               \"bn_a\": circuit_bn_a,\n",
    "               \"bn_b\": circuit_bn_b,\n",
    "               \"bn_out\": circuit_bn_out,\n",
    "               \"bn_remainder\": circuit_bn_remainder,\n",
    "               \n",
    "               \"relu_out\": relu_out,\n",
    "               }, input_file)\n",
    "\n",
    "os.chdir(\"circuits\")\n",
    "!./head/head_cpp/head ../head_input1.json head.wtns\n",
    "# !npx snarkjs groth16 prove head/circuit_final.zkey head.wtns proof.json public_test.json\n",
    "os.chdir(\"../\")\n",
    "\n",
    "print(\"ok\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "7b676b91-c53d-4a8a-9267-7ad771a993f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 8, 32, 32])\n",
      "(30, 30, 8)\n"
     ]
    }
   ],
   "source": [
    "image, label = testset[0]\n",
    "image = image.unsqueeze(0)\n",
    "conv_out = model.conv(image)\n",
    "bn_out = model.bn(conv_out)\n",
    "dw_input = model.relu(bn_out)\n",
    "dw_conv_expected_output = model.features[0].dw_conv[0](dw_input)\n",
    "dw_bn_expected_output = model.features[0].dw_conv[1](dw_conv_expected_output)\n",
    "\n",
    "dw_conv_expected_output = dw_conv_expected_output.detach().numpy().squeeze().transpose((1,2,0))\n",
    "dw_bn_expected_output = dw_bn_expected_output.detach().numpy().squeeze().transpose((1,2,0))\n",
    "print(dw_input.shape)\n",
    "print(expected_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "a770fc91-74c8-4c69-b2e8-b29442b32407",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30, 30, 16)\n",
      "shape:  (32, 32, 8)\n"
     ]
    }
   ],
   "source": [
    "# def BatchNormalizationInt(nRows, nCols, nChannels, n, X_in, a_in, b_in):\n",
    "#     p = CIRCOM_PRIME\n",
    "#     X = [[[str(int(X_in[i][j][k]) % p) for k in range(nChannels)] for j in range(nCols)] for i in range(nRows)]\n",
    "#     A = [str(int(a_in[k]) % p) for k in range(nChannels)]\n",
    "#     B = [str(int(b_in[k]) % p) for k in range(nChannels)]\n",
    "#     out = [[[0 for _ in range(nChannels)] for _ in range(nCols)] for _ in range(nRows)]\n",
    "#     remainder = [[[None for _ in range(nChannels)] for _ in range(nCols)] for _ in range(nRows)]\n",
    "#     for i in range(nRows):\n",
    "#         for j in range(nCols):\n",
    "#             for k in range(nChannels):\n",
    "#                 out[i][j][k] = int(int(X_in[i][j][k]) * int(a_in[k]) + int(b_in[k]))\n",
    "#                 remainder[i][j][k] = str(int(out[i][j][k]) % n)\n",
    "#                 out[i][j][k] = int(out[i][j][k] // n)\n",
    "#     return X, A, B, out, remainder\n",
    "    \n",
    "class CleanUp():\n",
    "    def __init__(self, model: ZkMobileNet, image: torch.Tensor):\n",
    "        ''' Image should be 3d tensor to start '''\n",
    "        # self.image = image.unsqueeze(0)\n",
    "        self.model = model\n",
    "        self.model.eval()\n",
    "        # conv_out = model.conv(self.image)\n",
    "        # bn_out = model.bn(conv_out)\n",
    "        # self.relu_out = model.relu(bn_out)\n",
    "        self.dw_conv_expected = model.features[0].dw_conv[0](image)\n",
    "        self.dw_bn_expected = model.features[0].dw_conv[1](self.dw_conv_expected)\n",
    "        \n",
    "        self.pw_conv_expected = model.features[0].pw_conv[0](self.dw_bn_expected)\n",
    "        self.pw_bn_expected = model.features[0].pw_conv[1](self.pw_conv_expected)\n",
    "        \n",
    "        self.dw_conv_expected = self.dw_conv_expected.squeeze().detach().numpy().transpose((1,2,0))\n",
    "        self.dw_bn_expected = self.dw_bn_expected.squeeze().detach().numpy().transpose((1,2,0))\n",
    "        \n",
    "        self.pw_conv_expected = self.pw_conv_expected.squeeze().detach().numpy().transpose((1,2,0))\n",
    "        self.pw_bn_expected = self.pw_bn_expected.squeeze().detach().numpy().transpose((1,2,0))\n",
    "        \n",
    "        # return self;\n",
    "\n",
    "    def get_depthwise_inputs(self, input):\n",
    "        conv_inputs = helper.get_depthwise_conv_inputs(input)\n",
    "        bn_inputs = helper.get_depthwise_bn_inputs(conv_inputs['out'])\n",
    "        return { \n",
    "            \"conv\": conv_inputs, \n",
    "            \"bn\": bn_inputs \n",
    "        } \n",
    "        \n",
    "    def get_depthwise_conv_inputs(self, input):\n",
    "        depth_weights = self.model.features[0].dw_conv[0].weight.squeeze().detach().numpy()\n",
    "        depth_bias = torch.zeros(depth_weights.shape[0]).numpy()\n",
    "        \n",
    "        depth_weights = depth_weights.transpose((1, 2, 0))\n",
    "        quantized_depth_weights = depth_weights * 10**EXPONENT\n",
    "        \n",
    "        circuit_in, circuit_depth_weights, circuit_depth_bias, circuit_depth_out, depth_out, circuit_depth_remainder = PaddedDepthwiseConv(32, 32, 8, 8, 3, 1, 10**EXPONENT, input, quantized_depth_weights.round(), depth_bias)\n",
    "        \n",
    "        test_output = np.array([[[int(out) / 10**EXPONENT for out in asdf] for asdf in asdfasdf] for asdfasdf in depth_out])\n",
    "        test_output = test_output[1:-1, 1:-1, :]\n",
    "        \n",
    "        assert(np.allclose(self.dw_conv_expected, test_output, atol=0.00001))\n",
    "        return { \n",
    "            \"in\": circuit_in,\n",
    "            \"weights\": circuit_depth_weights,\n",
    "            \"bias\": circuit_depth_bias,\n",
    "            \"out_str\": circuit_depth_out,\n",
    "            \"out\": depth_out,\n",
    "            \"remainder\": circuit_depth_remainder,\n",
    "        }\n",
    "\n",
    "    def get_depthwise_bn_inputs(self, input):\n",
    "        gamma = self.model.features[0].dw_conv[1].weight\n",
    "        beta = self.model.features[0].dw_conv[1].bias\n",
    "        mean = self.model.features[0].dw_conv[1].running_mean\n",
    "        var = self.model.features[0].dw_conv[1].running_var\n",
    "        eps = self.model.features[0].dw_conv[1].eps\n",
    "        \n",
    "        a = (gamma/(var+eps)**.5).detach()\n",
    "        b = (beta-gamma*mean/(var+eps)**.5).detach()\n",
    "        \n",
    "        b = b.tolist()\n",
    "        \n",
    "        quantized_a = (a * 10**EXPONENT).tolist()\n",
    "        quantized_b = [bi * 10**(2*EXPONENT) for bi in b]\n",
    "        \n",
    "        _, circuit_depth_bn_a, circuit_depth_bn_b, circuit_depth_bn_out, circuit_depth_bn_remainder = BatchNormalizationInt(32, 32, 8, 10**EXPONENT, input, quantized_a, quantized_b)\n",
    "        \n",
    "        test_output = np.array([[[int(out) / 10**EXPONENT for out in asdf] for asdf in asdfasdf] for asdfasdf in circuit_point_bn_out])\n",
    "        test_output = test_output[1:-1, 1:-1, :]\n",
    "        \n",
    "        # print(\"Expected shape: \", dw_bn_expected_output.shape)\n",
    "        # print(\"test shape: \", test_output.shape)\n",
    "        assert(np.allclose(test_output, self.dw_bn_expected, atol=1e-5))\n",
    "        \n",
    "        return { \n",
    "            \"a\": circuit_depth_bn_a,\n",
    "            \"b\": circuit_depth_bn_b,\n",
    "            \"out\": circuit_depth_out,\n",
    "            \"remainder\": circuit_depth_remainder,\n",
    "        }\n",
    "\n",
    "    def get_pointwise_conv_inputs(self, input):\n",
    "        # point_weights = model.pw_conv[0].weight.detach().numpy()\n",
    "        point_weights = self.model.features[0].pw_conv[0].weight.detach().numpy()\n",
    "        print(\"point weights shape: \", point_weights.shape)\n",
    "        point_bias = torch.zeros(point_weights.shape[0]).numpy()\n",
    "        \n",
    "        # print(\"Input shape: \", input.shape)\n",
    "        # depth_expected = test_model.dw_conv[0](input)\n",
    "        # bn_expected = test_model.dw_conv[1](depth_expected)\n",
    "        # point_expected = test_model.pw_conv[0](bn_expected)\n",
    "        # print(\"Depth Expected shape: \", depth_expected.shape)\n",
    "        # print(\"Point Expected shape: \", point_expected.shape)\n",
    "        \n",
    "        # point_expected = point_expected.squeeze().detach().numpy().transpose((1,2,0))\n",
    "        # print(\"Point Expected shape: \", point_expected.shape)\n",
    "        \n",
    "        point_weights = point_weights.transpose((2, 3, 1, 0)).squeeze()\n",
    "        quantized_point_weights = point_weights * 10**EXPONENT\n",
    "        print(\"point weights shape: \", quantized_point_weights.shape)\n",
    "        \n",
    "        # circuit_in, circuit_depth_weights, circuit_depth_bias, circuit_depth_out, depth_out, circuit_depth_remainder = PaddedDepthwiseConv(7, 7, 3, 3, 3, 1, 10**EXPONENT, quantized_image.round(), quantized_depth_weights.round(), depth_bias)\n",
    "        # point_input, circuit_point_weights, circuit_point_bias, circuit_point_out, point_out, circuit_point_remainder = PointwiseConv2d(7, 7, 3, 6, 1, 10**EXPONENT, circuit_depth_bn_out, quantized_point_weights.round(), point_bias)\n",
    "        point_input, circuit_point_weights, circuit_point_bias, circuit_point_out, point_out, circuit_point_remainder = PointwiseConv2d(32, 32, 8, 16, 1, 10**EXPONENT, input, quantized_point_weights.round(), point_bias)\n",
    "        \n",
    "        test_output = np.array([[[int(out) / 10**EXPONENT for out in asdf] for asdf in asdfasdf] for asdfasdf in point_out])\n",
    "        test_output = test_output[1:-1, 1:-1, :]\n",
    "        print(\"EXPECTED SHAPE: \", self.pw_conv_expected.shape)\n",
    "        print(\"ACTUAL SHAPE: \", test_output.shape)\n",
    "        print(self.pw_conv_expected[0][0])\n",
    "        print(\"---------------------------------------------\")\n",
    "        print(test_output[1][1])\n",
    "        \n",
    "        assert(np.allclose(self.pw_conv_expected, test_output, atol=1e-6))\n",
    "\n",
    "image, _ = testset[0]\n",
    "conv_out = model.conv(image.unsqueeze(0))\n",
    "bn_out = model.bn(conv_out)\n",
    "relu_out = model.relu(bn_out)\n",
    "helper = CleanUp(model, relu_out)\n",
    "\n",
    "dw_conv_input = [[[out if out > 0 else 0 for out in vec] for vec in matrix] for matrix in circuit_bn_out]\n",
    "circuit_depth_inputs = helper.get_depthwise_inputs(dw_conv_input)\n",
    "print(helper.pw_conv_expected.shape)\n",
    "print(\"shape: \", np.array(circuit_depth_inputs['bn']['out']).shape)\n",
    "# circuit_point_conv_inputs = helper.get_pointwise_conv_inputs(circuit_depth_inputs['bn']['out'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07e90b1e-be65-4144-8624-97dd10f906e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
